{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6_BERTê¸°ë°˜ë‘ë¬¸ì¥ê´€ê³„ë¶„ë¥˜ëª¨ë¸í•™ìŠµ_ë‘ë¬¸ì¥ê´€ê³„ë¶„ë¥˜í•™ìŠµ.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SFcdbaSa6d3"
      },
      "source": [
        "# BERT ëª¨ë¸ì„ í™œìš©í•œ ë‘ ë¬¸ì¥ ê´€ê³„ ë¶„ë¥˜ í•™ìŠµ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJAMXQMuas08"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QArCDIVtbB60"
      },
      "source": [
        "import torch\n",
        "import sys"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jv8YcVXbCrf"
      },
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9latvMrbElJ"
      },
      "source": [
        "* í•™ìŠµ ë°ì´í„° í™•ì¸\n",
        "  * ë‘ ë¬¸ì¥ê³¼ labelì´ tab(\\t)ìœ¼ë¡œ êµ¬ë¶„ë˜ì–´ìˆìŒ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIRZi3sDbDRW"
      },
      "source": [
        "data = open('/content/para_kqc_sim_data.txt', 'r', encoding='utf-8')\n",
        "lines = data.readlines()\n",
        "\n",
        "# ë°ì´í„°ì…‹ êµ¬ì¡° í™•ì¸\n",
        "print(lines[0:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4Dfw1ZtbUXl"
      },
      "source": [
        "import random\n",
        "random.shuffle(lines)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQJYjnOwbcsV"
      },
      "source": [
        "* train dataì™€ test data êµ¬ì„±\n",
        "  * train data 80%, test data 20%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnPGNMtcbU_R"
      },
      "source": [
        "train = {'sent_a':[], 'sent_b':[], 'label':[]}\n",
        "test = {'sent_a':[], 'sent_b':[], 'label':[]}\n",
        "for i, line in enumerate(lines):\n",
        "    if i < len(lines) * 0.8:\n",
        "        line = line.strip()\n",
        "        train['sent_a'].append(line.split('\\t')[0])\n",
        "        train['sent_b'].append(line.split('\\t')[1])\n",
        "        train['label'].append(int(line.split('\\t')[2]))\n",
        "    else:\n",
        "        line = line.strip()\n",
        "        test['sent_a'].append(line.split('\\t')[0])\n",
        "        test['sent_b'].append(line.split('\\t')[1])\n",
        "        test['label'].append(int(line.split('\\t')[2]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GINFMJq6bYYg"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1_2QzVobZNe"
      },
      "source": [
        "train_data = pd.DataFrame({\"sent_a\":train['sent_a'], \"sent_b\":train['sent_b'], \"label\":train['label']})\n",
        "test_data = pd.DataFrame({\"sent_a\":test['sent_a'], \"sent_b\":test['sent_b'], \"label\":test['label']})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWJcCgLxbxQf"
      },
      "source": [
        "* ì¤‘ë³µ ë°ì´í„° ì œê±°"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0ZzCe-SbZ9-"
      },
      "source": [
        "# ë°ì´í„° ì¤‘ë³µì„ ì œì™¸í•œ ê°¯ìˆ˜ í™•ì¸\n",
        "print(\"í•™ìŠµë°ì´í„° : \",train_data.groupby(['sent_a', 'sent_b']).ngroups,\" ë¼ë°¸ : \",train_data['label'].nunique())\n",
        "# í•™ìŠµë°ì´í„° :  15183  ë¼ë°¸ :  2\n",
        "print(\"ë°ìŠ¤íŠ¸ ë°ì´í„° : \",test_data.groupby(['sent_a', 'sent_b']).ngroups,\" ë¼ë²¨ : \",test_data['label'].nunique())\n",
        "# ë°ìŠ¤íŠ¸ ë°ì´í„° :  3796  ë¼ë²¨ :  2\n",
        "\n",
        "# ì¤‘ë³µ ë°ì´í„° ì œê±°\n",
        "train_data.drop_duplicates(subset=['sent_a', 'sent_b'], inplace= True)\n",
        "test_data.drop_duplicates(subset=['sent_a', 'sent_b'], inplace= True)\n",
        "\n",
        "# ë°ì´í„°ì…‹ ê°¯ìˆ˜ í™•ì¸\n",
        "print('ì¤‘ë³µ ì œê±° í›„ í•™ìŠµ ë°ì´í„°ì…‹ : {}'.format(len(train_data)))\n",
        "# ì¤‘ë³µ ì œê±° í›„ í•™ìŠµ ë°ì´í„°ì…‹ : 15183\n",
        "print('ì¤‘ë³µ ì œê±° í›„ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ : {}'.format(len(test_data)))\n",
        "# ì¤‘ë³µ ì œê±° í›„ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ : 3796"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTLtUqstbzgC"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjE3SsOJb0PQ"
      },
      "source": [
        "* null ë°ì´í„° ì œê±°"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHLV25P9b1li"
      },
      "source": [
        "# null ë°ì´í„° ì œê±°\n",
        "train_data.replace('', np.nan, inplace=True)\n",
        "test_data.replace('', np.nan, inplace=True)\n",
        "\n",
        "train_data = train_data.dropna(how = 'any')\n",
        "test_data = test_data.dropna(how = 'any')\n",
        "\n",
        "print('null ì œê±° í›„ í•™ìŠµ ë°ì´í„°ì…‹ : {}'.format(len(train_data)))\n",
        "# null ì œê±° í›„ í•™ìŠµ ë°ì´í„°ì…‹ : 15183\n",
        "print('null ì œê±° í›„ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ : {}'.format(len(test_data)))\n",
        "# null ì œê±° í›„ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ : 3796"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCiMEa3nb2bh"
      },
      "source": [
        "print(train_data['sent_a'][0])\n",
        "# ì˜¤ëŠ˜ ê´€ì•…êµ¬ ìŠµë„ëŠ”?\n",
        "print(train_data['sent_b'][0])\n",
        "# ì˜¤ëŠ˜ ê´€ì•…êµ¬ ìŠµë„ ì•Œê³ ì‹¶ìŠµë‹ˆë‹¤.\n",
        "print(train_data['label'][0])\n",
        "# 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qo6x-2iFcFoE"
      },
      "source": [
        "# í•™ìŠµ ì „ì œ ë¬¸ì¥ ê¸¸ì´ì¡°ì‚¬\n",
        "print('í•™ìŠµ ì „ì œ ë¬¸ì¥ì˜ ìµœëŒ€ ê¸¸ì´ :',max(len(l) for l in train_data['sent_a']))\n",
        "# í•™ìŠµ ì „ì œ ë¬¸ì¥ì˜ ìµœëŒ€ ê¸¸ì´ : 49\n",
        "print('ì „ì œ ë¬¸ì¥ì˜ í‰ê·  ê¸¸ì´ :',sum(map(len, train_data['sent_a']))/len(train_data['sent_a']))\n",
        "# ì „ì œ ë¬¸ì¥ì˜ í‰ê·  ê¸¸ì´ : 22.360995850622405\n",
        "\n",
        "plt.hist([len(s) for s in train_data['sent_a']], bins=50)\n",
        "plt.xlabel('length of data')\n",
        "plt.ylabel('number of data')\n",
        "plt.show()\n",
        "\n",
        "# í•™ìŠµ ê°€ì • ë¬¸ì¥ ê¸¸ì´ì¡°ì‚¬\n",
        "print('í•™ìŠµ ê°€ì • ë¬¸ì¥ì˜ ìµœëŒ€ ê¸¸ì´ :',max(len(l) for l in train_data['sent_b']))\n",
        "# í•™ìŠµ ê°€ì • ë¬¸ì¥ì˜ ìµœëŒ€ ê¸¸ì´ : 65\n",
        "print('ê°€ì • ë¬¸ì¥ì˜ í‰ê·  ê¸¸ì´ :',sum(map(len, train_data['sent_b']))/len(train_data['sent_b']))\n",
        "# ê°€ì • ë¬¸ì¥ì˜ í‰ê·  ê¸¸ì´ : 25.547322663505238\n",
        "\n",
        "plt.hist([len(s) for s in train_data['sent_b']], bins=50)\n",
        "plt.xlabel('length of data')\n",
        "plt.ylabel('number of data')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zD1S4afLcUQZ"
      },
      "source": [
        "* BERTë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fe6pir9hcNzm"
      },
      "source": [
        "# Store the model we want to use\n",
        "from transformers import AutoModel, AutoTokenizer, BertTokenizer\n",
        "MODEL_NAME = \"bert-base-multilingual-cased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pa-CFdfzca1Q"
      },
      "source": [
        "* tokenizerì—ì„œ ë‘ ë¬¸ì¥ ê´€ê³„ ë¶„ë¥˜ taskì—ì„œ ë¬¸ì¥ 2ê°œë¥¼ inputìœ¼ë¡œ ë„£ìŒ\n",
        "  * tokenizerê°€ ìë™ìœ¼ë¡œ `[CLS] sentenceA [SEP] sentenceB [SEP]` í˜•íƒœë¡œ tokenì„ ë¶€ì°©í•˜ì—¬ tokenizingì„ í•¨\n",
        "  * token_type_idsë¥¼ segmentAëŠ” 0, segmentBëŠ” 1ë¡œ taggingí•¨\n",
        "\n",
        "* train data ì „ì²´ë¥¼ í•œë²ˆì— embeddingí•¨\n",
        "  * input : list => output : list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_zF5nMJcOj-"
      },
      "source": [
        "tokenized_train_sentences = tokenizer(\n",
        "    list(train_data['sent_a'][0:]),\n",
        "    list(train_data['sent_b'][0:]),\n",
        "    return_tensors=\"pt\",\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    add_special_tokens=True,\n",
        "    max_length=64\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4xhSZwVdhZp"
      },
      "source": [
        "* `attention_mask` : ì‹¤ì œ ë¶„ë¥˜ë¥¼ ìœ„í•´ ì‚¬ìš©ë˜ëŠ” ë°ì´í„°ëŠ” 1, ë‚˜ë¨¸ì§€ëŠ” 0ìœ¼ë¡œ taggingë¨"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "178UC_RPcSJX"
      },
      "source": [
        "print(tokenized_train_sentences[0])\n",
        "# Encoding(num_tokens=64, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
        "print(tokenized_train_sentences[0].tokens)\n",
        "# ['[CLS]', 'ì˜¤', '##ëŠ˜', 'ê´€', '##ì•…', '##êµ¬', 'ìŠµ', '##ë„ëŠ”', '?', '[SEP]', 'ì˜¤', '##ëŠ˜', 'ê´€', '##ì•…', '##êµ¬', 'ìŠµ', '##ë„', 'ì•Œ', '##ê³ ', '##ì‹¶', '##ìŠµ', '##ë‹ˆë‹¤', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
        "print(tokenized_train_sentences[0].ids)\n",
        "# [101, 9580, 118762, 8900, 119110, 17196, 9482, 60884, 136, 102, 9580, 118762, 8900, 119110, 17196, 9482, 12092, 9524, 11664, 119088, 119081, 48345, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "print(tokenized_train_sentences[0].attention_mask)\n",
        "# [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gLMyVgGdqhf"
      },
      "source": [
        " * í‰ê°€ë¥¼ ìœ„í•œ test data tokenizing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkUDWFj7cWxE"
      },
      "source": [
        "tokenized_test_sentences = tokenizer(\n",
        "    list(test_data['sent_a'][0:]),\n",
        "    list(test_data['sent_b'][0:]),\n",
        "    return_tensors=\"pt\",\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    add_special_tokens=True,\n",
        "    max_length=64\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pK6iPlx0dyzS"
      },
      "source": [
        "* label ì €ì¥ ë° í™•ì¸"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Uk8-5H5cXmp"
      },
      "source": [
        "train_label = train_data['label'].values[0:]\n",
        "test_label = test_data['label'].values[0:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyhz6EtrcYNd"
      },
      "source": [
        "print(train_label[0]) # 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3R_UVDqRd64N"
      },
      "source": [
        "* `__getitem__()` : stepì´ ì§„í–‰ë¨ì— ë”°ë¼ modelì— ì§€ì†ì ìœ¼ë¡œ ì…ë ¥ë˜ëŠ” ë°ì´í„°\n",
        "  * input : tokenizerë¥¼ í†µí•´ì„œ ë‚˜ì˜¨ ê²°ê³¼(key, value)ì™€ ì‚¬ì „ì— ì •ì˜ëœ label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIaybvCXdWFL"
      },
      "source": [
        "class MultiSentDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCjGn_EEdYXb"
      },
      "source": [
        "train_dataset = MultiSentDataset(tokenized_train_sentences, train_label)\n",
        "test_dataset = MultiSentDataset(tokenized_test_sentences, test_label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jyou8XiMeWs_"
      },
      "source": [
        "* BERTë¥¼ í™œìš©í•˜ì—¬ train\n",
        "  * model ì…ì¥ì—ì„œ í•œ ë¬¸ì¥ì´ë“ ì§€ ë‘ ë¬¸ì¥ì´ë“ ì§€ ìƒê´€ì—†ì´ tokenizeëœ sentenceê°€ inputìœ¼ë¡œ ë“¤ì–´ê°€ê³ , ë§ˆì§€ë§‰ì— [CLS] token í•˜ë‚˜ë§Œ ë¶„ë¥˜ë¥¼ í•˜ê¸° ë•Œë¬¸ì— ë‹¨ì¼ë¬¸ì¥ë¶„ë¥˜(`BertForSequenceClassification`)ì—ì„œ ì‚¬ìš©í–ˆë˜ model ì‚¬ìš©ì´ ê°€ëŠ¥í•¨"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hht3uXy8dZNU"
      },
      "source": [
        "from transformers import BertForSequenceClassification, Trainer, TrainingArguments,  BertConfig\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    num_train_epochs=3,              # total number of training epochs\n",
        "    per_device_train_batch_size=8,  # batch size per device during training\n",
        "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        "    logging_steps=500,\n",
        "    save_total_limit=2,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MBDkH4Je8C9"
      },
      "source": [
        "* model initialized"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOP2bsRMdaKJ"
      },
      "source": [
        "model = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2) \n",
        "model.parameters\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eP9m8RRyfFJr"
      },
      "source": [
        "* evaluation ê²°ê³¼ ì¶œë ¥í•˜ëŠ” `compute_metrics` í•¨ìˆ˜ êµ¬í˜„"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eksSknqhdbj5"
      },
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4kvzW4RdcYJ"
      },
      "source": [
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated ğŸ¤— Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=test_dataset,             # evaluation dataset\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdmXDQQUddH7"
      },
      "source": [
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_mnpo0Odeyk"
      },
      "source": [
        "trainer.evaluate(eval_dataset=test_dataset)\n",
        "'''\n",
        "{'epoch': 3.0,\n",
        " 'eval_accuracy': 0.9802423603793466,\n",
        " 'eval_f1': 0.9792186201163757,\n",
        " 'eval_loss': 0.0979667603969574,\n",
        " 'eval_precision': 0.9746276889134032,\n",
        " 'eval_recall': 0.9838530066815144,\n",
        " 'eval_runtime': 7.0701,\n",
        " 'eval_samples_per_second': 536.911}\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-B5aQnfdfwD"
      },
      "source": [
        "trainer.save_model('./results')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPCJ9_cpd3U0"
      },
      "source": [
        "# native training using torch\n",
        "\n",
        "# bert_config = BertConfig.from_pretrained(MODEL_NAME)\n",
        "# bert_config.num_labels = 3\n",
        "# model = BertForSequenceClassification(bert_config) \n",
        "# model.to(device)\n",
        "# model.train()\n",
        "\n",
        "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# optim = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# for epoch in range(3):\n",
        "#     for batch in train_loader:\n",
        "#         optim.zero_grad()\n",
        "#         input_ids = batch['input_ids'].to(device)\n",
        "#         attention_mask = batch['attention_mask'].to(device)\n",
        "#         labels = batch['labels'].to(device)\n",
        "#         outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "#         loss = outputs[0]\n",
        "#         loss.backward()\n",
        "#         optim.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRBYV41CfcTe"
      },
      "source": [
        "* prediction í•¨ìˆ˜"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkjDR385e6gp"
      },
      "source": [
        "# predictí•¨ìˆ˜\n",
        "# 0: \"non_similar\", 1: \"similar\"\n",
        "def sentences_predict(sent_A, sent_B):\n",
        "    model.eval()\n",
        "    tokenized_sent = tokenizer(\n",
        "            sent_A,\n",
        "            sent_B,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            add_special_tokens=True,\n",
        "            max_length=64\n",
        "    )\n",
        "    \n",
        "    tokenized_sent.to('cuda:0')\n",
        "    with torch.no_grad():# ê·¸ë¼ë””ì—”íŠ¸ ê³„ì‚° ë¹„í™œì„±í™”\n",
        "        outputs = model(\n",
        "            input_ids=tokenized_sent['input_ids'],\n",
        "            attention_mask=tokenized_sent['attention_mask'],\n",
        "            token_type_ids=tokenized_sent['token_type_ids']\n",
        "            )\n",
        "\n",
        "    logits = outputs[0]\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    result = np.argmax(logits) # softmaxë¥¼ í†µê³¼í•˜ê³  ë‚˜ì˜¨ ê°€ì¥ ë†’ì€ ì ìˆ˜ë¥¼ ê°€ì§„ indexë¥¼ return\n",
        "\n",
        "    if result == 0:\n",
        "      result = 'non_similar'\n",
        "    elif result == 1:\n",
        "      result = 'similar'\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENF75PV6fPL3"
      },
      "source": [
        "print(sentences_predict('ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì–´ë•Œìš”?','ì˜¤ëŠ˜ì˜ ë‚ ì”¨ë¥¼ ì•Œë ¤ì¤˜')) # similar\n",
        "print(sentences_predict('ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì–´ë•Œìš”?','ê¸°ë¶„ ì§„ì§œ ì•ˆì¢‹ë‹¤.')) # non_similar\n",
        "print(sentences_predict('ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì–´ë•Œìš”?','ì˜¤ëŠ˜ ê¸°ë¶„ ì–´ë– ì„¸ìš”?')) # non_similar\n",
        "print(sentences_predict('ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì–´ë•Œìš”?','ì˜¤ëŠ˜ ê¸°ë¶„ì´ ì–´ë•Œìš”?')) # non_similar\n",
        "print(sentences_predict('ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì–´ë•Œìš”?','ì§€ê¸ˆ ë‚ ì”¨ê°€ ì–´ë•Œìš”?')) # non_similar\n",
        "print(sentences_predict('ë¬´í˜‘ ì†Œì„¤ ì¶”ì²œí•´ì£¼ì„¸ìš”.','ë¬´í˜‘ ì¥ë¥´ì˜ ì†Œì„¤ ì¶”ì²œ ë¶€íƒë“œë¦½ë‹ˆë‹¤.')) # similar\n",
        "print(sentences_predict('ë¬´í˜‘ ì†Œì„¤ ì¶”ì²œí•´ì£¼ì„¸ìš”.','íŒíƒ€ì§€ ì†Œì„¤ ì¶”ì²œí•´ì£¼ì„¸ìš”.')) # non_similar\n",
        "print(sentences_predict('ë¬´í˜‘ ì†Œì„¤ ì¶”ì²œí•´ì£¼ì„¸ìš”.','ë¬´í˜‘ ëŠë‚Œë‚˜ëŠ” ì†Œì„¤ í•˜ë‚˜ ì¶”ì²œí•´ì£¼ì‹¤ ìˆ˜ ìˆìœ¼ì‹¤ê¹Œìš”?')) # similar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BydnY73Xf-lV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}