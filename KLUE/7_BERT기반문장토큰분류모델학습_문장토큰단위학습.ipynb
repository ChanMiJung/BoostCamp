{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "7_BERT기반문장토큰분류모델학습_문장토큰단위학습.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADvFohuddykb"
      },
      "source": [
        "# 문장 토큰 단위 분류 모델 학습"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IQY-0izd3G8"
      },
      "source": [
        "## 1. CPU 및 GPU 환경설정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFf-7FfCdqjs"
      },
      "source": [
        "import torch\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZIPa-i5d6QL"
      },
      "source": [
        "torch.cuda.get_device_name(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hF3J-5Ld7rM"
      },
      "source": [
        "## 2. 데이터셋"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XN76HfEhd-Ar"
      },
      "source": [
        "* 한국해양대에서 공개한 개체명 인식 데이터셋"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cosoQ_ZeGHD"
      },
      "source": [
        "!git clone https://github.com/kmounlp/NER.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zkrcl2WyeGzI"
      },
      "source": [
        "import os\n",
        "import glob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nCffs73eHS6"
      },
      "source": [
        "file_list = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vp_bjG-zeHvg"
      },
      "source": [
        "for x in os.walk('/content/NER/'):\n",
        "    for y in glob.glob(os.path.join(x[0], '*_NER.txt')):    # ner.*, *_NER.txt\n",
        "        file_list.append(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUYPGh1MeJe5"
      },
      "source": [
        "file_list = sorted(file_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md5GvIIyeLea"
      },
      "source": [
        "* 데이터셋 확인\n",
        "  * 개체명인식 데이터셋이 여러 파일로 분류되어 있음"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xt6MqQLIeJ-m"
      },
      "source": [
        "for file_path in file_list:\n",
        "    print(file_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpTj70U5ebtN"
      },
      "source": [
        "## 3. 허깅페이스 트랜스포머 설치"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVshMTwzeeKn"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4yql8E1eezs"
      },
      "source": [
        "## 4. 데이터셋 샘플"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHntgaYnehfX"
      },
      "source": [
        "from pathlib import Path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jG12exmkeicn"
      },
      "source": [
        "file_path = file_list[0]\n",
        "file_path = Path(file_path)\n",
        "raw_text = file_path.read_text().strip()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcETVGzReq1D"
      },
      "source": [
        "* 데이터셋 샘플 확인\n",
        "  * 형태소 단위로 tokenizing되어 있음\n",
        "  * 'B', 'I', 'O' 형태로 태그가 부착되어 있음"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vu6Jiuzyei_w"
      },
      "source": [
        "print(raw_text[0:1000])\n",
        "'''\n",
        "## 1\n",
        "## 오에 겐자부로는 일본 현대문학의 초석을 놓은 것으로 평가받는 작가 나쓰메 소세키(1867~1916)의 대표작 ‘마음’에 담긴 군국주의적 요소, 야스쿠니 신사 참배 행위까지 소설의 삽화로 동원하며 일본 사회의 ‘비정상성’을 문제 삼는다.\n",
        "## <오에 겐자부로:PER>는 <일본:LOC> 현대문학의 초석을 놓은 것으로 평가받는 작가 <나쓰메 소세키:PER>(<1867~1916:DUR>)의 대표작 ‘<마음:POH>’에 담긴 군국주의적 요소, <야스쿠니 신사:ORG> 참배 행위까지 소설의 삽화로 동원하며 <일본:ORG> 사회의 ‘비정상성’을 문제 삼는다.\n",
        "오에\t오에\tNNG\tB-PER\n",
        "_\t_\t_\tI-PER\n",
        "겐자부로\t겐자부로\tNNP\tI-PER\n",
        "는\t는\tJX\tO\n",
        "_\t_\t_\tO\n",
        "일본\t일본\tNNP\tB-LOC\n",
        "_\t_\t_\tO\n",
        "현대\t현대\tNNG\tO\n",
        "문학\t문학\tNNG\tO\n",
        "의\t의\tJKG\tO\n",
        "_\t_\t_\tO\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkfPSgUTekTN"
      },
      "source": [
        "## 5. 데이터셋 전처리"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAP9Fa_me8Ab"
      },
      "source": [
        "* 음절단위로 변환\n",
        "\n",
        "* 현재 데이터셋의 형태는 형태소 단위로 enter로 구분되어 있음\n",
        "* BERT로 입력하기 위해서 하나의 문장으로 합쳐야함\n",
        "* 태그는 각각 token에 맞는 label로 넣어줘야함"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2iCEyMSfvKn"
      },
      "source": [
        "* 전처리 과정\n",
        "  * 데이터셋을 전부 읽고 이중엔터를 기준으로 document로 구분함\n",
        "  * 각 line을 읽으면서 token들을 문장으로 부착함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CG9o9Kn5emYq"
      },
      "source": [
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPxUMLI_enDK"
      },
      "source": [
        "def read_file(file_list):\n",
        "    token_docs = []\n",
        "    tag_docs = []\n",
        "    for file_path in file_list:\n",
        "        # print(\"read file from \", file_path)\n",
        "        file_path = Path(file_path)\n",
        "        raw_text = file_path.read_text().strip()\n",
        "        raw_docs = re.split(r'\\n\\t?\\n', raw_text)\n",
        "        for doc in raw_docs:\n",
        "            tokens = []\n",
        "            tags = []\n",
        "            for line in doc.split('\\n'):\n",
        "                if line[0:1] == \"$\" or line[0:1] == \";\" or line[0:2] == \"##\":\n",
        "                    continue\n",
        "                try:\n",
        "                    token = line.split('\\t')[0]\n",
        "                    tag = line.split('\\t')[3]   # 2: pos, 3: ner\n",
        "                    for i, syllable in enumerate(token):    # 음절 단위로 token을 자름\n",
        "                        tokens.append(syllable) # 음절 단위 정보를 가져와서 문장을 만듬\n",
        "                        modi_tag = tag\n",
        "                        if i > 0:\n",
        "                            if tag[0] == 'B':\n",
        "                                modi_tag = 'I' + tag[1:]    # 음절에 대해 BIO tag를 부착함\n",
        "                        tags.append(modi_tag)\n",
        "                except:\n",
        "                    print(line)\n",
        "            token_docs.append(tokens)\n",
        "            tag_docs.append(tags)\n",
        "\n",
        "    return token_docs, tag_docs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-s1DUfBte-lV"
      },
      "source": [
        "texts, tags = read_file(file_list[:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xa_4ZlMoe_HL"
      },
      "source": [
        "print(len(texts)) # 19263\n",
        "print(len(tags)) # 19263"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuIGWJHRgX3S"
      },
      "source": [
        "* 데이터셋 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mk4NVPO3fYwA"
      },
      "source": [
        "print(texts[0], end='\\n\\n') # 음절 단위로 잘 잘렸네요!\n",
        "# ['오', '에', '_', '겐', '자', '부', '로', '는', '_', '일', '본', '_', '현', '대', '문', '학', '의', '_', '초', '석', '을', '_', '놓', '은', '_', '것', '으', '로', '_', '평', '가', '받', '는', '_', '작', '가', '_', '나', '쓰', '메', '_', '소', '세', '키', '(', '1', '8', '6', '7', '~', '1', '9', '1', '6', ')', '의', '_', '대', '표', '작', '_', '‘', '마', '음', '’', '에', '_', '담', '긴', '_', '군', '국', '주', '의', '적', '_', '요', '소', ',', '_', '야', '스', '쿠', '니', '_', '신', '사', '_', '참', '배', '_', '행', '위', '까', '지', '_', '소', '설', '의', '_', '삽', '화', '로', '_', '동', '원', '하', '며', '_', '일', '본', '_', '사', '회', '의', '_', '‘', '비', '정', '상', '성', '’', '을', '_', '문', '제', '_', '삼', '는', '다', '.']\n",
        "print(tags[0])\n",
        "# ['B-PER', 'I-PER', 'I-PER', 'I-PER', 'I-PER', 'I-PER', 'I-PER', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'I-PER', 'I-PER', 'I-PER', 'I-PER', 'I-PER', 'O', 'B-DUR', 'I-DUR', 'I-DUR', 'I-DUR', 'I-DUR', 'I-DUR', 'I-DUR', 'I-DUR', 'I-DUR', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-POH', 'I-POH', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KACyesHVglLk"
      },
      "source": [
        "* 학습을 위해 tag를 (vocab id)처럼 tag label id로 바꿔줘야함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCA8fuxSfdME"
      },
      "source": [
        "unique_tags = set(tag for doc in tags for tag in doc)\n",
        "tag2id = {tag: id for id, tag in enumerate(unique_tags)}\n",
        "id2tag = {id: tag for tag, id in tag2id.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6Knq8m4g3iO"
      },
      "source": [
        "* 데이터가 제공하는 개체명 태그 종류 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjITaATAfd5p"
      },
      "source": [
        "for i, tag in enumerate(unique_tags):\n",
        "    print(tag)  # 학습을 위한 label list를 확인합니다.\n",
        "\n",
        "'''\n",
        "I-NOH\n",
        "I-MNY\n",
        "I-LOC\n",
        "B-TIM\n",
        "I-PNT\n",
        "I-DAT\n",
        "B-DAT\n",
        "B-PER\n",
        "I-POH\n",
        "I-DUR\n",
        "B-ORG\n",
        "B-LOC\n",
        "B-MNY\n",
        "O\n",
        "I-PER\n",
        "I-ORG\n",
        "B-POH\n",
        "I-TIM\n",
        "B-NOH\n",
        "B-DUR\n",
        "B-PNT\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOmoM0befibz"
      },
      "source": [
        "## 6. EDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHwLT7Axfkix"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQ8e2LqpflkQ"
      },
      "source": [
        "texts_len = [len(x) for x in texts]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyOBnwmzfnlu"
      },
      "source": [
        "### 1. 문장의 길이의 히스토그램"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gj-WjRsXgcxm"
      },
      "source": [
        "plt.figure(figsize=(16,10))\n",
        "plt.hist(texts_len, bins=50, range=[0,800], facecolor='b', density=True, label='Text Length')\n",
        "plt.title('Text Length Histogram')\n",
        "plt.legend()\n",
        "plt.xlabel('Number of Words')\n",
        "plt.ylabel('Probability')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elaqyKm2gfcZ"
      },
      "source": [
        "### 2. 각 NER 태그별 데이터에 포함된 갯수"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DazbLqjwg7AD"
      },
      "source": [
        "for tag in list(tag2id.keys()) : \n",
        "    globals()[tag] = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fksoPiwVg8Uf"
      },
      "source": [
        "for tag in tags : \n",
        "    for ner in tag : \n",
        "        globals()[ner] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-Fa9sGbhSXf"
      },
      "source": [
        "* 개수가 부족한 태그에 대해서는 학습성능이 떨어짐\n",
        "  * 해당 태그에 관련된 데이터셋을 더 추가하여 보완함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m60qw6cag89I"
      },
      "source": [
        "for tag in list(tag2id.keys()) : \n",
        "    print('{:>6} : {:>7,}'. format(tag, globals()[tag]))\n",
        "'''\n",
        " I-NOH :  23,967\n",
        " I-MNY :   6,930\n",
        " I-LOC :  16,537\n",
        " B-TIM :     371\n",
        " I-PNT :   4,613\n",
        " I-DAT :  14,433\n",
        " B-DAT :   5,383\n",
        " B-PER :  13,779\n",
        " I-POH :  37,156\n",
        " I-DUR :   4,573\n",
        " B-ORG :  13,089\n",
        " B-LOC :   6,313\n",
        " B-MNY :   1,440\n",
        "     O : 983,746\n",
        " I-PER :  26,206\n",
        " I-ORG :  41,320\n",
        " B-POH :   6,686\n",
        " I-TIM :   1,876\n",
        " B-NOH :  11,051\n",
        " B-DUR :   1,207\n",
        " B-PNT :   1,672\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AaWcNyqhAVG"
      },
      "source": [
        "## 7. Train Test Split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQ7bw3-Uhd7z"
      },
      "source": [
        "* train과 test 데이터셋을 나누고 학습을 위한 데이터셋 만듬\n",
        "  * train : 80%, test : 20%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMMZvc3ThDAv"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_texts, test_texts, train_tags, test_tags = train_test_split(texts, tags, test_size=.2) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQk-aJmyhDl7"
      },
      "source": [
        "print('Train 문장 : {:>6,}' .format(len(train_texts)))\n",
        "# Train 문장 : 15,410\n",
        "print('Train 태그 : {:>6,}' .format(len(train_tags)))\n",
        "# Train 태그 : 15,410\n",
        "print('Test  문장 : {:>6,}' .format(len(test_texts)))\n",
        "# Test  문장 :  3,853\n",
        "print('Test  태그 : {:>6,}' .format(len(test_tags)))\n",
        "# Test  태그 :  3,853"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1s13-wWohIrO"
      },
      "source": [
        "## 8. BERT 토크나이저"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prYdLHFbhLfd"
      },
      "source": [
        "from transformers import AutoModel, AutoTokenizer, BertTokenizer\n",
        "MODEL_NAME = \"bert-base-multilingual-cased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zTKxmyVh1Sw"
      },
      "source": [
        "* [CLS], [PAD], [SEP] token의 label이 데이터셋에 존재하지 않기 때문에 임의로 데이터셋을 만들어줌\n",
        "  * 'O' 태그로 label 지정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNmoFBOOhMTp"
      },
      "source": [
        "pad_token_id = tokenizer.pad_token_id # 0\n",
        "cls_token_id = tokenizer.cls_token_id # 101\n",
        "sep_token_id = tokenizer.sep_token_id # 102\n",
        "pad_token_label_id = tag2id['O']    # tag2id['O']\n",
        "cls_token_label_id = tag2id['O']\n",
        "sep_token_label_id = tag2id['O']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g50tpZ5EiM3v"
      },
      "source": [
        "* 현재는 음절단위로 나눴기 때문에 wordPiece tokenizer를 사용하면 다른 결과가 나옴\n",
        "* 음절 단위 tokenizer를 만들어서 사용"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfW1-9r2ibHr"
      },
      "source": [
        "* 음절단위 tokenizer 함수 구현\n",
        "  * 중간 음절에는 모두 prefix(##)을 붙임\n",
        "  * 기존 tokenizer와 동일한 return값을 가짐\n",
        "\n",
        "* 'bert-base-multilingual-cased' 모델은 한국어가 대부분 음절단위이기 때문에 음절단위 tokenizer를 적용해도 vocab id를 대부분 획득할 수 있음"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdehcJRUhQ2E"
      },
      "source": [
        "# 기존 토크나이저는 wordPiece tokenizer로 tokenizing 결과를 반환합니다.\n",
        "# 데이터 단위를 음절 단위로 변경했기 때문에, tokenizer도 음절 tokenizer로 바꿀게요! :-)\n",
        "\n",
        "def ner_tokenizer(sent, max_seq_length):    \n",
        "    pre_syllable = \"_\"\n",
        "    input_ids = [pad_token_id] * (max_seq_length - 1)\n",
        "    attention_mask = [0] * (max_seq_length - 1)\n",
        "    token_type_ids = [0] * max_seq_length\n",
        "    sent = sent[:max_seq_length-2]\n",
        "\n",
        "    for i, syllable in enumerate(sent):\n",
        "        if syllable == '_':\n",
        "            pre_syllable = syllable\n",
        "        if pre_syllable != \"_\":\n",
        "            syllable = '##' + syllable  # 중간 음절에는 모두 prefix를 붙입니다.\n",
        "            # 이순신은 조선 -> [이, ##순, ##신, ##은, 조, ##선]\n",
        "        pre_syllable = syllable\n",
        "\n",
        "        input_ids[i] = (tokenizer.convert_tokens_to_ids(syllable))\n",
        "        attention_mask[i] = 1\n",
        "    \n",
        "    input_ids = [cls_token_id] + input_ids\n",
        "    input_ids[len(sent)+1] = sep_token_id\n",
        "    attention_mask = [1] + attention_mask\n",
        "    attention_mask[len(sent)+1] = 1\n",
        "    return {\"input_ids\":input_ids,\n",
        "            \"attention_mask\":attention_mask,\n",
        "            \"token_type_ids\":token_type_ids}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37Cfo6aChbJ_"
      },
      "source": [
        "print(ner_tokenizer(train_texts[0], 5))\n",
        "# {'input_ids': [101, 9954, 20479, 37824, 102], 'attention_mask': [1, 1, 1, 1, 1], 'token_type_ids': [0, 0, 0, 0, 0]}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9gOmzbdhbwV"
      },
      "source": [
        "tokenized_train_sentences = []\n",
        "tokenized_test_sentences = []\n",
        "for text in train_texts:    # 전체 데이터를 tokenizing 합니다.\n",
        "    tokenized_train_sentences.append(ner_tokenizer(text, 128))\n",
        "for text in test_texts:\n",
        "    tokenized_test_sentences.append(ner_tokenizer(text, 128))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaPKuIGkjjZp"
      },
      "source": [
        "* `encode_tags()`\n",
        "  * label을 truncation과 padding과정이 포함된 함수\n",
        "  * tokenizer에 truncation과 padding과정이 포함되어 있기 때문에, label 데이터도 truncation과 padding과정이 필요함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rN1BZmWdhcP0"
      },
      "source": [
        "def encode_tags(tags, max_seq_length):\n",
        "    # label 역시 입력 token과 개수를 맞춰줍니다 :-)\n",
        "    tags = tags[:max_seq_length-2]\n",
        "    labels = [tag2id[tag] for tag in tags]\n",
        "    labels = [tag2id['O']] + labels\n",
        "\n",
        "    padding_length = max_seq_length - len(labels)\n",
        "    labels = labels + ([pad_token_label_id] * padding_length)\n",
        "\n",
        "    return labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KaVtFH-XiBMa"
      },
      "source": [
        "encode_tags(train_tags[0], 5)\n",
        "# [13, 10, 15, 15, 13]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjxYiK6miG3z"
      },
      "source": [
        "train_labels = []\n",
        "test_labels = []\n",
        "\n",
        "for tag in train_tags:\n",
        "    train_labels.append(encode_tags(tag, 128))\n",
        "\n",
        "for tag in test_tags:\n",
        "    test_labels.append(encode_tags(tag, 128))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U02wk3rwiqUa"
      },
      "source": [
        "len(train_labels), len(test_labels)\n",
        "# (15410, 3853)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L41dJuVPjPAJ"
      },
      "source": [
        "## 9. Token 데이터셋"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eO9EIQCkT_l"
      },
      "source": [
        "* TokenDataset 구현\n",
        "  * `__getitem__()`\n",
        "    * input이 들어옴\n",
        "    * 사전에 정의된 label이 순차적으로 들어감"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRTzTz9XkRjL"
      },
      "source": [
        "import torch\n",
        "\n",
        "class TokenDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val) for key, val in self.encodings[idx].items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = TokenDataset(tokenized_train_sentences, train_labels)\n",
        "test_dataset = TokenDataset(tokenized_test_sentences, test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QDbznmbkquH"
      },
      "source": [
        "from transformers import BertForTokenClassification, Trainer, TrainingArguments\n",
        "import sys\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    num_train_epochs=5,              # total number of training epochs\n",
        "    per_device_train_batch_size=8,  # batch size per device during training\n",
        "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        "    logging_steps=100,\n",
        "    learning_rate=3e-5,\n",
        "    save_total_limit=5\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiV4r8GikrqV"
      },
      "source": [
        "## 10. BertForTokenClassification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBH7Yh__kxZA"
      },
      "source": [
        "* 각각의 token 마다 classification이 부착되어 해당 token이 어떤 label 값인지 분류하는 과정을 진행함"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0R493kOGkp3G"
      },
      "source": [
        "* model이 TokenDataset을 가져와서 학습을 진행함"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4mwuir7lHrQ"
      },
      "source": [
        "* model initialize\n",
        "  * `num_labels` : 구분해야하는 label의 개수 지정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnT0se6ik-L3"
      },
      "source": [
        "model = BertForTokenClassification.from_pretrained(MODEL_NAME, num_labels=len(unique_tags))\n",
        "model.to(device)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=test_dataset            # evaluation dataset\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTNP7L4Nk-_7"
      },
      "source": [
        "trainer.train() # 1 epoch에 대략 5분 정도 걸립니다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhTety2dlAr0"
      },
      "source": [
        "## 11. New Data Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhPGcbdnlahM"
      },
      "source": [
        "* 음절 tokenizer를 사용하여 학습했기 때문에\n",
        "* inference에서도 입력된 문장에 대해서 음절 tokenizer를 거친 후에 model의 입력으로 들어가야함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxdSqMpzlDPz"
      },
      "source": [
        "def ner_inference(text) : \n",
        "  \n",
        "    model.eval()\n",
        "    text = text.replace(' ', '_')\n",
        "\n",
        "    predictions , true_labels = [], []\n",
        "    \n",
        "    tokenized_sent = ner_tokenizer(text, len(text)+2)\n",
        "    input_ids = torch.tensor(tokenized_sent['input_ids']).unsqueeze(0).to(device)\n",
        "    attention_mask = torch.tensor(tokenized_sent['attention_mask']).unsqueeze(0).to(device)\n",
        "    token_type_ids = torch.tensor(tokenized_sent['token_type_ids']).unsqueeze(0).to(device)    \n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids)\n",
        "        \n",
        "    logits = outputs['logits']\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = token_type_ids.cpu().numpy()\n",
        "\n",
        "    predictions.extend([list(p) for p in np.argmax(logits, axis=2)]) # 각 token에 대해 softmax가 최대로 되는 값이 무엇인지 가져와서 token 결과 return\n",
        "    true_labels.append(label_ids)\n",
        "\n",
        "    pred_tags = [list(tag2id.keys())[p_i] for p in predictions for p_i in p]\n",
        "\n",
        "    print('{}\\t{}'.format(\"TOKEN\", \"TAG\"))\n",
        "    print(\"===========\")\n",
        "    # for token, tag in zip(tokenizer.decode(tokenized_sent['input_ids']), pred_tags):\n",
        "    #   print(\"{:^5}\\t{:^5}\".format(token, tag))\n",
        "    for i, tag in enumerate(pred_tags):\n",
        "        print(\"{:^5}\\t{:^5}\".format(tokenizer.convert_ids_to_tokens(tokenized_sent['input_ids'][i]), tag))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHnhFcKglP2N"
      },
      "source": [
        "text = '이순신은 조선 중기의 무신이다.'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLuqK4o1lQf_"
      },
      "source": [
        "ner_inference(text)\n",
        "'''\n",
        "TOKEN\tTAG\n",
        "===========\n",
        "[CLS]\t  O  \n",
        "  이  \tB-PER\n",
        " ##순 \tI-PER\n",
        " ##신 \tI-PER\n",
        " ##은 \t  O  \n",
        "  _  \t  O  \n",
        "  조  \t  O  \n",
        " ##선 \t  O  \n",
        "  _  \t  O  \n",
        "  중  \t  O  \n",
        " ##기 \t  O  \n",
        " ##의 \t  O  \n",
        "  _  \t  O  \n",
        "  무  \t  O  \n",
        " ##신 \t  O  \n",
        " ##이 \t  O  \n",
        " ##다 \t  O  \n",
        " ##. \t  O  \n",
        "[SEP]\t  O  \n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REXDdjzdlT-u"
      },
      "source": [
        "text = '로스트아크는 스마일게이트 RPG가 개발한 쿼터뷰 액션 MMORPG 게임이다.'\n",
        "ner_inference(text)\n",
        "'''\n",
        "TOKEN\tTAG\n",
        "===========\n",
        "[CLS]\t  O  \n",
        "  로  \tB-POH\n",
        " ##스 \tI-POH\n",
        " ##트 \tI-POH\n",
        " ##아 \tI-POH\n",
        " ##크 \tI-POH\n",
        " ##는 \t  O  \n",
        "  _  \t  O  \n",
        "  스  \tB-ORG\n",
        " ##마 \tI-ORG\n",
        " ##일 \tI-ORG\n",
        " ##게 \tI-ORG\n",
        " ##이 \tI-ORG\n",
        " ##트 \tI-ORG\n",
        "  _  \t  O  \n",
        "  R  \t  O  \n",
        " ##P \t  O  \n",
        " ##G \t  O  \n",
        " ##가 \t  O  \n",
        "  _  \t  O  \n",
        "  개  \t  O  \n",
        " ##발 \t  O  \n",
        " ##한 \t  O  \n",
        "  _  \t  O  \n",
        "  쿼  \t  O  \n",
        " ##터 \t  O  \n",
        " ##뷰 \t  O  \n",
        "  _  \t  O  \n",
        "  액  \t  O  \n",
        " ##션 \t  O  \n",
        "  _  \t  O  \n",
        "  M  \t  O  \n",
        " ##M \t  O  \n",
        " ##O \t  O  \n",
        " ##R \t  O  \n",
        " ##P \t  O  \n",
        " ##G \t  O  \n",
        "  _  \t  O  \n",
        "  게  \t  O  \n",
        " ##임 \t  O  \n",
        " ##이 \t  O  \n",
        " ##다 \t  O  \n",
        " ##. \t  O  \n",
        "[SEP]\t  O  \n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWJEOWeFlmZl"
      },
      "source": [
        "text = '2014년 11월 12일 최초 공개했으며 2018년 11월 7일부터 오픈 베타 테스트를 진행하다 2019년 12월 4일 정식 오픈했다.'\n",
        "ner_inference(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WheXumRDlrWU"
      },
      "source": [
        "text = '짜장면 7,000원'\n",
        "ner_inference(text)\n",
        "'''\n",
        "TOKEN\tTAG\n",
        "===========\n",
        "[CLS]\t  O  \n",
        "  짜  \t  O  \n",
        " ##장 \t  O  \n",
        " ##면 \t  O  \n",
        "  _  \t  O  \n",
        "  7  \tB-MNY\n",
        " ##, \t  O  \n",
        " ##0 \tI-MNY\n",
        " ##0 \tI-MNY\n",
        " ##0 \tI-MNY\n",
        " ##원 \tI-MNY\n",
        "[SEP]\t  O  \n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}