{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "7_BERTê¸°ë°˜ë¬¸ì¥í† í°ë¶„ë¥˜ëª¨ë¸í•™ìŠµ_ë¬¸ì¥í† í°ë‹¨ìœ„í•™ìŠµ.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADvFohuddykb"
      },
      "source": [
        "# ë¬¸ì¥ í† í° ë‹¨ìœ„ ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IQY-0izd3G8"
      },
      "source": [
        "## 1. CPU ë° GPU í™˜ê²½ì„¤ì •"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFf-7FfCdqjs"
      },
      "source": [
        "import torch\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZIPa-i5d6QL"
      },
      "source": [
        "torch.cuda.get_device_name(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hF3J-5Ld7rM"
      },
      "source": [
        "## 2. ë°ì´í„°ì…‹"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XN76HfEhd-Ar"
      },
      "source": [
        "* í•œêµ­í•´ì–‘ëŒ€ì—ì„œ ê³µê°œí•œ ê°œì²´ëª… ì¸ì‹ ë°ì´í„°ì…‹"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cosoQ_ZeGHD"
      },
      "source": [
        "!git clone https://github.com/kmounlp/NER.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zkrcl2WyeGzI"
      },
      "source": [
        "import os\n",
        "import glob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nCffs73eHS6"
      },
      "source": [
        "file_list = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vp_bjG-zeHvg"
      },
      "source": [
        "for x in os.walk('/content/NER/'):\n",
        "    for y in glob.glob(os.path.join(x[0], '*_NER.txt')):    # ner.*, *_NER.txt\n",
        "        file_list.append(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUYPGh1MeJe5"
      },
      "source": [
        "file_list = sorted(file_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md5GvIIyeLea"
      },
      "source": [
        "* ë°ì´í„°ì…‹ í™•ì¸\n",
        "  * ê°œì²´ëª…ì¸ì‹ ë°ì´í„°ì…‹ì´ ì—¬ëŸ¬ íŒŒì¼ë¡œ ë¶„ë¥˜ë˜ì–´ ìˆìŒ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xt6MqQLIeJ-m"
      },
      "source": [
        "for file_path in file_list:\n",
        "    print(file_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpTj70U5ebtN"
      },
      "source": [
        "## 3. í—ˆê¹…í˜ì´ìŠ¤ íŠ¸ëœìŠ¤í¬ë¨¸ ì„¤ì¹˜"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVshMTwzeeKn"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4yql8E1eezs"
      },
      "source": [
        "## 4. ë°ì´í„°ì…‹ ìƒ˜í”Œ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHntgaYnehfX"
      },
      "source": [
        "from pathlib import Path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jG12exmkeicn"
      },
      "source": [
        "file_path = file_list[0]\n",
        "file_path = Path(file_path)\n",
        "raw_text = file_path.read_text().strip()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcETVGzReq1D"
      },
      "source": [
        "* ë°ì´í„°ì…‹ ìƒ˜í”Œ í™•ì¸\n",
        "  * í˜•íƒœì†Œ ë‹¨ìœ„ë¡œ tokenizingë˜ì–´ ìˆìŒ\n",
        "  * 'B', 'I', 'O' í˜•íƒœë¡œ íƒœê·¸ê°€ ë¶€ì°©ë˜ì–´ ìˆìŒ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vu6Jiuzyei_w"
      },
      "source": [
        "print(raw_text[0:1000])\n",
        "'''\n",
        "## 1\n",
        "## ì˜¤ì— ê²ìë¶€ë¡œëŠ” ì¼ë³¸ í˜„ëŒ€ë¬¸í•™ì˜ ì´ˆì„ì„ ë†“ì€ ê²ƒìœ¼ë¡œ í‰ê°€ë°›ëŠ” ì‘ê°€ ë‚˜ì“°ë©” ì†Œì„¸í‚¤(1867~1916)ì˜ ëŒ€í‘œì‘ â€˜ë§ˆìŒâ€™ì— ë‹´ê¸´ êµ°êµ­ì£¼ì˜ì  ìš”ì†Œ, ì•¼ìŠ¤ì¿ ë‹ˆ ì‹ ì‚¬ ì°¸ë°° í–‰ìœ„ê¹Œì§€ ì†Œì„¤ì˜ ì‚½í™”ë¡œ ë™ì›í•˜ë©° ì¼ë³¸ ì‚¬íšŒì˜ â€˜ë¹„ì •ìƒì„±â€™ì„ ë¬¸ì œ ì‚¼ëŠ”ë‹¤.\n",
        "## <ì˜¤ì— ê²ìë¶€ë¡œ:PER>ëŠ” <ì¼ë³¸:LOC> í˜„ëŒ€ë¬¸í•™ì˜ ì´ˆì„ì„ ë†“ì€ ê²ƒìœ¼ë¡œ í‰ê°€ë°›ëŠ” ì‘ê°€ <ë‚˜ì“°ë©” ì†Œì„¸í‚¤:PER>(<1867~1916:DUR>)ì˜ ëŒ€í‘œì‘ â€˜<ë§ˆìŒ:POH>â€™ì— ë‹´ê¸´ êµ°êµ­ì£¼ì˜ì  ìš”ì†Œ, <ì•¼ìŠ¤ì¿ ë‹ˆ ì‹ ì‚¬:ORG> ì°¸ë°° í–‰ìœ„ê¹Œì§€ ì†Œì„¤ì˜ ì‚½í™”ë¡œ ë™ì›í•˜ë©° <ì¼ë³¸:ORG> ì‚¬íšŒì˜ â€˜ë¹„ì •ìƒì„±â€™ì„ ë¬¸ì œ ì‚¼ëŠ”ë‹¤.\n",
        "ì˜¤ì—\tì˜¤ì—\tNNG\tB-PER\n",
        "_\t_\t_\tI-PER\n",
        "ê²ìë¶€ë¡œ\tê²ìë¶€ë¡œ\tNNP\tI-PER\n",
        "ëŠ”\tëŠ”\tJX\tO\n",
        "_\t_\t_\tO\n",
        "ì¼ë³¸\tì¼ë³¸\tNNP\tB-LOC\n",
        "_\t_\t_\tO\n",
        "í˜„ëŒ€\tí˜„ëŒ€\tNNG\tO\n",
        "ë¬¸í•™\të¬¸í•™\tNNG\tO\n",
        "ì˜\tì˜\tJKG\tO\n",
        "_\t_\t_\tO\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkfPSgUTekTN"
      },
      "source": [
        "## 5. ë°ì´í„°ì…‹ ì „ì²˜ë¦¬"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAP9Fa_me8Ab"
      },
      "source": [
        "* ìŒì ˆë‹¨ìœ„ë¡œ ë³€í™˜\n",
        "\n",
        "* í˜„ì¬ ë°ì´í„°ì…‹ì˜ í˜•íƒœëŠ” í˜•íƒœì†Œ ë‹¨ìœ„ë¡œ enterë¡œ êµ¬ë¶„ë˜ì–´ ìˆìŒ\n",
        "* BERTë¡œ ì…ë ¥í•˜ê¸° ìœ„í•´ì„œ í•˜ë‚˜ì˜ ë¬¸ì¥ìœ¼ë¡œ í•©ì³ì•¼í•¨\n",
        "* íƒœê·¸ëŠ” ê°ê° tokenì— ë§ëŠ” labelë¡œ ë„£ì–´ì¤˜ì•¼í•¨"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2iCEyMSfvKn"
      },
      "source": [
        "* ì „ì²˜ë¦¬ ê³¼ì •\n",
        "  * ë°ì´í„°ì…‹ì„ ì „ë¶€ ì½ê³  ì´ì¤‘ì—”í„°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ documentë¡œ êµ¬ë¶„í•¨\n",
        "  * ê° lineì„ ì½ìœ¼ë©´ì„œ tokenë“¤ì„ ë¬¸ì¥ìœ¼ë¡œ ë¶€ì°©í•¨"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CG9o9Kn5emYq"
      },
      "source": [
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPxUMLI_enDK"
      },
      "source": [
        "def read_file(file_list):\n",
        "    token_docs = []\n",
        "    tag_docs = []\n",
        "    for file_path in file_list:\n",
        "        # print(\"read file from \", file_path)\n",
        "        file_path = Path(file_path)\n",
        "        raw_text = file_path.read_text().strip()\n",
        "        raw_docs = re.split(r'\\n\\t?\\n', raw_text)\n",
        "        for doc in raw_docs:\n",
        "            tokens = []\n",
        "            tags = []\n",
        "            for line in doc.split('\\n'):\n",
        "                if line[0:1] == \"$\" or line[0:1] == \";\" or line[0:2] == \"##\":\n",
        "                    continue\n",
        "                try:\n",
        "                    token = line.split('\\t')[0]\n",
        "                    tag = line.split('\\t')[3]   # 2: pos, 3: ner\n",
        "                    for i, syllable in enumerate(token):    # ìŒì ˆ ë‹¨ìœ„ë¡œ tokenì„ ìë¦„\n",
        "                        tokens.append(syllable) # ìŒì ˆ ë‹¨ìœ„ ì •ë³´ë¥¼ ê°€ì ¸ì™€ì„œ ë¬¸ì¥ì„ ë§Œë“¬\n",
        "                        modi_tag = tag\n",
        "                        if i > 0:\n",
        "                            if tag[0] == 'B':\n",
        "                                modi_tag = 'I' + tag[1:]    # ìŒì ˆì— ëŒ€í•´ BIO tagë¥¼ ë¶€ì°©í•¨\n",
        "                        tags.append(modi_tag)\n",
        "                except:\n",
        "                    print(line)\n",
        "            token_docs.append(tokens)\n",
        "            tag_docs.append(tags)\n",
        "\n",
        "    return token_docs, tag_docs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-s1DUfBte-lV"
      },
      "source": [
        "texts, tags = read_file(file_list[:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xa_4ZlMoe_HL"
      },
      "source": [
        "print(len(texts)) # 19263\n",
        "print(len(tags)) # 19263"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuIGWJHRgX3S"
      },
      "source": [
        "* ë°ì´í„°ì…‹ í™•ì¸"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mk4NVPO3fYwA"
      },
      "source": [
        "print(texts[0], end='\\n\\n') # ìŒì ˆ ë‹¨ìœ„ë¡œ ì˜ ì˜ë ¸ë„¤ìš”!\n",
        "# ['ì˜¤', 'ì—', '_', 'ê²', 'ì', 'ë¶€', 'ë¡œ', 'ëŠ”', '_', 'ì¼', 'ë³¸', '_', 'í˜„', 'ëŒ€', 'ë¬¸', 'í•™', 'ì˜', '_', 'ì´ˆ', 'ì„', 'ì„', '_', 'ë†“', 'ì€', '_', 'ê²ƒ', 'ìœ¼', 'ë¡œ', '_', 'í‰', 'ê°€', 'ë°›', 'ëŠ”', '_', 'ì‘', 'ê°€', '_', 'ë‚˜', 'ì“°', 'ë©”', '_', 'ì†Œ', 'ì„¸', 'í‚¤', '(', '1', '8', '6', '7', '~', '1', '9', '1', '6', ')', 'ì˜', '_', 'ëŒ€', 'í‘œ', 'ì‘', '_', 'â€˜', 'ë§ˆ', 'ìŒ', 'â€™', 'ì—', '_', 'ë‹´', 'ê¸´', '_', 'êµ°', 'êµ­', 'ì£¼', 'ì˜', 'ì ', '_', 'ìš”', 'ì†Œ', ',', '_', 'ì•¼', 'ìŠ¤', 'ì¿ ', 'ë‹ˆ', '_', 'ì‹ ', 'ì‚¬', '_', 'ì°¸', 'ë°°', '_', 'í–‰', 'ìœ„', 'ê¹Œ', 'ì§€', '_', 'ì†Œ', 'ì„¤', 'ì˜', '_', 'ì‚½', 'í™”', 'ë¡œ', '_', 'ë™', 'ì›', 'í•˜', 'ë©°', '_', 'ì¼', 'ë³¸', '_', 'ì‚¬', 'íšŒ', 'ì˜', '_', 'â€˜', 'ë¹„', 'ì •', 'ìƒ', 'ì„±', 'â€™', 'ì„', '_', 'ë¬¸', 'ì œ', '_', 'ì‚¼', 'ëŠ”', 'ë‹¤', '.']\n",
        "print(tags[0])\n",
        "# ['B-PER', 'I-PER', 'I-PER', 'I-PER', 'I-PER', 'I-PER', 'I-PER', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'I-PER', 'I-PER', 'I-PER', 'I-PER', 'I-PER', 'O', 'B-DUR', 'I-DUR', 'I-DUR', 'I-DUR', 'I-DUR', 'I-DUR', 'I-DUR', 'I-DUR', 'I-DUR', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-POH', 'I-POH', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KACyesHVglLk"
      },
      "source": [
        "* í•™ìŠµì„ ìœ„í•´ tagë¥¼ (vocab id)ì²˜ëŸ¼ tag label idë¡œ ë°”ê¿”ì¤˜ì•¼í•¨"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCA8fuxSfdME"
      },
      "source": [
        "unique_tags = set(tag for doc in tags for tag in doc)\n",
        "tag2id = {tag: id for id, tag in enumerate(unique_tags)}\n",
        "id2tag = {id: tag for tag, id in tag2id.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6Knq8m4g3iO"
      },
      "source": [
        "* ë°ì´í„°ê°€ ì œê³µí•˜ëŠ” ê°œì²´ëª… íƒœê·¸ ì¢…ë¥˜ í™•ì¸"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjITaATAfd5p"
      },
      "source": [
        "for i, tag in enumerate(unique_tags):\n",
        "    print(tag)  # í•™ìŠµì„ ìœ„í•œ label listë¥¼ í™•ì¸í•©ë‹ˆë‹¤.\n",
        "\n",
        "'''\n",
        "I-NOH\n",
        "I-MNY\n",
        "I-LOC\n",
        "B-TIM\n",
        "I-PNT\n",
        "I-DAT\n",
        "B-DAT\n",
        "B-PER\n",
        "I-POH\n",
        "I-DUR\n",
        "B-ORG\n",
        "B-LOC\n",
        "B-MNY\n",
        "O\n",
        "I-PER\n",
        "I-ORG\n",
        "B-POH\n",
        "I-TIM\n",
        "B-NOH\n",
        "B-DUR\n",
        "B-PNT\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOmoM0befibz"
      },
      "source": [
        "## 6. EDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHwLT7Axfkix"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQ8e2LqpflkQ"
      },
      "source": [
        "texts_len = [len(x) for x in texts]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyOBnwmzfnlu"
      },
      "source": [
        "### 1. ë¬¸ì¥ì˜ ê¸¸ì´ì˜ íˆìŠ¤í† ê·¸ë¨"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gj-WjRsXgcxm"
      },
      "source": [
        "plt.figure(figsize=(16,10))\n",
        "plt.hist(texts_len, bins=50, range=[0,800], facecolor='b', density=True, label='Text Length')\n",
        "plt.title('Text Length Histogram')\n",
        "plt.legend()\n",
        "plt.xlabel('Number of Words')\n",
        "plt.ylabel('Probability')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elaqyKm2gfcZ"
      },
      "source": [
        "### 2. ê° NER íƒœê·¸ë³„ ë°ì´í„°ì— í¬í•¨ëœ ê°¯ìˆ˜"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DazbLqjwg7AD"
      },
      "source": [
        "for tag in list(tag2id.keys()) : \n",
        "    globals()[tag] = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fksoPiwVg8Uf"
      },
      "source": [
        "for tag in tags : \n",
        "    for ner in tag : \n",
        "        globals()[ner] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-Fa9sGbhSXf"
      },
      "source": [
        "* ê°œìˆ˜ê°€ ë¶€ì¡±í•œ íƒœê·¸ì— ëŒ€í•´ì„œëŠ” í•™ìŠµì„±ëŠ¥ì´ ë–¨ì–´ì§\n",
        "  * í•´ë‹¹ íƒœê·¸ì— ê´€ë ¨ëœ ë°ì´í„°ì…‹ì„ ë” ì¶”ê°€í•˜ì—¬ ë³´ì™„í•¨"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m60qw6cag89I"
      },
      "source": [
        "for tag in list(tag2id.keys()) : \n",
        "    print('{:>6} : {:>7,}'. format(tag, globals()[tag]))\n",
        "'''\n",
        " I-NOH :  23,967\n",
        " I-MNY :   6,930\n",
        " I-LOC :  16,537\n",
        " B-TIM :     371\n",
        " I-PNT :   4,613\n",
        " I-DAT :  14,433\n",
        " B-DAT :   5,383\n",
        " B-PER :  13,779\n",
        " I-POH :  37,156\n",
        " I-DUR :   4,573\n",
        " B-ORG :  13,089\n",
        " B-LOC :   6,313\n",
        " B-MNY :   1,440\n",
        "     O : 983,746\n",
        " I-PER :  26,206\n",
        " I-ORG :  41,320\n",
        " B-POH :   6,686\n",
        " I-TIM :   1,876\n",
        " B-NOH :  11,051\n",
        " B-DUR :   1,207\n",
        " B-PNT :   1,672\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AaWcNyqhAVG"
      },
      "source": [
        "## 7. Train Test Split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQ7bw3-Uhd7z"
      },
      "source": [
        "* trainê³¼ test ë°ì´í„°ì…‹ì„ ë‚˜ëˆ„ê³  í•™ìŠµì„ ìœ„í•œ ë°ì´í„°ì…‹ ë§Œë“¬\n",
        "  * train : 80%, test : 20%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMMZvc3ThDAv"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_texts, test_texts, train_tags, test_tags = train_test_split(texts, tags, test_size=.2) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQk-aJmyhDl7"
      },
      "source": [
        "print('Train ë¬¸ì¥ : {:>6,}' .format(len(train_texts)))\n",
        "# Train ë¬¸ì¥ : 15,410\n",
        "print('Train íƒœê·¸ : {:>6,}' .format(len(train_tags)))\n",
        "# Train íƒœê·¸ : 15,410\n",
        "print('Test  ë¬¸ì¥ : {:>6,}' .format(len(test_texts)))\n",
        "# Test  ë¬¸ì¥ :  3,853\n",
        "print('Test  íƒœê·¸ : {:>6,}' .format(len(test_tags)))\n",
        "# Test  íƒœê·¸ :  3,853"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1s13-wWohIrO"
      },
      "source": [
        "## 8. BERT í† í¬ë‚˜ì´ì €"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prYdLHFbhLfd"
      },
      "source": [
        "from transformers import AutoModel, AutoTokenizer, BertTokenizer\n",
        "MODEL_NAME = \"bert-base-multilingual-cased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zTKxmyVh1Sw"
      },
      "source": [
        "* [CLS], [PAD], [SEP] tokenì˜ labelì´ ë°ì´í„°ì…‹ì— ì¡´ì¬í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— ì„ì˜ë¡œ ë°ì´í„°ì…‹ì„ ë§Œë“¤ì–´ì¤Œ\n",
        "  * 'O' íƒœê·¸ë¡œ label ì§€ì •"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNmoFBOOhMTp"
      },
      "source": [
        "pad_token_id = tokenizer.pad_token_id # 0\n",
        "cls_token_id = tokenizer.cls_token_id # 101\n",
        "sep_token_id = tokenizer.sep_token_id # 102\n",
        "pad_token_label_id = tag2id['O']    # tag2id['O']\n",
        "cls_token_label_id = tag2id['O']\n",
        "sep_token_label_id = tag2id['O']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g50tpZ5EiM3v"
      },
      "source": [
        "* í˜„ì¬ëŠ” ìŒì ˆë‹¨ìœ„ë¡œ ë‚˜ëˆ´ê¸° ë•Œë¬¸ì— wordPiece tokenizerë¥¼ ì‚¬ìš©í•˜ë©´ ë‹¤ë¥¸ ê²°ê³¼ê°€ ë‚˜ì˜´\n",
        "* ìŒì ˆ ë‹¨ìœ„ tokenizerë¥¼ ë§Œë“¤ì–´ì„œ ì‚¬ìš©"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfW1-9r2ibHr"
      },
      "source": [
        "* ìŒì ˆë‹¨ìœ„ tokenizer í•¨ìˆ˜ êµ¬í˜„\n",
        "  * ì¤‘ê°„ ìŒì ˆì—ëŠ” ëª¨ë‘ prefix(##)ì„ ë¶™ì„\n",
        "  * ê¸°ì¡´ tokenizerì™€ ë™ì¼í•œ returnê°’ì„ ê°€ì§\n",
        "\n",
        "* 'bert-base-multilingual-cased' ëª¨ë¸ì€ í•œêµ­ì–´ê°€ ëŒ€ë¶€ë¶„ ìŒì ˆë‹¨ìœ„ì´ê¸° ë•Œë¬¸ì— ìŒì ˆë‹¨ìœ„ tokenizerë¥¼ ì ìš©í•´ë„ vocab idë¥¼ ëŒ€ë¶€ë¶„ íšë“í•  ìˆ˜ ìˆìŒ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdehcJRUhQ2E"
      },
      "source": [
        "# ê¸°ì¡´ í† í¬ë‚˜ì´ì €ëŠ” wordPiece tokenizerë¡œ tokenizing ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
        "# ë°ì´í„° ë‹¨ìœ„ë¥¼ ìŒì ˆ ë‹¨ìœ„ë¡œ ë³€ê²½í–ˆê¸° ë•Œë¬¸ì—, tokenizerë„ ìŒì ˆ tokenizerë¡œ ë°”ê¿€ê²Œìš”! :-)\n",
        "\n",
        "def ner_tokenizer(sent, max_seq_length):    \n",
        "    pre_syllable = \"_\"\n",
        "    input_ids = [pad_token_id] * (max_seq_length - 1)\n",
        "    attention_mask = [0] * (max_seq_length - 1)\n",
        "    token_type_ids = [0] * max_seq_length\n",
        "    sent = sent[:max_seq_length-2]\n",
        "\n",
        "    for i, syllable in enumerate(sent):\n",
        "        if syllable == '_':\n",
        "            pre_syllable = syllable\n",
        "        if pre_syllable != \"_\":\n",
        "            syllable = '##' + syllable  # ì¤‘ê°„ ìŒì ˆì—ëŠ” ëª¨ë‘ prefixë¥¼ ë¶™ì…ë‹ˆë‹¤.\n",
        "            # ì´ìˆœì‹ ì€ ì¡°ì„  -> [ì´, ##ìˆœ, ##ì‹ , ##ì€, ì¡°, ##ì„ ]\n",
        "        pre_syllable = syllable\n",
        "\n",
        "        input_ids[i] = (tokenizer.convert_tokens_to_ids(syllable))\n",
        "        attention_mask[i] = 1\n",
        "    \n",
        "    input_ids = [cls_token_id] + input_ids\n",
        "    input_ids[len(sent)+1] = sep_token_id\n",
        "    attention_mask = [1] + attention_mask\n",
        "    attention_mask[len(sent)+1] = 1\n",
        "    return {\"input_ids\":input_ids,\n",
        "            \"attention_mask\":attention_mask,\n",
        "            \"token_type_ids\":token_type_ids}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37Cfo6aChbJ_"
      },
      "source": [
        "print(ner_tokenizer(train_texts[0], 5))\n",
        "# {'input_ids': [101, 9954, 20479, 37824, 102], 'attention_mask': [1, 1, 1, 1, 1], 'token_type_ids': [0, 0, 0, 0, 0]}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9gOmzbdhbwV"
      },
      "source": [
        "tokenized_train_sentences = []\n",
        "tokenized_test_sentences = []\n",
        "for text in train_texts:    # ì „ì²´ ë°ì´í„°ë¥¼ tokenizing í•©ë‹ˆë‹¤.\n",
        "    tokenized_train_sentences.append(ner_tokenizer(text, 128))\n",
        "for text in test_texts:\n",
        "    tokenized_test_sentences.append(ner_tokenizer(text, 128))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaPKuIGkjjZp"
      },
      "source": [
        "* `encode_tags()`\n",
        "  * labelì„ truncationê³¼ paddingê³¼ì •ì´ í¬í•¨ëœ í•¨ìˆ˜\n",
        "  * tokenizerì— truncationê³¼ paddingê³¼ì •ì´ í¬í•¨ë˜ì–´ ìˆê¸° ë•Œë¬¸ì—, label ë°ì´í„°ë„ truncationê³¼ paddingê³¼ì •ì´ í•„ìš”í•¨"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rN1BZmWdhcP0"
      },
      "source": [
        "def encode_tags(tags, max_seq_length):\n",
        "    # label ì—­ì‹œ ì…ë ¥ tokenê³¼ ê°œìˆ˜ë¥¼ ë§ì¶°ì¤ë‹ˆë‹¤ :-)\n",
        "    tags = tags[:max_seq_length-2]\n",
        "    labels = [tag2id[tag] for tag in tags]\n",
        "    labels = [tag2id['O']] + labels\n",
        "\n",
        "    padding_length = max_seq_length - len(labels)\n",
        "    labels = labels + ([pad_token_label_id] * padding_length)\n",
        "\n",
        "    return labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KaVtFH-XiBMa"
      },
      "source": [
        "encode_tags(train_tags[0], 5)\n",
        "# [13, 10, 15, 15, 13]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjxYiK6miG3z"
      },
      "source": [
        "train_labels = []\n",
        "test_labels = []\n",
        "\n",
        "for tag in train_tags:\n",
        "    train_labels.append(encode_tags(tag, 128))\n",
        "\n",
        "for tag in test_tags:\n",
        "    test_labels.append(encode_tags(tag, 128))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U02wk3rwiqUa"
      },
      "source": [
        "len(train_labels), len(test_labels)\n",
        "# (15410, 3853)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L41dJuVPjPAJ"
      },
      "source": [
        "## 9. Token ë°ì´í„°ì…‹"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eO9EIQCkT_l"
      },
      "source": [
        "* TokenDataset êµ¬í˜„\n",
        "  * `__getitem__()`\n",
        "    * inputì´ ë“¤ì–´ì˜´\n",
        "    * ì‚¬ì „ì— ì •ì˜ëœ labelì´ ìˆœì°¨ì ìœ¼ë¡œ ë“¤ì–´ê°"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRTzTz9XkRjL"
      },
      "source": [
        "import torch\n",
        "\n",
        "class TokenDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val) for key, val in self.encodings[idx].items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = TokenDataset(tokenized_train_sentences, train_labels)\n",
        "test_dataset = TokenDataset(tokenized_test_sentences, test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QDbznmbkquH"
      },
      "source": [
        "from transformers import BertForTokenClassification, Trainer, TrainingArguments\n",
        "import sys\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    num_train_epochs=5,              # total number of training epochs\n",
        "    per_device_train_batch_size=8,  # batch size per device during training\n",
        "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        "    logging_steps=100,\n",
        "    learning_rate=3e-5,\n",
        "    save_total_limit=5\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiV4r8GikrqV"
      },
      "source": [
        "## 10. BertForTokenClassification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBH7Yh__kxZA"
      },
      "source": [
        "* ê°ê°ì˜ token ë§ˆë‹¤ classificationì´ ë¶€ì°©ë˜ì–´ í•´ë‹¹ tokenì´ ì–´ë–¤ label ê°’ì¸ì§€ ë¶„ë¥˜í•˜ëŠ” ê³¼ì •ì„ ì§„í–‰í•¨"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0R493kOGkp3G"
      },
      "source": [
        "* modelì´ TokenDatasetì„ ê°€ì ¸ì™€ì„œ í•™ìŠµì„ ì§„í–‰í•¨"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4mwuir7lHrQ"
      },
      "source": [
        "* model initialize\n",
        "  * `num_labels` : êµ¬ë¶„í•´ì•¼í•˜ëŠ” labelì˜ ê°œìˆ˜ ì§€ì •"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnT0se6ik-L3"
      },
      "source": [
        "model = BertForTokenClassification.from_pretrained(MODEL_NAME, num_labels=len(unique_tags))\n",
        "model.to(device)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated ğŸ¤— Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=test_dataset            # evaluation dataset\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTNP7L4Nk-_7"
      },
      "source": [
        "trainer.train() # 1 epochì— ëŒ€ëµ 5ë¶„ ì •ë„ ê±¸ë¦½ë‹ˆë‹¤."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhTety2dlAr0"
      },
      "source": [
        "## 11. New Data Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhPGcbdnlahM"
      },
      "source": [
        "* ìŒì ˆ tokenizerë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµí–ˆê¸° ë•Œë¬¸ì—\n",
        "* inferenceì—ì„œë„ ì…ë ¥ëœ ë¬¸ì¥ì— ëŒ€í•´ì„œ ìŒì ˆ tokenizerë¥¼ ê±°ì¹œ í›„ì— modelì˜ ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ê°€ì•¼í•¨"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxdSqMpzlDPz"
      },
      "source": [
        "def ner_inference(text) : \n",
        "  \n",
        "    model.eval()\n",
        "    text = text.replace(' ', '_')\n",
        "\n",
        "    predictions , true_labels = [], []\n",
        "    \n",
        "    tokenized_sent = ner_tokenizer(text, len(text)+2)\n",
        "    input_ids = torch.tensor(tokenized_sent['input_ids']).unsqueeze(0).to(device)\n",
        "    attention_mask = torch.tensor(tokenized_sent['attention_mask']).unsqueeze(0).to(device)\n",
        "    token_type_ids = torch.tensor(tokenized_sent['token_type_ids']).unsqueeze(0).to(device)    \n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids)\n",
        "        \n",
        "    logits = outputs['logits']\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = token_type_ids.cpu().numpy()\n",
        "\n",
        "    predictions.extend([list(p) for p in np.argmax(logits, axis=2)]) # ê° tokenì— ëŒ€í•´ softmaxê°€ ìµœëŒ€ë¡œ ë˜ëŠ” ê°’ì´ ë¬´ì—‡ì¸ì§€ ê°€ì ¸ì™€ì„œ token ê²°ê³¼ return\n",
        "    true_labels.append(label_ids)\n",
        "\n",
        "    pred_tags = [list(tag2id.keys())[p_i] for p in predictions for p_i in p]\n",
        "\n",
        "    print('{}\\t{}'.format(\"TOKEN\", \"TAG\"))\n",
        "    print(\"===========\")\n",
        "    # for token, tag in zip(tokenizer.decode(tokenized_sent['input_ids']), pred_tags):\n",
        "    #   print(\"{:^5}\\t{:^5}\".format(token, tag))\n",
        "    for i, tag in enumerate(pred_tags):\n",
        "        print(\"{:^5}\\t{:^5}\".format(tokenizer.convert_ids_to_tokens(tokenized_sent['input_ids'][i]), tag))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHnhFcKglP2N"
      },
      "source": [
        "text = 'ì´ìˆœì‹ ì€ ì¡°ì„  ì¤‘ê¸°ì˜ ë¬´ì‹ ì´ë‹¤.'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLuqK4o1lQf_"
      },
      "source": [
        "ner_inference(text)\n",
        "'''\n",
        "TOKEN\tTAG\n",
        "===========\n",
        "[CLS]\t  O  \n",
        "  ì´  \tB-PER\n",
        " ##ìˆœ \tI-PER\n",
        " ##ì‹  \tI-PER\n",
        " ##ì€ \t  O  \n",
        "  _  \t  O  \n",
        "  ì¡°  \t  O  \n",
        " ##ì„  \t  O  \n",
        "  _  \t  O  \n",
        "  ì¤‘  \t  O  \n",
        " ##ê¸° \t  O  \n",
        " ##ì˜ \t  O  \n",
        "  _  \t  O  \n",
        "  ë¬´  \t  O  \n",
        " ##ì‹  \t  O  \n",
        " ##ì´ \t  O  \n",
        " ##ë‹¤ \t  O  \n",
        " ##. \t  O  \n",
        "[SEP]\t  O  \n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REXDdjzdlT-u"
      },
      "source": [
        "text = 'ë¡œìŠ¤íŠ¸ì•„í¬ëŠ” ìŠ¤ë§ˆì¼ê²Œì´íŠ¸ RPGê°€ ê°œë°œí•œ ì¿¼í„°ë·° ì•¡ì…˜ MMORPG ê²Œì„ì´ë‹¤.'\n",
        "ner_inference(text)\n",
        "'''\n",
        "TOKEN\tTAG\n",
        "===========\n",
        "[CLS]\t  O  \n",
        "  ë¡œ  \tB-POH\n",
        " ##ìŠ¤ \tI-POH\n",
        " ##íŠ¸ \tI-POH\n",
        " ##ì•„ \tI-POH\n",
        " ##í¬ \tI-POH\n",
        " ##ëŠ” \t  O  \n",
        "  _  \t  O  \n",
        "  ìŠ¤  \tB-ORG\n",
        " ##ë§ˆ \tI-ORG\n",
        " ##ì¼ \tI-ORG\n",
        " ##ê²Œ \tI-ORG\n",
        " ##ì´ \tI-ORG\n",
        " ##íŠ¸ \tI-ORG\n",
        "  _  \t  O  \n",
        "  R  \t  O  \n",
        " ##P \t  O  \n",
        " ##G \t  O  \n",
        " ##ê°€ \t  O  \n",
        "  _  \t  O  \n",
        "  ê°œ  \t  O  \n",
        " ##ë°œ \t  O  \n",
        " ##í•œ \t  O  \n",
        "  _  \t  O  \n",
        "  ì¿¼  \t  O  \n",
        " ##í„° \t  O  \n",
        " ##ë·° \t  O  \n",
        "  _  \t  O  \n",
        "  ì•¡  \t  O  \n",
        " ##ì…˜ \t  O  \n",
        "  _  \t  O  \n",
        "  M  \t  O  \n",
        " ##M \t  O  \n",
        " ##O \t  O  \n",
        " ##R \t  O  \n",
        " ##P \t  O  \n",
        " ##G \t  O  \n",
        "  _  \t  O  \n",
        "  ê²Œ  \t  O  \n",
        " ##ì„ \t  O  \n",
        " ##ì´ \t  O  \n",
        " ##ë‹¤ \t  O  \n",
        " ##. \t  O  \n",
        "[SEP]\t  O  \n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWJEOWeFlmZl"
      },
      "source": [
        "text = '2014ë…„ 11ì›” 12ì¼ ìµœì´ˆ ê³µê°œí–ˆìœ¼ë©° 2018ë…„ 11ì›” 7ì¼ë¶€í„° ì˜¤í”ˆ ë² íƒ€ í…ŒìŠ¤íŠ¸ë¥¼ ì§„í–‰í•˜ë‹¤ 2019ë…„ 12ì›” 4ì¼ ì •ì‹ ì˜¤í”ˆí–ˆë‹¤.'\n",
        "ner_inference(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WheXumRDlrWU"
      },
      "source": [
        "text = 'ì§œì¥ë©´ 7,000ì›'\n",
        "ner_inference(text)\n",
        "'''\n",
        "TOKEN\tTAG\n",
        "===========\n",
        "[CLS]\t  O  \n",
        "  ì§œ  \t  O  \n",
        " ##ì¥ \t  O  \n",
        " ##ë©´ \t  O  \n",
        "  _  \t  O  \n",
        "  7  \tB-MNY\n",
        " ##, \t  O  \n",
        " ##0 \tI-MNY\n",
        " ##0 \tI-MNY\n",
        " ##0 \tI-MNY\n",
        " ##ì› \tI-MNY\n",
        "[SEP]\t  O  \n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}