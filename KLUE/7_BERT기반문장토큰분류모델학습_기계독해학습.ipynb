{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "7_BERTê¸°ë°˜ë¬¸ì¥í† í°ë¶„ë¥˜ëª¨ë¸í•™ìŠµ_ê¸°ê³„ë…í•´í•™ìŠµ.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nK2yZYvamcB4"
      },
      "source": [
        "# ê¸°ê³„ë…í•´ ëª¨ë¸ í•™ìŠµ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EznhcjBLmhQj"
      },
      "source": [
        "* ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì´ í¬í•¨ëœ ë¬¸ì„œê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ë¬¸ì„œ token ë‚´ì—ì„œ ì •ë‹µì˜ ìœ„ì¹˜ë¥¼ íŒŒì•…í•˜ëŠ” task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-7C415amV3Y"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YEIqMJMmtDz"
      },
      "source": [
        "import torch\n",
        "import sys"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tCJ7HrOmtkX"
      },
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhK7sVbmmuHg"
      },
      "source": [
        "!mkdir dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgBfyj3Tmv1x"
      },
      "source": [
        "* KorQuAD\n",
        "  * LGCNSì—ì„œ ê³µê°œí•œ í•œêµ­ì–´ ê¸°ê³„ë…í•´ dataset\n",
        "  * json í˜•íƒœë¡œ ë˜ì–´ìˆìŒ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JSiD23omvAX"
      },
      "source": [
        "!wget https://korquad.github.io/dataset/KorQuAD_v1.0_train.json\n",
        "!wget https://korquad.github.io/dataset/KorQuAD_v1.0_dev.json\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DePITjUBm28C"
      },
      "source": [
        "!mv /content/KorQuAD_v1.0_train.json dataset\n",
        "!mv /content/KorQuAD_v1.0_dev.json dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdWsRWIwm-J0"
      },
      "source": [
        "* json parsing\n",
        "  * ë°ì´í„°ì…‹ì„ í•™ìŠµí•˜ê¸° í¸í•œ í˜•íƒœë¡œ ë³€í™˜\n",
        "  * paragraph, question, answerìœ¼ë¡œ ë¶„ë¥˜"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAdtztm6m3wD"
      },
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "def read_squad(path):\n",
        "    path = Path(path)\n",
        "    with open(path, 'rb') as f:\n",
        "        squad_dict = json.load(f)\n",
        "\n",
        "    contexts = []\n",
        "    questions = []\n",
        "    answers = []\n",
        "    for group in squad_dict['data']:\n",
        "        for passage in group['paragraphs']:\n",
        "            context = passage['context']\n",
        "            for qa in passage['qas']:\n",
        "                question = qa['question']\n",
        "                for answer in qa['answers']:\n",
        "                    contexts.append(context)\n",
        "                    questions.append(question)\n",
        "                    answers.append(answer)\n",
        "\n",
        "    return contexts, questions, answers\n",
        "\n",
        "\n",
        "train_contexts, train_questions, train_answers = read_squad('dataset/KorQuAD_v1.0_train.json')\n",
        "val_contexts, val_questions, val_answers = read_squad('dataset/KorQuAD_v1.0_dev.json')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2w5cvORnFBD"
      },
      "source": [
        "print(train_contexts[52471])\n",
        "# ì‚¬ìš°ìŠ¤ëŸ°ë˜ ë¸Œë¦­ìŠ¤í†¤ íƒœìƒ ë³´ìœ„ëŠ” ì–´ë¦´ ì  ìŒì•…ì— ëŒ€í•œ ê´€ì‹¬ì„ í‚¤ì›Œì™”ìœ¼ë©°, ê²°êµ­ì—ëŠ” ì˜ˆìˆ , ìŒì•…, ë””ìì¸ì„ ë°°ì›Œ ì „ë¬¸ì ì¸ ìŒì•…ê°€ ê²½ë ¥ì„ 1963ë…„ë¶€í„° ì°©ìˆ˜í–ˆë‹¤. ã€ˆSpace Oddityã€‰ëŠ” 1969ë…„ 7ì›” ë°œí‘œ ë’¤ ì˜êµ­ ìŒë°˜ ì°¨íŠ¸ì—ì„œ ìƒìœ„ 5ìœ„ì— ì˜¤ë¥¸ ê·¸ì˜ ì²« ì‹±ê¸€ì´ë‹¤. ì‹¤í—˜ í™œë™ì„ ê±°ì¹œ ê·¸ëŠ” 1972ë…„ ìŒì•…ì„ ì¬ê°œ, ìì‹ ì˜ ê¸€ë¨ë¡ ì‹œê¸° ë™ì•ˆ ì´ìƒ‰ì ì´ê³  ì–‘ì„±ì ì¸ ì œ2ì˜ ìì•„ì¸ ì§€ê¸° ìŠ¤íƒ€ë”ìŠ¤íŠ¸ë¡œ í™œë™ì„ ì´ì–´ë‚˜ê°”ë‹¤. ì„±ê³µì„ ê±°ë‘” ì‹±ê¸€ ã€ˆStarmanã€‰ê³¼ ì „ ì„¸ê³„ì ì¸ ì¸ê¸°ë¥¼ ëˆ ìŒë°˜ ã€ŠThe Rise and Fall of Ziggy Stardust and the Spiders from Marsã€‹ìœ¼ë¡œ ìºë¦­í„°ë¥¼ ë‚´ì„¸ìš´ ë³´ìœ„ëŠ” 1975ë…„ \"í”Œë¼ìŠ¤í‹± ì†”\"ì„ ìºë¦­í„°í™”ì‹œì¼œ ìì‹ ì„ ì² ì €íˆ ë°”ê¾¼ë‹¤. ì´ í–‰ë™ì€ ë‹¹ì´ˆ ì˜êµ­ì—ì„œ ê·¸ì˜ ì—´í˜ˆíŒ¬ì˜ ë°˜ë°œì„ ìƒ€ìœ¼ë‚˜ ë¯¸êµ­ì—ì„œëŠ” ì‹±ê¸€ ã€ˆFameã€‰ê³¼ ìŒë°˜ ã€ŠYoung Americansã€‹ì„ í†µí•´ ì²˜ìŒìœ¼ë¡œ ë©”ì´ì €í•œ ì„±ê³µì„ ê±°ë‘ê²Œ ëœë‹¤. 1976ë…„ ë³´ìœ„ëŠ” ì»¬íŠ¸ ì˜í™” ã€Šì§€êµ¬ì— ë–¨ì–´ì§„ ì‚¬ë‚˜ì´ã€‹ì— ì¶œì—°í•˜ê³  ìŒë°˜ ã€ŠStation to Stationã€‹ì„ ë°œí‘œí•œë‹¤. ì´ë“¬í•´ì—ëŠ” ì¼ë ‰íŠ¸ë¦­ ìŒì•…ì„ ì ‘ëª©í•œ ìŒë°˜ ã€ŠLowã€‹ (1977)ì„ ë°œí‘œí•˜ë©´ì„œ ìŒì•…ì  ì˜ˆìƒì„ ê¹¨ëœ¨ë ¸ë‹¤. ì´ ìŒë°˜ì€ ë¸Œë¼ì´ì–¸ ì´ë…¸ì™€ì˜ ì„¸ ë²ˆì˜ í˜‘ì—… ì¤‘ ì²« ë²ˆì§¸ë¡œ ì´ëŠ” ì´í›„ \"ë² ë¥¼ë¦° ì‚¼ë¶€ì‘\"ìœ¼ë¡œ ì¼ì»¬ì–´ì§„ë‹¤. ë’¤ë¥¼ ì´ì–´ ë°œí‘œëœ ã€Š\"Heroes\"ã€‹ (1977)ì™€ ã€ŠLodgerã€‹ (1979)ëŠ” ì˜êµ­ ì°¨íŠ¸ ìƒìœ„ 5ìœ„ì— ì§„ì…, ì§€ì†ì ì¸ ê·¹ì°¬ì„ ë°›ì•˜ë‹¤.\n",
        "print(len(train_contexts[52471]))\n",
        "# 734"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZRcd5w3nO4J"
      },
      "source": [
        "print(train_questions[52471])\n",
        "# ë³´ìœ„ê°€ 1977ë…„ ì¼ë ‰íŠ¸ë¦­ ìŒì•…ì„ ì ‘ëª©í•˜ì—¬ ë°œí‘œí•œ ìŒë°˜ì€?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7z7p2v2wnZV5"
      },
      "source": [
        "* answer_start : ì •ë‹µì´ ì‹œì‘ë˜ëŠ” ìŒì ˆ ìˆœì„œ(index)\n",
        "  * endëŠ” ì •ë‹µì˜ lengthì™€ answer_startë¡œ ì•Œ ìˆ˜ ìˆìŒ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhRPu0EvnWkC"
      },
      "source": [
        "print(train_answers[52471]) # ë³¸ë¬¸ ë‚´ì—ì„œ ì •ë‹µì´ ì‹œì‘ë˜ëŠ” ìŒì ˆ ìˆœì„œê°€ 'answer start'ì— ì €ì¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n",
        "# {'text': 'Low', 'answer_start': 568}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRiFdO4HnwFy"
      },
      "source": [
        "* ëª¨ë¸ì˜ input : answer start position, answer end position\n",
        "  * `gold_text` : ì •ë‹µ text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0r7xT-8niBz"
      },
      "source": [
        "def add_end_idx(answers, contexts):\n",
        "    for answer, context in zip(answers, contexts):\n",
        "        # ëª¨ë¸ í•™ìŠµì„ ìœ„í•´ ì •ë‹µ ë°ì´í„°ë¥¼ ë§Œë“¤ê² ìŠµë‹ˆë‹¤.\n",
        "        # ì •ë‹µ ë°ì´í„°ëŠ” startìŒì ˆê³¼ end ìŒì ˆë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n",
        "        # ëª¨ë¸ì€ ì „ì²´ í† í° ì¤‘ì—ì„œ start tokenê³¼ end tokenì„ ì°¾ì•„ë‚´ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•™ìŠµí•˜ê²Œ ë©ë‹ˆë‹¤.\n",
        "        gold_text = answer['text']\n",
        "        start_idx = answer['answer_start']\n",
        "        end_idx = start_idx + len(gold_text)\n",
        "        \n",
        "\n",
        "        # sometimes squad answers are off by a character or two â€“ fix this\n",
        "        # ì‹¤ì œ ë³¸ë¬¸ì—ì„œ í•´ë‹¹ ìŒì ˆ ë²ˆí˜¸ë¡œ ì˜ë¼ëƒˆì„ ë•Œ, ì •ë‹µê³¼ ê°™ì€ì§€ ê²€ì‚¬í•´ì„œ start, endë¥¼ ë³´ì •í•©ë‹ˆë‹¤ :-)\n",
        "        # 'ì´ìˆœì‹ ì€ ì¡°ì„  ì¤‘ê¸°ì˜ ë¬´ì‹ ì´ë‹¤' -> 'ì´ìˆœì‹ ' -> start: 0, end: 4\n",
        "        if context[start_idx:end_idx] == gold_text:\n",
        "            answer['answer_end'] = end_idx\n",
        "        elif context[start_idx-1:end_idx-1] == gold_text:\n",
        "            answer['answer_start'] = start_idx - 1\n",
        "            answer['answer_end'] = end_idx - 1     # When the gold label is off by one character\n",
        "        elif context[start_idx-2:end_idx-2] == gold_text:\n",
        "            answer['answer_start'] = start_idx - 2\n",
        "            answer['answer_end'] = end_idx - 2     # When the gold label is off by two characters\n",
        "    return answers\n",
        "\n",
        "train_answers = add_end_idx(train_answers, train_contexts)\n",
        "val_answers = add_end_idx(val_answers, val_contexts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZP2TD6Hnuz8"
      },
      "source": [
        "from transformers import AutoModel, AutoTokenizer, BertTokenizer\n",
        "MODEL_NAME = \"bert-base-multilingual-cased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "train_encodings = tokenizer(train_contexts, train_questions, truncation=True, padding=True)\n",
        "val_encodings = tokenizer(val_contexts, val_questions, truncation=True, padding=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4lUvfxtouUp"
      },
      "source": [
        "* ê¸°ê³„ë…í•´ì˜ í•µì‹¬\n",
        "  * êµ¬ì¶•í•œ dataëŠ” ìŒì ˆ indexë¥¼ ê°€ì ¸ì˜´\n",
        "  * BERT ëª¨ë¸ì€ wordPiece ë‹¨ìœ„ë¡œ ë˜ì–´ìˆìŒ\n",
        "  * ìŒì ˆ ìˆ«ìë¥¼ token indexë¡œ ë°”ê¿”ì£¼ì–´ í•™ìŠµë°ì´í„°ê°€ start positionì— ëŒ€í•œ ì •ë³´ì™€ end positionì— ëŒ€í•œ ì •ë³´ë¥¼ token ë‹¨ìœ„ë¡œ ê°€ì§€ê²Œ ë¨\n",
        "    * í•´ë‹¹ token indexê°€ ì •ë‹µì˜ ì‹œì‘ì„ì„ ì•Œ ìˆ˜ ìˆê²Œë¨\n",
        "  * `char_to_token()`\n",
        "    * ìŒì ˆ ìˆ«ìë¥¼ token indexë¡œ ë°”ê¾¸ëŠ” í•¨ìˆ˜\n",
        "\n",
        "* ì˜ˆì™¸ì²˜ë¦¬\n",
        "  * max_lengthë¥¼ ë„˜ì–´ì„œëŠ” ìœ„ì¹˜ì— ì •ë‹µ tokenì´ ìˆìœ¼ë©´ í•™ìŠµì´ ë˜ì§€ ì•Šê¸° ë•Œë¬¸ì— ì˜ˆì™¸ì²˜ë¦¬í•¨"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "044VIInyosjR"
      },
      "source": [
        "def add_token_positions(encodings, answers):\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "    # ì´ì œ ìŒì ˆ indexë¥¼ token indexì™€ mappingí•˜ëŠ” ì‘ì—…\n",
        "    for i in range(len(answers)):\n",
        "        # tokenizerì˜ char_to_token í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ë©´ ìŒì ˆ ìˆ«ìë¥¼ token indexë¡œ ë°”ê¿”ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "        start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))\n",
        "        end_positions.append(encodings.char_to_token(i, answers[i]['answer_end']))\n",
        "        # ì•„ë˜ ë¶€ë¶„ì€ truncationì„ ìœ„í•œ ê³¼ì •ì…ë‹ˆë‹¤.\n",
        "        # if start position is None, the answer passage has been truncated\n",
        "        if start_positions[-1] is None:\n",
        "            start_positions[-1] = tokenizer.model_max_length\n",
        "\n",
        "        # if end position is None, the 'char_to_token' function points to the space before the correct token - > add + 1\n",
        "        if end_positions[-1] is None:\n",
        "            end_positions[-1] = encodings.char_to_token(i, answers[i]['answer_end'] + 1)\n",
        "\n",
        "        # ì¶”ê°€ëœ ì˜ˆì™¸ ì²˜ë¦¬, ì˜ˆë¥¼ë“¤ì–´ì„œ tokenizerì™€ model inputì˜ max_lengthê°€ 512ì¸ë°, startì™€ end positionì´ 600ê³¼ 610 ì´ë©´ ë‘˜ë‹¤ max_lengthë¡œ ë³€ê²½í•´ì•¼í•¨.\n",
        "        # ì–´ì°¨í”¼ max_lengthê°€ 512ì¸ ëª¨ë¸ì€ ì •ë‹µì„ ë³¼ ìˆ˜ ì—†ìŒ.\n",
        "        if start_positions[-1] is None or start_positions[-1] > tokenizer.model_max_length:\n",
        "            start_positions[-1] = tokenizer.model_max_length\n",
        "        \n",
        "        if end_positions[-1] is None or end_positions[-1] > tokenizer.model_max_length:\n",
        "            end_positions[-1] = tokenizer.model_max_length\n",
        "\n",
        "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
        "    return encodings\n",
        "\n",
        "train_encodings = add_token_positions(train_encodings, train_answers)\n",
        "val_encodings = add_token_positions(val_encodings, val_answers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxCAB5xmqF2m"
      },
      "source": [
        "* return ê°’\n",
        "  * tokenizing ëœ ê²°ê³¼, ì •ë‹µì— ëŒ€í•œ token index ì •ë³´(labelë¡œ ì‚¬ìš©)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNx--QzTqEOg"
      },
      "source": [
        "import torch\n",
        "\n",
        "class SquadDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings.input_ids)\n",
        "\n",
        "train_dataset = SquadDataset(train_encodings)\n",
        "val_dataset = SquadDataset(val_encodings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-Rn71xdqYXQ"
      },
      "source": [
        "* `BertForQuestionAnswering`\n",
        "  * ê¸°ê³„ë…í•´ë¥¼ ìœ„í•´ huggingfaceì—ì„œ ì œê³µ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtJrJuNdqFEi"
      },
      "source": [
        "from transformers import BertForQuestionAnswering\n",
        "model = BertForQuestionAnswering.from_pretrained(MODEL_NAME)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzRPNt68qSMf"
      },
      "source": [
        "from transformers import BertForQuestionAnswering, Trainer, TrainingArguments\n",
        "import sys\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    num_train_epochs=1,              # total number of training epochs\n",
        "    per_device_train_batch_size=8,  # batch size per device during training\n",
        "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        "    logging_steps=100,\n",
        "    learning_rate=3e-5,\n",
        "    save_total_limit=5\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OF6N39_mqTbt"
      },
      "source": [
        "model = BertForQuestionAnswering.from_pretrained(MODEL_NAME)\n",
        "model.to(device)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated ğŸ¤— Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=val_dataset            # evaluation dataset\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehH9bYTqqUWL"
      },
      "source": [
        "trainer.train() # 1 epochì— ëŒ€ëµ 1ì‹œê°„ ì •ë„ ê±¸ë¦½ë‹ˆë‹¤."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3Ngp-LsqVly"
      },
      "source": [
        "trainer.save_model(\".\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgsTM0b_qWQD"
      },
      "source": [
        "from transformers import pipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3JUBRxsqntZ"
      },
      "source": [
        "* pipelineì„ ì‚¬ìš©í•˜ì—¬ inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qM2IMqxTqW3U"
      },
      "source": [
        "nlp = pipeline(\"question-answering\", model=model, tokenizer=tokenizer, device=0)\n",
        "\n",
        "context = r\"\"\"\n",
        "ì´ìˆœì‹ (æèˆœè‡£, 1545ë…„ 4ì›” 28ì¼ ~ 1598ë…„ 12ì›” 16ì¼ (ìŒë ¥ 11ì›” 19ì¼))ì€ ì¡°ì„  ì¤‘ê¸°ì˜ ë¬´ì‹ ì´ì—ˆë‹¤.\n",
        "ë³¸ê´€ì€ ë•ìˆ˜(å¾·æ°´), ìëŠ” ì—¬í•´(æ±è«§), ì‹œí˜¸ëŠ” ì¶©ë¬´(å¿ æ­¦)ì˜€ìœ¼ë©°, í•œì„± ì¶œì‹ ì´ì—ˆë‹¤.\n",
        "ë¬¸ë°˜ ê°€ë¬¸ ì¶œì‹ ìœ¼ë¡œ 1576ë…„(ì„ ì¡° 9ë…„) ë¬´ê³¼(æ­¦ç§‘)ì— ê¸‰ì œí•˜ì—¬ ê·¸ ê´€ì§ì´ ë™êµ¬ë¹„ë³´ ê¶Œê´€, í›ˆë ¨ì› ë´‰ì‚¬, ë°œí¬ì§„ ìˆ˜êµ°ë§Œí˜¸, ì¡°ì‚°ë³´ ë§Œí˜¸, ì „ë¼ì¢Œë„ ìˆ˜êµ°ì ˆë„ì‚¬ë¥¼ ê±°ì³ ì •í—ŒëŒ€ë¶€ ì‚¼ë„ìˆ˜êµ°í†µì œì‚¬ì— ì´ë¥´ë €ë‹¤.\n",
        "\"\"\"\n",
        "\n",
        "print(nlp(question=\"ì´ìˆœì‹ ì´ íƒœì–´ë‚œ ë‚ ì§œëŠ”?\", context=context))\n",
        "# {'score': 0.1287170797586441, 'start': 25, 'end': 40, 'answer': '1598ë…„ 12ì›” 16ì¼ ('}\n",
        "print(nlp(question=\"ì´ìˆœì‹ ì˜ ë³¸ê´€ì€?\", context=context))\n",
        "# {'score': 0.5768629908561707, 'start': 72, 'end': 75, 'answer': 'ë•ìˆ˜('}\n",
        "print(nlp(question=\"ì´ìˆœì‹ ì˜ ì‹œí˜¸ëŠ”?\", context=context))\n",
        "# {'score': 0.4932706952095032, 'start': 95, 'end': 98, 'answer': 'ì¶©ë¬´('}\n",
        "print(nlp(question=\"ì´ìˆœì‹ ì˜ ê³ í–¥ì€?\", context=context))\n",
        "# {'score': 0.1482970416545868, 'start': 106, 'end': 114, 'answer': 'í•œì„± ì¶œì‹ ì´ì—ˆë‹¤'}\n",
        "print(nlp(question=\"ì´ìˆœì‹ ì˜ ë§ˆì§€ë§‰ ì§ì±…ì€?\", context=context))\n",
        "# {'score': 0.038699887692928314, 'start': 214, 'end': 222, 'answer': 'ì‚¼ë„ìˆ˜êµ°í†µì œì‚¬ì—'}"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}