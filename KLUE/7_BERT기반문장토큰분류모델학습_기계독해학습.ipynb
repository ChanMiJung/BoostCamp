{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "7_BERT기반문장토큰분류모델학습_기계독해학습.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nK2yZYvamcB4"
      },
      "source": [
        "# 기계독해 모델 학습"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EznhcjBLmhQj"
      },
      "source": [
        "* 질문에 대한 답변이 포함된 문서가 주어졌을 때, 문서 token 내에서 정답의 위치를 파악하는 task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-7C415amV3Y"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YEIqMJMmtDz"
      },
      "source": [
        "import torch\n",
        "import sys"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tCJ7HrOmtkX"
      },
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhK7sVbmmuHg"
      },
      "source": [
        "!mkdir dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgBfyj3Tmv1x"
      },
      "source": [
        "* KorQuAD\n",
        "  * LGCNS에서 공개한 한국어 기계독해 dataset\n",
        "  * json 형태로 되어있음"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JSiD23omvAX"
      },
      "source": [
        "!wget https://korquad.github.io/dataset/KorQuAD_v1.0_train.json\n",
        "!wget https://korquad.github.io/dataset/KorQuAD_v1.0_dev.json\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DePITjUBm28C"
      },
      "source": [
        "!mv /content/KorQuAD_v1.0_train.json dataset\n",
        "!mv /content/KorQuAD_v1.0_dev.json dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdWsRWIwm-J0"
      },
      "source": [
        "* json parsing\n",
        "  * 데이터셋을 학습하기 편한 형태로 변환\n",
        "  * paragraph, question, answer으로 분류"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAdtztm6m3wD"
      },
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "def read_squad(path):\n",
        "    path = Path(path)\n",
        "    with open(path, 'rb') as f:\n",
        "        squad_dict = json.load(f)\n",
        "\n",
        "    contexts = []\n",
        "    questions = []\n",
        "    answers = []\n",
        "    for group in squad_dict['data']:\n",
        "        for passage in group['paragraphs']:\n",
        "            context = passage['context']\n",
        "            for qa in passage['qas']:\n",
        "                question = qa['question']\n",
        "                for answer in qa['answers']:\n",
        "                    contexts.append(context)\n",
        "                    questions.append(question)\n",
        "                    answers.append(answer)\n",
        "\n",
        "    return contexts, questions, answers\n",
        "\n",
        "\n",
        "train_contexts, train_questions, train_answers = read_squad('dataset/KorQuAD_v1.0_train.json')\n",
        "val_contexts, val_questions, val_answers = read_squad('dataset/KorQuAD_v1.0_dev.json')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2w5cvORnFBD"
      },
      "source": [
        "print(train_contexts[52471])\n",
        "# 사우스런던 브릭스톤 태생 보위는 어릴 적 음악에 대한 관심을 키워왔으며, 결국에는 예술, 음악, 디자인을 배워 전문적인 음악가 경력을 1963년부터 착수했다. 〈Space Oddity〉는 1969년 7월 발표 뒤 영국 음반 차트에서 상위 5위에 오른 그의 첫 싱글이다. 실험 활동을 거친 그는 1972년 음악을 재개, 자신의 글램록 시기 동안 이색적이고 양성적인 제2의 자아인 지기 스타더스트로 활동을 이어나갔다. 성공을 거둔 싱글 〈Starman〉과 전 세계적인 인기를 끈 음반 《The Rise and Fall of Ziggy Stardust and the Spiders from Mars》으로 캐릭터를 내세운 보위는 1975년 \"플라스틱 솔\"을 캐릭터화시켜 자신을 철저히 바꾼다. 이 행동은 당초 영국에서 그의 열혈팬의 반발을 샀으나 미국에서는 싱글 〈Fame〉과 음반 《Young Americans》을 통해 처음으로 메이저한 성공을 거두게 된다. 1976년 보위는 컬트 영화 《지구에 떨어진 사나이》에 출연하고 음반 《Station to Station》을 발표한다. 이듬해에는 일렉트릭 음악을 접목한 음반 《Low》 (1977)을 발표하면서 음악적 예상을 깨뜨렸다. 이 음반은 브라이언 이노와의 세 번의 협업 중 첫 번째로 이는 이후 \"베를린 삼부작\"으로 일컬어진다. 뒤를 이어 발표된 《\"Heroes\"》 (1977)와 《Lodger》 (1979)는 영국 차트 상위 5위에 진입, 지속적인 극찬을 받았다.\n",
        "print(len(train_contexts[52471]))\n",
        "# 734"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZRcd5w3nO4J"
      },
      "source": [
        "print(train_questions[52471])\n",
        "# 보위가 1977년 일렉트릭 음악을 접목하여 발표한 음반은?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7z7p2v2wnZV5"
      },
      "source": [
        "* answer_start : 정답이 시작되는 음절 순서(index)\n",
        "  * end는 정답의 length와 answer_start로 알 수 있음"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhRPu0EvnWkC"
      },
      "source": [
        "print(train_answers[52471]) # 본문 내에서 정답이 시작되는 음절 순서가 'answer start'에 저장되어 있습니다.\n",
        "# {'text': 'Low', 'answer_start': 568}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRiFdO4HnwFy"
      },
      "source": [
        "* 모델의 input : answer start position, answer end position\n",
        "  * `gold_text` : 정답 text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0r7xT-8niBz"
      },
      "source": [
        "def add_end_idx(answers, contexts):\n",
        "    for answer, context in zip(answers, contexts):\n",
        "        # 모델 학습을 위해 정답 데이터를 만들겠습니다.\n",
        "        # 정답 데이터는 start음절과 end 음절로 구성되어 있습니다.\n",
        "        # 모델은 전체 토큰 중에서 start token과 end token을 찾아내는 것을 목표로 학습하게 됩니다.\n",
        "        gold_text = answer['text']\n",
        "        start_idx = answer['answer_start']\n",
        "        end_idx = start_idx + len(gold_text)\n",
        "        \n",
        "\n",
        "        # sometimes squad answers are off by a character or two – fix this\n",
        "        # 실제 본문에서 해당 음절 번호로 잘라냈을 때, 정답과 같은지 검사해서 start, end를 보정합니다 :-)\n",
        "        # '이순신은 조선 중기의 무신이다' -> '이순신' -> start: 0, end: 4\n",
        "        if context[start_idx:end_idx] == gold_text:\n",
        "            answer['answer_end'] = end_idx\n",
        "        elif context[start_idx-1:end_idx-1] == gold_text:\n",
        "            answer['answer_start'] = start_idx - 1\n",
        "            answer['answer_end'] = end_idx - 1     # When the gold label is off by one character\n",
        "        elif context[start_idx-2:end_idx-2] == gold_text:\n",
        "            answer['answer_start'] = start_idx - 2\n",
        "            answer['answer_end'] = end_idx - 2     # When the gold label is off by two characters\n",
        "    return answers\n",
        "\n",
        "train_answers = add_end_idx(train_answers, train_contexts)\n",
        "val_answers = add_end_idx(val_answers, val_contexts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZP2TD6Hnuz8"
      },
      "source": [
        "from transformers import AutoModel, AutoTokenizer, BertTokenizer\n",
        "MODEL_NAME = \"bert-base-multilingual-cased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "train_encodings = tokenizer(train_contexts, train_questions, truncation=True, padding=True)\n",
        "val_encodings = tokenizer(val_contexts, val_questions, truncation=True, padding=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4lUvfxtouUp"
      },
      "source": [
        "* 기계독해의 핵심\n",
        "  * 구축한 data는 음절 index를 가져옴\n",
        "  * BERT 모델은 wordPiece 단위로 되어있음\n",
        "  * 음절 숫자를 token index로 바꿔주어 학습데이터가 start position에 대한 정보와 end position에 대한 정보를 token 단위로 가지게 됨\n",
        "    * 해당 token index가 정답의 시작임을 알 수 있게됨\n",
        "  * `char_to_token()`\n",
        "    * 음절 숫자를 token index로 바꾸는 함수\n",
        "\n",
        "* 예외처리\n",
        "  * max_length를 넘어서는 위치에 정답 token이 있으면 학습이 되지 않기 때문에 예외처리함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "044VIInyosjR"
      },
      "source": [
        "def add_token_positions(encodings, answers):\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "    # 이제 음절 index를 token index와 mapping하는 작업\n",
        "    for i in range(len(answers)):\n",
        "        # tokenizer의 char_to_token 함수를 호출하면 음절 숫자를 token index로 바꿔줄 수 있습니다.\n",
        "        start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))\n",
        "        end_positions.append(encodings.char_to_token(i, answers[i]['answer_end']))\n",
        "        # 아래 부분은 truncation을 위한 과정입니다.\n",
        "        # if start position is None, the answer passage has been truncated\n",
        "        if start_positions[-1] is None:\n",
        "            start_positions[-1] = tokenizer.model_max_length\n",
        "\n",
        "        # if end position is None, the 'char_to_token' function points to the space before the correct token - > add + 1\n",
        "        if end_positions[-1] is None:\n",
        "            end_positions[-1] = encodings.char_to_token(i, answers[i]['answer_end'] + 1)\n",
        "\n",
        "        # 추가된 예외 처리, 예를들어서 tokenizer와 model input의 max_length가 512인데, start와 end position이 600과 610 이면 둘다 max_length로 변경해야함.\n",
        "        # 어차피 max_length가 512인 모델은 정답을 볼 수 없음.\n",
        "        if start_positions[-1] is None or start_positions[-1] > tokenizer.model_max_length:\n",
        "            start_positions[-1] = tokenizer.model_max_length\n",
        "        \n",
        "        if end_positions[-1] is None or end_positions[-1] > tokenizer.model_max_length:\n",
        "            end_positions[-1] = tokenizer.model_max_length\n",
        "\n",
        "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
        "    return encodings\n",
        "\n",
        "train_encodings = add_token_positions(train_encodings, train_answers)\n",
        "val_encodings = add_token_positions(val_encodings, val_answers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxCAB5xmqF2m"
      },
      "source": [
        "* return 값\n",
        "  * tokenizing 된 결과, 정답에 대한 token index 정보(label로 사용)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNx--QzTqEOg"
      },
      "source": [
        "import torch\n",
        "\n",
        "class SquadDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings.input_ids)\n",
        "\n",
        "train_dataset = SquadDataset(train_encodings)\n",
        "val_dataset = SquadDataset(val_encodings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-Rn71xdqYXQ"
      },
      "source": [
        "* `BertForQuestionAnswering`\n",
        "  * 기계독해를 위해 huggingface에서 제공"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtJrJuNdqFEi"
      },
      "source": [
        "from transformers import BertForQuestionAnswering\n",
        "model = BertForQuestionAnswering.from_pretrained(MODEL_NAME)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzRPNt68qSMf"
      },
      "source": [
        "from transformers import BertForQuestionAnswering, Trainer, TrainingArguments\n",
        "import sys\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    num_train_epochs=1,              # total number of training epochs\n",
        "    per_device_train_batch_size=8,  # batch size per device during training\n",
        "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        "    logging_steps=100,\n",
        "    learning_rate=3e-5,\n",
        "    save_total_limit=5\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OF6N39_mqTbt"
      },
      "source": [
        "model = BertForQuestionAnswering.from_pretrained(MODEL_NAME)\n",
        "model.to(device)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=val_dataset            # evaluation dataset\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehH9bYTqqUWL"
      },
      "source": [
        "trainer.train() # 1 epoch에 대략 1시간 정도 걸립니다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3Ngp-LsqVly"
      },
      "source": [
        "trainer.save_model(\".\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgsTM0b_qWQD"
      },
      "source": [
        "from transformers import pipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3JUBRxsqntZ"
      },
      "source": [
        "* pipeline을 사용하여 inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qM2IMqxTqW3U"
      },
      "source": [
        "nlp = pipeline(\"question-answering\", model=model, tokenizer=tokenizer, device=0)\n",
        "\n",
        "context = r\"\"\"\n",
        "이순신(李舜臣, 1545년 4월 28일 ~ 1598년 12월 16일 (음력 11월 19일))은 조선 중기의 무신이었다.\n",
        "본관은 덕수(德水), 자는 여해(汝諧), 시호는 충무(忠武)였으며, 한성 출신이었다.\n",
        "문반 가문 출신으로 1576년(선조 9년) 무과(武科)에 급제하여 그 관직이 동구비보 권관, 훈련원 봉사, 발포진 수군만호, 조산보 만호, 전라좌도 수군절도사를 거쳐 정헌대부 삼도수군통제사에 이르렀다.\n",
        "\"\"\"\n",
        "\n",
        "print(nlp(question=\"이순신이 태어난 날짜는?\", context=context))\n",
        "# {'score': 0.1287170797586441, 'start': 25, 'end': 40, 'answer': '1598년 12월 16일 ('}\n",
        "print(nlp(question=\"이순신의 본관은?\", context=context))\n",
        "# {'score': 0.5768629908561707, 'start': 72, 'end': 75, 'answer': '덕수('}\n",
        "print(nlp(question=\"이순신의 시호는?\", context=context))\n",
        "# {'score': 0.4932706952095032, 'start': 95, 'end': 98, 'answer': '충무('}\n",
        "print(nlp(question=\"이순신의 고향은?\", context=context))\n",
        "# {'score': 0.1482970416545868, 'start': 106, 'end': 114, 'answer': '한성 출신이었다'}\n",
        "print(nlp(question=\"이순신의 마지막 직책은?\", context=context))\n",
        "# {'score': 0.038699887692928314, 'start': 214, 'end': 222, 'answer': '삼도수군통제사에'}"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}