{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5_BERTê¸°ë°˜ë‹¨ì¼ë¬¸ì¥ë¶„ë¥˜ëª¨ë¸í•™ìŠµ_ë‹¨ì¼ë¬¸ì¥ë¶„ë¥˜.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7KnffUBeEFe"
      },
      "source": [
        "# BERTë¥¼ í™œìš©í•œ ë‹¨ì¼ ë¬¸ì¥ ë¶„ë¥˜ ì‹¤ìŠµ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W12-wzuqevzU"
      },
      "source": [
        "* datasets\n",
        "  * huggingfaceì—ì„œ ì¶œì‹œí•œ dataset ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
        "  * ì›í•˜ëŠ” datasetì´ë¦„ í˜¸ì¶œì„ í†µí•´ datasetì„ ê°€ì ¸ì˜´"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWFz3cvLeJ3f"
      },
      "source": [
        "# !pip install transformers\n",
        "# !pip install datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g83nvBVze-7Z"
      },
      "source": [
        "import torch\n",
        "import datasets\n",
        "import sys"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUQ1Z5awfDAZ"
      },
      "source": [
        "* ë””ë°”ì´ìŠ¤ ì„¤ì •"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lC2hNZpfACR"
      },
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqPL-L-1fGye"
      },
      "source": [
        "* ì €ì¥ëœ dataset listí™•ì¸"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00O3uPWgfAqo"
      },
      "source": [
        "# ì‚¬ìš©ê°€ëŠ¥í•œ dataset list ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "dataset_list = datasets.list_datasets()\n",
        "\n",
        "# dataset list í™•ì¸\n",
        "for datas in dataset_list:\n",
        "    if 'ko' in datas:\n",
        "        print(datas)\n",
        "\n",
        "'''\n",
        "kor_3i4k\n",
        "kor_hate\n",
        "kor_ner\n",
        "kor_nli\n",
        "kor_nlu\n",
        "kor_qpair\n",
        "kor_sae\n",
        "kor_sarcasm\n",
        "squad_kor_v1\n",
        "squad_kor_v2\n",
        "KETI-AIR/kor_corpora\n",
        "KETI-AIR/korquad\n",
        "abwicke/koplo\n",
        "huggingartists/aikko\n",
        "huggingartists/boris-grebenshikov\n",
        "huggingartists/krept-and-konan-bugzy-malone-sl-morisson-abra-cadabra-rv-and-snap-capone\n",
        "huggingartists/lyapis-trubetskoy\n",
        "huggingartists/max-korzh\n",
        "roskoN/dailydialog\n",
        "roskoN/dstc8-reddit-corpus\n",
        "toriving/kosimcse\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FE9EuMQhfOP0"
      },
      "source": [
        "* 'nsmc'\n",
        "  * ë„¤ì´ë²„ì—ì„œ ì¶œì‹œí•œ ì˜í™” ëŒ“ê¸€ì˜ ê°ì • ë¶„ë¥˜ ë°ì´í„°ì…‹\n",
        "\n",
        "* ë‹¨ì¼ ë¬¸ì¥ ë¶„ë¥˜ taskì—ëŠ” 'nsmc' ì™¸ì—ë„ 'hate', 'sarcasm'ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ\n",
        "\n",
        "* datasetì€ dictionary í˜•íƒœë¡œ trainê³¼ testê°€ ë‚˜ë‰˜ì–´ì ¸ ìˆìŒ\n",
        "  * 'document' : sentence\n",
        "  * 'label' : ê°ì •(ê¸ì •/ë¶€ì •)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTRtm8_tfBkN"
      },
      "source": [
        "# nsmc ë°ì´í„° ë¡œë“œ\n",
        "dataset = datasets.load_dataset('nsmc') # nsmc, hate, sarcasm\n",
        "\n",
        "# ë°ì´í„°ì…‹ êµ¬ì¡° í™•ì¸\n",
        "print(dataset)\n",
        "'''\n",
        "DatasetDict({\n",
        "    train: Dataset({\n",
        "        features: ['id', 'document', 'label'],\n",
        "        num_rows: 150000\n",
        "    })\n",
        "    test: Dataset({\n",
        "        features: ['id', 'document', 'label'],\n",
        "        num_rows: 50000\n",
        "    })\n",
        "})\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMzcDB1RfV6B"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdCFP3ohgCCP"
      },
      "source": [
        "* datasetì„ ë‹¤ë£¨ê¸° í¸í•œ í˜•íƒœë¡œ ë³€í™˜"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79LimqGJfX6A"
      },
      "source": [
        "# í•„ìš”í•œ ë°ì´í„°ì¸ documentì™€ label ì •ë³´ë§Œ pandasë¼ì´ë¸ŒëŸ¬ë¦¬ DataFrame í˜•ì‹ìœ¼ë¡œ ë³€í™˜\n",
        "train_data = pd.DataFrame({\"document\":dataset['train']['document'], \"label\":dataset['train']['label'],})\n",
        "test_data = pd.DataFrame({\"document\":dataset['test']['document'], \"label\":dataset['test']['label'],})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66Nsqoo3fhwt"
      },
      "source": [
        "# ë°ì´í„°ì…‹ ê°¯ìˆ˜ í™•ì¸\n",
        "print('í•™ìŠµ ë°ì´í„°ì…‹ : {}'.format(len(train_data)))\n",
        "# í•™ìŠµ ë°ì´í„°ì…‹ : 150000\n",
        "print('í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ : {}'.format(len(test_data)))\n",
        "# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ : 50000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CIThWk-d_3P"
      },
      "source": [
        "# ë°ì´í„°ì…‹ ë‚´ìš© í™•ì¸\n",
        "train_data[:5]\n",
        "'''\n",
        "\t                                                                              document\tlabel\n",
        "0\t                                                      ì•„ ë”ë¹™.. ì§„ì§œ ì§œì¦ë‚˜ë„¤ìš” ëª©ì†Œë¦¬\t    0\n",
        "1\t                              í ...í¬ìŠ¤í„°ë³´ê³  ì´ˆë”©ì˜í™”ì¤„....ì˜¤ë²„ì—°ê¸°ì¡°ì°¨ ê°€ë³ì§€ ì•Šêµ¬ë‚˜\t    1\n",
        "2\t                                                    ë„ˆë¬´ì¬ë°“ì—ˆë‹¤ê·¸ë˜ì„œë³´ëŠ”ê²ƒì„ì¶”ì²œí•œë‹¤\t    0\n",
        "3\t                                     êµë„ì†Œ ì´ì•¼ê¸°êµ¬ë¨¼ ..ì†”ì§íˆ ì¬ë¯¸ëŠ” ì—†ë‹¤..í‰ì  ì¡°ì •\t    0\n",
        "4\tì‚¬ì´ëª¬í˜ê·¸ì˜ ìµì‚´ìŠ¤ëŸ° ì—°ê¸°ê°€ ë‹ë³´ì˜€ë˜ ì˜í™”!ìŠ¤íŒŒì´ë”ë§¨ì—ì„œ ëŠ™ì–´ë³´ì´ê¸°ë§Œ í–ˆë˜ ì»¤ìŠ¤í‹´ ...\t    1\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhNtGwLMf74J"
      },
      "source": [
        "test_data[:5]\n",
        "'''\n",
        "\t                                                                      document\tlabel\n",
        "0\t                                                                         êµ³ ã…‹\t    1\n",
        "1\t                                                          GDNTOPCLASSINTHECLUB\t    0\n",
        "2\t                ë­ì•¼ ì´ í‰ì ë“¤ì€.... ë‚˜ì˜ì§„ ì•Šì§€ë§Œ 10ì  ì§œë¦¬ëŠ” ë”ë”ìš± ì•„ë‹ˆì–ì•„\t    0\n",
        "3\t                          ì§€ë£¨í•˜ì§€ëŠ” ì•Šì€ë° ì™„ì „ ë§‰ì¥ì„... ëˆì£¼ê³  ë³´ê¸°ì—ëŠ”....\t    0\n",
        "4\t3Dë§Œ ì•„ë‹ˆì—ˆì–´ë„ ë³„ ë‹¤ì„¯ ê°œ ì¤¬ì„í…ë°.. ì™œ 3Dë¡œ ë‚˜ì™€ì„œ ì œ ì‹¬ê¸°ë¥¼ ë¶ˆí¸í•˜ê²Œ í•˜ì£ ??\t    0\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52bU8-Ukg0vM"
      },
      "source": [
        "* ë°ì´í„° ì „ì²˜ë¦¬ ê³¼ì •\n",
        "  * ì¤‘ë³µ ë°ì´í„° ì œê±°"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdaspzRfgHaN"
      },
      "source": [
        "# ë°ì´í„° ì¤‘ë³µì„ ì œì™¸í•œ ê°¯ìˆ˜ í™•ì¸\n",
        "print(\"í•™ìŠµë°ì´í„° : \",train_data['document'].nunique(),\" ë¼ë²¨ : \",train_data['label'].nunique())\n",
        "# í•™ìŠµë°ì´í„° :  146183  ë¼ë²¨ :  2\n",
        "print(\"ë°ìŠ¤íŠ¸ ë°ì´í„° : \",test_data['document'].nunique(),\" ë¼ë²¨ : \",test_data['label'].nunique())\n",
        "# ë°ìŠ¤íŠ¸ ë°ì´í„° :  49158  ë¼ë²¨ :  2\n",
        "\n",
        "# ì¤‘ë³µ ë°ì´í„° ì œê±°\n",
        "train_data.drop_duplicates(subset=['document'], inplace= True)\n",
        "test_data.drop_duplicates(subset=['document'], inplace= True)\n",
        "\n",
        "# ë°ì´í„°ì…‹ ê°¯ìˆ˜ í™•ì¸\n",
        "print('ì¤‘ë³µ ì œê±° í›„ í•™ìŠµ ë°ì´í„°ì…‹ : {}'.format(len(train_data)))\n",
        "# ì¤‘ë³µ ì œê±° í›„ í•™ìŠµ ë°ì´í„°ì…‹ : 146183\n",
        "print('ì¤‘ë³µ ì œê±° í›„ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ : {}'.format(len(test_data)))\n",
        "# ì¤‘ë³µ ì œê±° í›„ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ : 49158"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJr8XoA0gxKS"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCbP_1QihIVu"
      },
      "source": [
        "* ë°ì´í„° ì „ì²˜ë¦¬\n",
        "  * null ë°ì´í„° ì œê±°"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfzsFmZ6gx6x"
      },
      "source": [
        "# null ë°ì´í„° ì œê±°\n",
        "train_data['document'].replace('', np.nan, inplace=True)\n",
        "test_data['document'].replace('', np.nan, inplace=True)\n",
        "train_data = train_data.dropna(how = 'any')\n",
        "test_data = test_data.dropna(how = 'any')\n",
        "\n",
        "print('null ì œê±° í›„ í•™ìŠµ ë°ì´í„°ì…‹ : {}'.format(len(train_data)))\n",
        "# null ì œê±° í›„ í•™ìŠµ ë°ì´í„°ì…‹ : 146182\n",
        "print('null ì œê±° í›„ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ : {}'.format(len(test_data)))\n",
        "# null ì œê±° í›„ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ : 49157"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fh2rISfBgzAq"
      },
      "source": [
        "print(train_data['document'][0])\n",
        "# ì•„ ë”ë¹™.. ì§„ì§œ ì§œì¦ë‚˜ë„¤ìš” ëª©ì†Œë¦¬\n",
        "print(train_data['label'][0])\n",
        "# 0\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-G-ueuchCT8"
      },
      "source": [
        "* ë°ì´í„° ì „ì²˜ë¦¬\n",
        "  * outlier ì œê±°"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fceDNficg5Or"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "#í•™ìŠµ ë¦¬ë·° ê¸¸ì´ì¡°ì‚¬\n",
        "print('í•™ìŠµ ë¬¸ì¥ ìµœëŒ€ ê¸¸ì´ :',max(len(l) for l in train_data['document']))\n",
        "# í•™ìŠµ ë¬¸ì¥ ìµœëŒ€ ê¸¸ì´ : 146\n",
        "print('í•™ìŠµ ë¬¸ì¥ì˜ í‰ê·  ê¸¸ì´ :',sum(map(len, train_data['document']))/len(train_data['document']))\n",
        "# í•™ìŠµ ë¬¸ì¥ì˜ í‰ê·  ê¸¸ì´ : 35.981338331668745\n",
        "\n",
        "plt.hist([len(s) for s in train_data['document']], bins=50)\n",
        "plt.xlabel('length of data')\n",
        "plt.ylabel('number of data')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVtiPTl7hhP_"
      },
      "source": [
        "* pretrainedëœ multilingual model ì‚¬ìš©"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgAz-DeMhOmz"
      },
      "source": [
        "# Store the model we want to use\n",
        "from transformers import AutoModel, AutoTokenizer, BertTokenizer\n",
        "MODEL_NAME = \"bert-base-multilingual-cased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0RRYwhWhseP"
      },
      "source": [
        "* tokenizerì— list í˜•íƒœë¡œ ì…ë ¥í•˜ë©´ list í˜•íƒœë¡œ ë°˜í™˜ë¨"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzsTqt-LhPkK"
      },
      "source": [
        "tokenized_train_sentences = tokenizer(\n",
        "    list(train_data['document']),\n",
        "    return_tensors=\"pt\",\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    add_special_tokens=True,\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H21PdmMlhdYR"
      },
      "source": [
        "print(tokenized_train_sentences[0])\n",
        "# Encoding(num_tokens=142, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
        "print(tokenized_train_sentences[0].tokens)\n",
        "# ['[CLS]', 'ì•„', 'ë”', '##ë¹™', '.', '.', 'ì§„', '##ì§œ', 'ì§œ', '##ì¦', '##ë‚˜', '##ë„¤', '##ìš”', 'ëª©', '##ì†Œ', '##ë¦¬', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
        "print(tokenized_train_sentences[0].ids)\n",
        "# [101, 9519, 9074, 119005, 119, 119, 9708, 119235, 9715, 119230, 16439, 77884, 48549, 9284, 22333, 12692, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "print(tokenized_train_sentences[0].attention_mask)\n",
        "# [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BovTN6n4hqkb"
      },
      "source": [
        "tokenized_test_sentences = tokenizer(\n",
        "    list(test_data['document']),\n",
        "    return_tensors=\"pt\",\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    add_special_tokens=True,\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sA_hEBmJhrdo"
      },
      "source": [
        "train_label = train_data['label'].values\n",
        "test_label = test_data['label'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opQhv_8Nhxeu"
      },
      "source": [
        "print(train_label[0]) # 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-IIQXh7h9PZ"
      },
      "source": [
        "* ì‹¤ì œ modelì— ì…ë ¥í•˜ê¸° ìœ„í•œ êµ¬ì¡°ì ì¸ í˜•íƒœë¡œ ë³€í™˜\n",
        "  * huggingfaceì˜ datasetê³¼ modelì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš° í˜•íƒœê°€ ëŒ€ë¶€ë¶„ ì¼ì¹˜í•¨\n",
        "\n",
        "* `__getitem__()`\n",
        "  * step ë§ëŠ” datasetì„ modelê°€ì ¸ì˜¤ëŠ” ì—­í• "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsiRufJNhyhS"
      },
      "source": [
        "class SingleSentDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx): \n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCc0-wQDh6IF"
      },
      "source": [
        "train_dataset = SingleSentDataset(tokenized_train_sentences, train_label)\n",
        "test_dataset = SingleSentDataset(tokenized_test_sentences, test_label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eV2EVYgBit4I"
      },
      "source": [
        "* `BertForSequenceClassification`\n",
        "  * ë¶„ë¥˜ë¥¼ ìœ„í•´ BERT ìœ„ì— ë¶€ì°©í•˜ëŠ” classicficationì„ ìœ„í•œ headë¥¼ ì œê³µ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUJuAxIjh6oo"
      },
      "source": [
        "from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
        "# ë¬¸ì¥ ë¶„ë¥˜ë¥¼ ìœ„í•´ì„  BERT ìœ„ì— classificationì„ ìœ„í•œ headë¥¼ ë¶€ì°©í•´ì•¼ í•©ë‹ˆë‹¤.\n",
        "# í•´ë‹¹ ë¶€ë¶„ì„ transformersì—ì„œëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ í•˜ë‚˜ë§Œ í˜¸ì¶œí•˜ë©´ ë©ë‹ˆë‹¤! :-)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoyL_z3Apd0V"
      },
      "source": [
        "* `warmup_steps`\n",
        "  * learning rateì˜ ë²”ìœ„\n",
        "  * learning rateë¥¼ ì¡°ì ˆí•¨"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAaZOlmth7MO"
      },
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    num_train_epochs=1,              # total number of training epochs\n",
        "    per_device_train_batch_size=32,  # batch size per device during training\n",
        "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        "    logging_steps=500,\n",
        "    save_steps=500,\n",
        "    save_total_limit=2\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CK09KMBdiRzI"
      },
      "source": [
        "model = BertForSequenceClassification.from_pretrained(MODEL_NAME)\n",
        "model.to(device)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated ğŸ¤— Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ME1YF6W3iSqc"
      },
      "source": [
        "trainer.train() # 1 epochì— ëŒ€ëµ 30ë¶„ ì •ë„ ì†Œìš”ë©ë‹ˆë‹¤ :-)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbrT1kCRqEBQ"
      },
      "source": [
        "* ì„±ëŠ¥ í‰ê°€ë¥¼ ìœ„í•œ í•¨ìˆ˜"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1EwSkLiiky5"
      },
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OVocAS-qLHN"
      },
      "source": [
        "* evaluateê°€ ì‹¤í–‰ë  ë•Œ, `compute_metrics`ì— í•´ë‹¹í•˜ëŠ” í•¨ìˆ˜ë¥¼ ë¶ˆëŸ¬ì™€ì„œ í‰ê°€í•¨"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEI3Kb7yilbJ"
      },
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjgfbSQMip7m"
      },
      "source": [
        "trainer.evaluate(eval_dataset=test_dataset)\n",
        "'''\n",
        "{'eval_accuracy': 0.8609150273613118,\n",
        " 'eval_f1': 0.8648627280453818,\n",
        " 'eval_loss': 0.3183152973651886,\n",
        " 'eval_mem_cpu_alloc_delta': 943706,\n",
        " 'eval_mem_cpu_peaked_delta': 2675650,\n",
        " 'eval_mem_gpu_alloc_delta': 0,\n",
        " 'eval_mem_gpu_peaked_delta': 280995328,\n",
        " 'eval_precision': 0.8452978904257785,\n",
        " 'eval_recall': 0.8853547003358828,\n",
        " 'eval_runtime': 194.1152,\n",
        " 'eval_samples_per_second': 253.236,\n",
        " 'init_mem_cpu_alloc_delta': 48822,\n",
        " 'init_mem_cpu_peaked_delta': 18306,\n",
        " 'init_mem_gpu_alloc_delta': 0,\n",
        " 'init_mem_gpu_peaked_delta': 0}\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7klnRAOqvHE"
      },
      "source": [
        "* trainerë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³  torchë¡œ í•™ìŠµ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrHUhws6qkBm"
      },
      "source": [
        "# native training using torch\n",
        "\n",
        "# model = BertForSequenceClassification.from_pretrained(MODEL_NAME)\n",
        "# model.to(device)\n",
        "# model.train()\n",
        "\n",
        "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# optim = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# for epoch in range(3):\n",
        "#     for batch in train_loader:\n",
        "#         optim.zero_grad()\n",
        "#         input_ids = batch['input_ids'].to(device)\n",
        "#         attention_mask = batch['attention_mask'].to(device)\n",
        "#         labels = batch['labels'].to(device)\n",
        "#         outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "#         loss = outputs[0]\n",
        "#         loss.backward()\n",
        "#         optim.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGYEDGByp_cn"
      },
      "source": [
        "* prediction í•¨ìˆ˜"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JVCGr5-q26P"
      },
      "source": [
        "# predictí•¨ìˆ˜\n",
        "def sentences_predict(sent):\n",
        "    model.eval()\n",
        "    tokenized_sent = tokenizer(\n",
        "            sent,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            add_special_tokens=True,\n",
        "            max_length=128\n",
        "    )\n",
        "    tokenized_sent.to(device)\n",
        "    \n",
        "    with torch.no_grad():# ê·¸ë¼ë””ì—”íŠ¸ ê³„ì‚° ë¹„í™œì„±í™”\n",
        "        outputs = model(\n",
        "            input_ids=tokenized_sent['input_ids'],\n",
        "            attention_mask=tokenized_sent['attention_mask'],\n",
        "            token_type_ids=tokenized_sent['token_type_ids']\n",
        "            )\n",
        "\n",
        "    logits = outputs[0]\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    result = np.argmax(logits)\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flgfEnaurD3t"
      },
      "source": [
        "print(sentences_predict(\"ì˜í™” ê°œì¬ë°Œì–´ ã…‹ã…‹ã…‹ã…‹ã…‹\")) # 1\n",
        "print(sentences_predict(\"ì§„ì§œ ì¬ë¯¸ì—†ë„¤ìš” ã…‹ã…‹\")) # 0\n",
        "print(sentences_predict(\"ë„ˆ ë•Œë¬¸ì— ì§„ì§œ ì§œì¦ë‚˜\")) # 0\n",
        "print(sentences_predict(\"ì •ë§ ì¬ë°Œê³  ì¢‹ì•˜ì–´ìš”.\")) # 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxVvfqpxrDOT"
      },
      "source": [
        "* `pipeline`ì„ ì‚¬ìš©í•˜ë©´ predictionì„ êµ¬í˜„í•˜ì§€ ì•Šì•„ë„ ë¨\n",
        "\n",
        "* modelì„ pipelineì— ì´ë¯¸ ì§€ì •í–ˆê¸° ë•Œë¬¸ì—, í…ŒìŠ¤íŠ¸í•  ë•Œ ë”°ë¡œ ë„£ì–´ì£¼ì§€ ì•Šì•„ë„ ë¨"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fkeiluk4rGyq"
      },
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "nlp_sentence_classif = pipeline('sentiment-analysis',model=model, tokenizer=tokenizer, device=0)\n",
        "\n",
        "print(nlp_sentence_classif('ì˜í™” ê°œì¬ë°Œì–´ ã…‹ã…‹ã…‹ã…‹ã…‹'))\n",
        "# [{'label': 'LABEL_1', 'score': 0.7245705723762512}]\n",
        "print(nlp_sentence_classif('ì§„ì§œ ì¬ë¯¸ì—†ë„¤ìš” ã…‹ã…‹',model= model)) # model ìƒëµ ê°€ëŠ¥\n",
        "# [{'label': 'LABEL_0', 'score': 0.9946942925453186}]\n",
        "print(nlp_sentence_classif('ë„ˆ ë•Œë¬¸ì— ì§„ì§œ ì§œì¦ë‚˜',model= model)) # model ìƒëµ ê°€ëŠ¥\n",
        "# [{'label': 'LABEL_0', 'score': 0.9947050213813782}]\n",
        "print(nlp_sentence_classif('ì •ë§ ì¬ë°Œê³  ì¢‹ì•˜ì–´ìš”.',model= model)) # model ìƒëµ ê°€ëŠ¥\n",
        "# [{'label': 'LABEL_1', 'score': 0.989768385887146}]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}