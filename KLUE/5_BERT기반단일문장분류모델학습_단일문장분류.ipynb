{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5_BERT기반단일문장분류모델학습_단일문장분류.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7KnffUBeEFe"
      },
      "source": [
        "# BERT를 활용한 단일 문장 분류 실습"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W12-wzuqevzU"
      },
      "source": [
        "* datasets\n",
        "  * huggingface에서 출시한 dataset 라이브러리\n",
        "  * 원하는 dataset이름 호출을 통해 dataset을 가져옴"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWFz3cvLeJ3f"
      },
      "source": [
        "# !pip install transformers\n",
        "# !pip install datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g83nvBVze-7Z"
      },
      "source": [
        "import torch\n",
        "import datasets\n",
        "import sys"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUQ1Z5awfDAZ"
      },
      "source": [
        "* 디바이스 설정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lC2hNZpfACR"
      },
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqPL-L-1fGye"
      },
      "source": [
        "* 저장된 dataset list확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00O3uPWgfAqo"
      },
      "source": [
        "# 사용가능한 dataset list 불러오기\n",
        "dataset_list = datasets.list_datasets()\n",
        "\n",
        "# dataset list 확인\n",
        "for datas in dataset_list:\n",
        "    if 'ko' in datas:\n",
        "        print(datas)\n",
        "\n",
        "'''\n",
        "kor_3i4k\n",
        "kor_hate\n",
        "kor_ner\n",
        "kor_nli\n",
        "kor_nlu\n",
        "kor_qpair\n",
        "kor_sae\n",
        "kor_sarcasm\n",
        "squad_kor_v1\n",
        "squad_kor_v2\n",
        "KETI-AIR/kor_corpora\n",
        "KETI-AIR/korquad\n",
        "abwicke/koplo\n",
        "huggingartists/aikko\n",
        "huggingartists/boris-grebenshikov\n",
        "huggingartists/krept-and-konan-bugzy-malone-sl-morisson-abra-cadabra-rv-and-snap-capone\n",
        "huggingartists/lyapis-trubetskoy\n",
        "huggingartists/max-korzh\n",
        "roskoN/dailydialog\n",
        "roskoN/dstc8-reddit-corpus\n",
        "toriving/kosimcse\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FE9EuMQhfOP0"
      },
      "source": [
        "* 'nsmc'\n",
        "  * 네이버에서 출시한 영화 댓글의 감정 분류 데이터셋\n",
        "\n",
        "* 단일 문장 분류 task에는 'nsmc' 외에도 'hate', 'sarcasm'을 사용할 수 있음\n",
        "\n",
        "* dataset은 dictionary 형태로 train과 test가 나뉘어져 있음\n",
        "  * 'document' : sentence\n",
        "  * 'label' : 감정(긍정/부정)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTRtm8_tfBkN"
      },
      "source": [
        "# nsmc 데이터 로드\n",
        "dataset = datasets.load_dataset('nsmc') # nsmc, hate, sarcasm\n",
        "\n",
        "# 데이터셋 구조 확인\n",
        "print(dataset)\n",
        "'''\n",
        "DatasetDict({\n",
        "    train: Dataset({\n",
        "        features: ['id', 'document', 'label'],\n",
        "        num_rows: 150000\n",
        "    })\n",
        "    test: Dataset({\n",
        "        features: ['id', 'document', 'label'],\n",
        "        num_rows: 50000\n",
        "    })\n",
        "})\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMzcDB1RfV6B"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdCFP3ohgCCP"
      },
      "source": [
        "* dataset을 다루기 편한 형태로 변환"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79LimqGJfX6A"
      },
      "source": [
        "# 필요한 데이터인 document와 label 정보만 pandas라이브러리 DataFrame 형식으로 변환\n",
        "train_data = pd.DataFrame({\"document\":dataset['train']['document'], \"label\":dataset['train']['label'],})\n",
        "test_data = pd.DataFrame({\"document\":dataset['test']['document'], \"label\":dataset['test']['label'],})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66Nsqoo3fhwt"
      },
      "source": [
        "# 데이터셋 갯수 확인\n",
        "print('학습 데이터셋 : {}'.format(len(train_data)))\n",
        "# 학습 데이터셋 : 150000\n",
        "print('테스트 데이터셋 : {}'.format(len(test_data)))\n",
        "# 테스트 데이터셋 : 50000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CIThWk-d_3P"
      },
      "source": [
        "# 데이터셋 내용 확인\n",
        "train_data[:5]\n",
        "'''\n",
        "\t                                                                              document\tlabel\n",
        "0\t                                                      아 더빙.. 진짜 짜증나네요 목소리\t    0\n",
        "1\t                              흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나\t    1\n",
        "2\t                                                    너무재밓었다그래서보는것을추천한다\t    0\n",
        "3\t                                     교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정\t    0\n",
        "4\t사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...\t    1\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhNtGwLMf74J"
      },
      "source": [
        "test_data[:5]\n",
        "'''\n",
        "\t                                                                      document\tlabel\n",
        "0\t                                                                         굳 ㅋ\t    1\n",
        "1\t                                                          GDNTOPCLASSINTHECLUB\t    0\n",
        "2\t                뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아\t    0\n",
        "3\t                          지루하지는 않은데 완전 막장임... 돈주고 보기에는....\t    0\n",
        "4\t3D만 아니었어도 별 다섯 개 줬을텐데.. 왜 3D로 나와서 제 심기를 불편하게 하죠??\t    0\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52bU8-Ukg0vM"
      },
      "source": [
        "* 데이터 전처리 과정\n",
        "  * 중복 데이터 제거"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdaspzRfgHaN"
      },
      "source": [
        "# 데이터 중복을 제외한 갯수 확인\n",
        "print(\"학습데이터 : \",train_data['document'].nunique(),\" 라벨 : \",train_data['label'].nunique())\n",
        "# 학습데이터 :  146183  라벨 :  2\n",
        "print(\"데스트 데이터 : \",test_data['document'].nunique(),\" 라벨 : \",test_data['label'].nunique())\n",
        "# 데스트 데이터 :  49158  라벨 :  2\n",
        "\n",
        "# 중복 데이터 제거\n",
        "train_data.drop_duplicates(subset=['document'], inplace= True)\n",
        "test_data.drop_duplicates(subset=['document'], inplace= True)\n",
        "\n",
        "# 데이터셋 갯수 확인\n",
        "print('중복 제거 후 학습 데이터셋 : {}'.format(len(train_data)))\n",
        "# 중복 제거 후 학습 데이터셋 : 146183\n",
        "print('중복 제거 후 테스트 데이터셋 : {}'.format(len(test_data)))\n",
        "# 중복 제거 후 테스트 데이터셋 : 49158"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJr8XoA0gxKS"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCbP_1QihIVu"
      },
      "source": [
        "* 데이터 전처리\n",
        "  * null 데이터 제거"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfzsFmZ6gx6x"
      },
      "source": [
        "# null 데이터 제거\n",
        "train_data['document'].replace('', np.nan, inplace=True)\n",
        "test_data['document'].replace('', np.nan, inplace=True)\n",
        "train_data = train_data.dropna(how = 'any')\n",
        "test_data = test_data.dropna(how = 'any')\n",
        "\n",
        "print('null 제거 후 학습 데이터셋 : {}'.format(len(train_data)))\n",
        "# null 제거 후 학습 데이터셋 : 146182\n",
        "print('null 제거 후 테스트 데이터셋 : {}'.format(len(test_data)))\n",
        "# null 제거 후 테스트 데이터셋 : 49157"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fh2rISfBgzAq"
      },
      "source": [
        "print(train_data['document'][0])\n",
        "# 아 더빙.. 진짜 짜증나네요 목소리\n",
        "print(train_data['label'][0])\n",
        "# 0\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-G-ueuchCT8"
      },
      "source": [
        "* 데이터 전처리\n",
        "  * outlier 제거"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fceDNficg5Or"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "#학습 리뷰 길이조사\n",
        "print('학습 문장 최대 길이 :',max(len(l) for l in train_data['document']))\n",
        "# 학습 문장 최대 길이 : 146\n",
        "print('학습 문장의 평균 길이 :',sum(map(len, train_data['document']))/len(train_data['document']))\n",
        "# 학습 문장의 평균 길이 : 35.981338331668745\n",
        "\n",
        "plt.hist([len(s) for s in train_data['document']], bins=50)\n",
        "plt.xlabel('length of data')\n",
        "plt.ylabel('number of data')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVtiPTl7hhP_"
      },
      "source": [
        "* pretrained된 multilingual model 사용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgAz-DeMhOmz"
      },
      "source": [
        "# Store the model we want to use\n",
        "from transformers import AutoModel, AutoTokenizer, BertTokenizer\n",
        "MODEL_NAME = \"bert-base-multilingual-cased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0RRYwhWhseP"
      },
      "source": [
        "* tokenizer에 list 형태로 입력하면 list 형태로 반환됨"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzsTqt-LhPkK"
      },
      "source": [
        "tokenized_train_sentences = tokenizer(\n",
        "    list(train_data['document']),\n",
        "    return_tensors=\"pt\",\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    add_special_tokens=True,\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H21PdmMlhdYR"
      },
      "source": [
        "print(tokenized_train_sentences[0])\n",
        "# Encoding(num_tokens=142, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
        "print(tokenized_train_sentences[0].tokens)\n",
        "# ['[CLS]', '아', '더', '##빙', '.', '.', '진', '##짜', '짜', '##증', '##나', '##네', '##요', '목', '##소', '##리', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
        "print(tokenized_train_sentences[0].ids)\n",
        "# [101, 9519, 9074, 119005, 119, 119, 9708, 119235, 9715, 119230, 16439, 77884, 48549, 9284, 22333, 12692, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "print(tokenized_train_sentences[0].attention_mask)\n",
        "# [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BovTN6n4hqkb"
      },
      "source": [
        "tokenized_test_sentences = tokenizer(\n",
        "    list(test_data['document']),\n",
        "    return_tensors=\"pt\",\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    add_special_tokens=True,\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sA_hEBmJhrdo"
      },
      "source": [
        "train_label = train_data['label'].values\n",
        "test_label = test_data['label'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opQhv_8Nhxeu"
      },
      "source": [
        "print(train_label[0]) # 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-IIQXh7h9PZ"
      },
      "source": [
        "* 실제 model에 입력하기 위한 구조적인 형태로 변환\n",
        "  * huggingface의 dataset과 model을 사용하는 경우 형태가 대부분 일치함\n",
        "\n",
        "* `__getitem__()`\n",
        "  * step 맞는 dataset을 model가져오는 역할"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsiRufJNhyhS"
      },
      "source": [
        "class SingleSentDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx): \n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCc0-wQDh6IF"
      },
      "source": [
        "train_dataset = SingleSentDataset(tokenized_train_sentences, train_label)\n",
        "test_dataset = SingleSentDataset(tokenized_test_sentences, test_label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eV2EVYgBit4I"
      },
      "source": [
        "* `BertForSequenceClassification`\n",
        "  * 분류를 위해 BERT 위에 부착하는 classicfication을 위한 head를 제공"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUJuAxIjh6oo"
      },
      "source": [
        "from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
        "# 문장 분류를 위해선 BERT 위에 classification을 위한 head를 부착해야 합니다.\n",
        "# 해당 부분을 transformers에서는 라이브러리 하나만 호출하면 됩니다! :-)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoyL_z3Apd0V"
      },
      "source": [
        "* `warmup_steps`\n",
        "  * learning rate의 범위\n",
        "  * learning rate를 조절함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAaZOlmth7MO"
      },
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    num_train_epochs=1,              # total number of training epochs\n",
        "    per_device_train_batch_size=32,  # batch size per device during training\n",
        "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        "    logging_steps=500,\n",
        "    save_steps=500,\n",
        "    save_total_limit=2\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CK09KMBdiRzI"
      },
      "source": [
        "model = BertForSequenceClassification.from_pretrained(MODEL_NAME)\n",
        "model.to(device)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ME1YF6W3iSqc"
      },
      "source": [
        "trainer.train() # 1 epoch에 대략 30분 정도 소요됩니다 :-)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbrT1kCRqEBQ"
      },
      "source": [
        "* 성능 평가를 위한 함수"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1EwSkLiiky5"
      },
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OVocAS-qLHN"
      },
      "source": [
        "* evaluate가 실행될 때, `compute_metrics`에 해당하는 함수를 불러와서 평가함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEI3Kb7yilbJ"
      },
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjgfbSQMip7m"
      },
      "source": [
        "trainer.evaluate(eval_dataset=test_dataset)\n",
        "'''\n",
        "{'eval_accuracy': 0.8609150273613118,\n",
        " 'eval_f1': 0.8648627280453818,\n",
        " 'eval_loss': 0.3183152973651886,\n",
        " 'eval_mem_cpu_alloc_delta': 943706,\n",
        " 'eval_mem_cpu_peaked_delta': 2675650,\n",
        " 'eval_mem_gpu_alloc_delta': 0,\n",
        " 'eval_mem_gpu_peaked_delta': 280995328,\n",
        " 'eval_precision': 0.8452978904257785,\n",
        " 'eval_recall': 0.8853547003358828,\n",
        " 'eval_runtime': 194.1152,\n",
        " 'eval_samples_per_second': 253.236,\n",
        " 'init_mem_cpu_alloc_delta': 48822,\n",
        " 'init_mem_cpu_peaked_delta': 18306,\n",
        " 'init_mem_gpu_alloc_delta': 0,\n",
        " 'init_mem_gpu_peaked_delta': 0}\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7klnRAOqvHE"
      },
      "source": [
        "* trainer를 사용하지 않고 torch로 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrHUhws6qkBm"
      },
      "source": [
        "# native training using torch\n",
        "\n",
        "# model = BertForSequenceClassification.from_pretrained(MODEL_NAME)\n",
        "# model.to(device)\n",
        "# model.train()\n",
        "\n",
        "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# optim = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# for epoch in range(3):\n",
        "#     for batch in train_loader:\n",
        "#         optim.zero_grad()\n",
        "#         input_ids = batch['input_ids'].to(device)\n",
        "#         attention_mask = batch['attention_mask'].to(device)\n",
        "#         labels = batch['labels'].to(device)\n",
        "#         outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "#         loss = outputs[0]\n",
        "#         loss.backward()\n",
        "#         optim.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGYEDGByp_cn"
      },
      "source": [
        "* prediction 함수"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JVCGr5-q26P"
      },
      "source": [
        "# predict함수\n",
        "def sentences_predict(sent):\n",
        "    model.eval()\n",
        "    tokenized_sent = tokenizer(\n",
        "            sent,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            add_special_tokens=True,\n",
        "            max_length=128\n",
        "    )\n",
        "    tokenized_sent.to(device)\n",
        "    \n",
        "    with torch.no_grad():# 그라디엔트 계산 비활성화\n",
        "        outputs = model(\n",
        "            input_ids=tokenized_sent['input_ids'],\n",
        "            attention_mask=tokenized_sent['attention_mask'],\n",
        "            token_type_ids=tokenized_sent['token_type_ids']\n",
        "            )\n",
        "\n",
        "    logits = outputs[0]\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    result = np.argmax(logits)\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flgfEnaurD3t"
      },
      "source": [
        "print(sentences_predict(\"영화 개재밌어 ㅋㅋㅋㅋㅋ\")) # 1\n",
        "print(sentences_predict(\"진짜 재미없네요 ㅋㅋ\")) # 0\n",
        "print(sentences_predict(\"너 때문에 진짜 짜증나\")) # 0\n",
        "print(sentences_predict(\"정말 재밌고 좋았어요.\")) # 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxVvfqpxrDOT"
      },
      "source": [
        "* `pipeline`을 사용하면 prediction을 구현하지 않아도 됨\n",
        "\n",
        "* model을 pipeline에 이미 지정했기 때문에, 테스트할 때 따로 넣어주지 않아도 됨"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fkeiluk4rGyq"
      },
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "nlp_sentence_classif = pipeline('sentiment-analysis',model=model, tokenizer=tokenizer, device=0)\n",
        "\n",
        "print(nlp_sentence_classif('영화 개재밌어 ㅋㅋㅋㅋㅋ'))\n",
        "# [{'label': 'LABEL_1', 'score': 0.7245705723762512}]\n",
        "print(nlp_sentence_classif('진짜 재미없네요 ㅋㅋ',model= model)) # model 생략 가능\n",
        "# [{'label': 'LABEL_0', 'score': 0.9946942925453186}]\n",
        "print(nlp_sentence_classif('너 때문에 진짜 짜증나',model= model)) # model 생략 가능\n",
        "# [{'label': 'LABEL_0', 'score': 0.9947050213813782}]\n",
        "print(nlp_sentence_classif('정말 재밌고 좋았어요.',model= model)) # model 생략 가능\n",
        "# [{'label': 'LABEL_1', 'score': 0.989768385887146}]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}