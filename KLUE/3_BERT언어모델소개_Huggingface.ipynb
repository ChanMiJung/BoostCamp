{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3_BERT언어모델소개_Huggingface.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ozmti7YJm7vA"
      },
      "source": [
        "# Huggingface library tutorial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4FItzzYNubd"
      },
      "source": [
        "* HuggingFace\n",
        "  * Transformer 라이브러리를 구축하고 유지하는 회사\n",
        "\n",
        "* HuggingFace library\n",
        "  * 공식화된 라이브러리\n",
        "  * language model은 huggingface의 transformers 를 기반으로 만들어짐"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6eXhLUFOeYn"
      },
      "source": [
        "* 모델 다운로드 및 작업 시작\n",
        "  * 아래 3줄만 있으면 됨\n",
        "  * 모델에 필요한 parameter나 configuration, vocab file 등을 다운로두함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y05Hm1vJobtD"
      },
      "source": [
        "from transformers import TFAutoModel, AutoTokenizer\n",
        "model = TFAutoModel.from_pretrained(\"<model-name>\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"<model-name>\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RtAnu8eOrQY"
      },
      "source": [
        "* 모델 검색\n",
        "  * HuggingFace 사이트의 검색창을 이용하여 모델 검색"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlMVFTbnO_37"
      },
      "source": [
        "### Tokenizer 실습\n",
        "* transformers 사용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nm3WhEP7PEas"
      },
      "source": [
        "# !pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnIarVQtPR3e"
      },
      "source": [
        "## Tokenizer 응용\n",
        "* BERT model과 BERT tokenizer가 필요함\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DReNd9S_PVos"
      },
      "source": [
        "from transformers import AutoModel, AutoTokenizer, BertTokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWxW3LsyPn_N"
      },
      "source": [
        "* 실습에서 사용할 모델 : Transformer기반의 대표 모델인 multi-lingual bert model\n",
        "  * 한국어도 포함되어 있음"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBFPVFJAPzKl"
      },
      "source": [
        "# Store the model we want to use\n",
        "MODEL_NAME = \"bert-base-multilingual-cased\"\n",
        "\n",
        "# We need to create the model and tokenizer\n",
        "model = AutoModel.from_pretrained(MODEL_NAME)\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDYv_oFtQAr4"
      },
      "source": [
        "* multi-lingual model이기 때문에 wordpiece vocab이 12만개정도 됨\n",
        "  * 그 중 한국어는 8천개 정도"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaW5CEq_QMUf"
      },
      "source": [
        "print(tokenizer.vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROEsbMq4QSzq"
      },
      "source": [
        "* 한국어 corpus를 이용해서 vocab 을 만들 때, 3만개 정도로 정의하면 적절함\n",
        "  * 한자와 특수기호도 대부분 들어감"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcV2177tQfZF"
      },
      "source": [
        "for i, key in enumerate(tokenizer.get_vocab()):\n",
        "    print(key)\n",
        "    if i > 50:\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ver5REsKQvTx"
      },
      "source": [
        "text = \"이순신은 조선 중기의 무신이다.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gy19UC3BQoEa"
      },
      "source": [
        "* Multi-lingual bert model은 `BERTTokenizeFast` class로 되어 있음"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OaQD4vNQmvT"
      },
      "source": [
        "print(type(tokenizer))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NV-citJjQ97o"
      },
      "source": [
        "* tokenizer 테스트\n",
        "  * `return_tensors=\"pt\"` : pyTorch로 return\n",
        "  * tokenize된 정보들이 dictionary화 되어 `tokenized_input_text` 에 들어가있음\n",
        "  * 'input_ids' : input text를 tokenizing 한 후의 vocab id\n",
        "    * 이 vocab id가 모델에 input으로 입력됨\n",
        "  * 'token_type_ids' : segment id\n",
        "    * 입력된 sentence index 번호(?)\n",
        "    * 같은 문장의 token들은 동일한 id를 가짐\n",
        "  * 'attention_mask' : \n",
        "    * padding은 0으로 초기화됨"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNpa0Dl-Q4a7"
      },
      "source": [
        "tokenized_input_text = tokenizer(text, return_tensors=\"pt\")\n",
        "for key, value in tokenized_input_text.items():\n",
        "    print(\"{}:\\n\\t{}\".format(key, value))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0uZ3HCvRKUE"
      },
      "source": [
        "print(tokenized_input_text['input_ids'])    # input text를 tokenizing한 후 vocab의 id\n",
        "print(tokenized_input_text.input_ids)\n",
        "print(tokenized_input_text['token_type_ids'])   # segment id (sentA or sentB)\n",
        "print(tokenized_input_text.token_type_ids)\n",
        "print(tokenized_input_text['attention_mask'])   # special token (pad, cls, sep) or not\n",
        "print(tokenized_input_text.attention_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDWjKGhHSnRh"
      },
      "source": [
        "* `tokenizer.tokenize()` : 입력된 input을 tokenizing함\n",
        "    * 명시적으로 보여줌\n",
        "    * tokenizer의 default값이 자동으로 함수에 special token을 부착하도록 되어 있음\n",
        "      * BERT의 입력은 항상 [CLS]가 문장 앞에, [SEP]가 문장 끝에 붙게 됨\n",
        "\n",
        "* encoding & decoding\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqbfyEj6SleJ"
      },
      "source": [
        "  tokenized_text = tokenizer.tokenize(text)\n",
        "print(tokenized_text)\n",
        "input_ids = tokenizer.encode(text)\n",
        "print(input_ids)\n",
        "decoded_ids = tokenizer.decode(input_ids)\n",
        "print(decoded_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zVP_PcLTd_d"
      },
      "source": [
        "* special token을 추가하고 싶지 않을 경우 옵션 변경\n",
        "  * `add_special_tokens=False` : [CLS], [SEP]가 붙지 않음"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9sFf6I4TdRS"
      },
      "source": [
        "tokenized_text = tokenizer.tokenize(text, add_special_tokens=False)\n",
        "print(tokenized_text)\n",
        "input_ids = tokenizer.encode(text, add_special_tokens=False)\n",
        "print(input_ids)\n",
        "decoded_ids = tokenizer.decode(input_ids)\n",
        "print(decoded_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYvUxNyeTy85"
      },
      "source": [
        "* tokenizing할 때, padding과 truncation이 중요함\n",
        "  * `add_special_tokens=False`, `max_length=5`, `truncation=True` 이면 [CLS]와 [SEP]가 붙지 않은 상태의 token들(입력문장을 tokenizing했을 때의 token들) 중에 5개의 token만 남기고 잘라냄\n",
        "\n",
        "* tokenizer token 단위로 자르는 것이기 때문에 원래의 input 문장의 length와 관련이 없음"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wOnu_ZITvL6"
      },
      "source": [
        "tokenized_text = tokenizer.tokenize(\n",
        "    text,\n",
        "    add_special_tokens=False,\n",
        "    max_length=5,\n",
        "    truncation=True\n",
        "    )\n",
        "print(tokenized_text)\n",
        "# ['이', '##순', '##신', '##은', '조선']\n",
        "\n",
        "input_ids = tokenizer.encode(\n",
        "    text,\n",
        "    add_special_tokens=False,\n",
        "    max_length=5,\n",
        "    truncation=True\n",
        "    )\n",
        "print(input_ids)\n",
        "# [9638, 119064, 25387, 10892, 59906]\n",
        "decoded_ids = tokenizer.decode(input_ids)\n",
        "print(decoded_ids)\n",
        "# 이순신은 조선"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hAAibEMU-O0"
      },
      "source": [
        "* padding\n",
        "  * tokenizer 함수에 padding옵션을 넣어주면 됨\n",
        "    * 원하는 padding조건에 따라 옵션이 바뀜\n",
        "  * 'max_length' : 기본 옵션\n",
        "    * max length만큼 padding을 뒤에 부착해서 채움"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3T4oXgjUTxI7"
      },
      "source": [
        "print(tokenizer.pad_token) # [PAD]\n",
        "print(tokenizer.pad_token_id) # 0\n",
        "\n",
        "tokenized_text = tokenizer.tokenize(\n",
        "    text,\n",
        "    add_special_tokens=False,\n",
        "    max_length=20,\n",
        "    padding=\"max_length\"\n",
        "    )\n",
        "print(tokenized_text)\n",
        "# ['이', '##순', '##신', '##은', '조선', '중', '##기의', '무', '##신', '##이다', '.', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
        "\n",
        "input_ids = tokenizer.encode(\n",
        "    text,\n",
        "    add_special_tokens=False,\n",
        "    max_length=20,\n",
        "    padding=\"max_length\"\n",
        "    )\n",
        "print(input_ids)\n",
        "# [9638, 119064, 25387, 10892, 59906, 9694, 46874, 9294, 25387, 11925, 119, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "\n",
        "decoded_ids = tokenizer.decode(input_ids)\n",
        "print(decoded_ids)\n",
        "# 이순신은 조선 중기의 무신이다. [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ou8pDV4dVjqg"
      },
      "source": [
        "* 새로운 token 추가"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNQkTjy9VnBb"
      },
      "source": [
        "print(tokenizer.vocab_size) # 119547"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KO0yZrd4Vuom"
      },
      "source": [
        "* 학습되어있는 정상적인 문장이 아니기 때문에 tokenizing이 되지 않음\n",
        "  * [UNK]로 바뀜\n",
        "  * [UNK]가 많을수록 원본 문장의 의미가 희석됨"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15tMF_98Vnfr"
      },
      "source": [
        "text = \"깟뻬뜨랑 리뿔이 뜨럽거 므리커럭이 케쇽 냐왜쇼 우뤼갸 쳥쇼섀료다혀뚜여\"\n",
        "\n",
        "tokenized_text = tokenizer.tokenize(text, add_special_tokens=False)\n",
        "print(tokenized_text)\n",
        "# ['[UNK]', '리', '##뿔', '##이', '뜨', '##럽', '##거', '므', '##리', '##커', '##럭', '##이', '[UNK]', '냐', '##왜', '##쇼', '[UNK]', '[UNK]']\n",
        "input_ids = tokenizer.encode(text, add_special_tokens=False)\n",
        "print(input_ids)\n",
        "# [100, 9238, 119021, 10739, 9151, 118867, 41521, 9308, 12692, 106826, 118864, 10739, 100, 9002, 119164, 119060, 100, 100]\n",
        "decoded_ids = tokenizer.decode(input_ids)\n",
        "print(decoded_ids)\n",
        "# [UNK] 리뿔이 뜨럽거 므리커럭이 [UNK] 냐왜쇼 [UNK] [UNK]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bK00LHTvWKsX"
      },
      "source": [
        "* 원하는 task, 작업할 focus에 맞춰서 tokenizer 수정이 필요함\n",
        "\n",
        "* 임의로 [UNK]에 해당하는 vocab들을 `add_tokens()`를 사용하여 token에 추가함\n",
        "  * list형태로 입력\n",
        "  * tokenizing할 때 이 token들이 반영됨"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dI0e6ooNWJrc"
      },
      "source": [
        "added_token_num = tokenizer.add_tokens([\"깟뻬뜨랑\", \"케쇽\", \"우뤼갸\", \"쳥쇼\", \"섀료\"])\n",
        "print(added_token_num)\n",
        "# 5\n",
        "\n",
        "tokenized_text = tokenizer.tokenize(text, add_special_tokens=False)\n",
        "print(tokenized_text)\n",
        "# ['깟뻬뜨랑', '리', '##뿔', '##이', '뜨', '##럽', '##거', '므', '##리', '##커', '##럭', '##이', '케쇽', '냐', '##왜', '##쇼', '우뤼갸', '쳥쇼', '섀료', '다', '##혀', '##뚜', '##여']\n",
        "input_ids = tokenizer.encode(text, add_special_tokens=False)\n",
        "print(input_ids)\n",
        "# [119547, 9238, 119021, 10739, 9151, 118867, 41521, 9308, 12692, 106826, 118864, 10739, 119548, 9002, 119164, 119060, 119549, 119550, 119551, 9056, 80579, 118841, 29935]\n",
        "decoded_ids = tokenizer.decode(input_ids)\n",
        "print(decoded_ids)\n",
        "# 깟뻬뜨랑 리뿔이 뜨럽거 므리커럭이 케쇽 냐왜쇼 우뤼갸 쳥쇼 섀료 다혀뚜여"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fT2WDRP-WkyV"
      },
      "source": [
        "print(tokenizer.vocab_size) # 119547"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYfYUgAFW79U"
      },
      "source": [
        " * 특정 역할을 위한 special token도 추가할 수 있음\n",
        "  * ex. [ENTITY]\n",
        "  * 문장에만 추가하면 token으로 인식함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EF4IOVouXHGO"
      },
      "source": [
        "text = \"[SHKIM]이순신은 조선 중기의 무신이다.[/SHKIM]\"\n",
        "# [ENTITY]이순신[/ENTITY]\n",
        "tokenized_text = tokenizer.tokenize(text, add_special_tokens=False)\n",
        "print(tokenized_text)\n",
        "# ['[', 'SH', '##KI', '##M', ']', '이', '##순', '##신', '##은', '조선', '중', '##기의', '무', '##신', '##이다', '.', '[', '/', 'SH', '##KI', '##M', ']']\n",
        "input_ids = tokenizer.encode(text, add_special_tokens=False)\n",
        "print(input_ids)\n",
        "# [164, 38702, 59879, 11517, 166, 9638, 119064, 25387, 10892, 59906, 9694, 46874, 9294, 25387, 11925, 119, 164, 120, 38702, 59879, 11517, 166]\n",
        "decoded_ids = tokenizer.decode(input_ids)\n",
        "print(decoded_ids)\n",
        "# [ SHKIM ] 이순신은 조선 중기의 무신이다. [ / SHKIM ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnYm7YrYXgYf"
      },
      "source": [
        "* `add_special_tokens()` 함수를 사용하여 special token을 추가\n",
        "  * dictionary 형태로 입력"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvRyNHD9Xks1"
      },
      "source": [
        "text = \"[SHKIM]이순신은 조선 중기의 무신이다.[/SHKIM]\"\n",
        "\n",
        "added_token_num += tokenizer.add_special_tokens({\"additional_special_tokens\":[\"[SHKIM]\", \"[/SHKIM]\"]})\n",
        "tokenized_text = tokenizer.tokenize(text, add_special_tokens=False)\n",
        "print(tokenized_text)\n",
        "# ['[SHKIM]', '이', '##순', '##신', '##은', '조선', '중', '##기의', '무', '##신', '##이다', '.', '[/SHKIM]']\n",
        "input_ids = tokenizer.encode(text, add_special_tokens=False)\n",
        "print(input_ids)\n",
        "# [119552, 9638, 119064, 25387, 10892, 59906, 9694, 46874, 9294, 25387, 11925, 119, 119553]\n",
        "decoded_ids = tokenizer.decode(input_ids)\n",
        "print(decoded_ids)\n",
        "# [SHKIM] 이순신은 조선 중기의 무신이다. [/SHKIM]\n",
        "decoded_ids = tokenizer.decode(input_ids,skip_special_tokens=True)\n",
        "print(decoded_ids)\n",
        "# 이순신은 조선 중기의 무신이다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OiS2EFoYUeV"
      },
      "source": [
        "* `add_token_num` : model을 resize하는 기준이 됨\n",
        "  * prefix된 model은 embedding되어있는 vocab size가 고정되어 있음\n",
        "  * vocab size를 벗어나는 id가 input으로 들어오면 에러 발생함\n",
        "  * model을 resize해주어야함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3_OFY6IYCCx"
      },
      "source": [
        "print(added_token_num) # 7"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idVpupe-Ywf0"
      },
      "source": [
        "* 자연어처리 task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkofquxMYNkQ"
      },
      "source": [
        "# Single segment input\n",
        "single_seg_input = tokenizer(\"이순신은 조선 중기의 무신이다.\")\n",
        "\n",
        "# Multiple segment input\n",
        "multi_seg_input = tokenizer(\"이순신은 조선 중기의 무신이다.\", \"그는 임진왜란을 승리로 이끌었다.\")\n",
        "\n",
        "print(\"Single segment token (str): {}\".format(tokenizer.convert_ids_to_tokens(single_seg_input['input_ids'])))\n",
        "# Single segment token (str): ['[CLS]', '이', '##순', '##신', '##은', '조선', '중', '##기의', '무', '##신', '##이다', '.', '[SEP]']\n",
        "print(\"Single segment token (int): {}\".format(single_seg_input['input_ids']))\n",
        "# Single segment token (int): [101, 9638, 119064, 25387, 10892, 59906, 9694, 46874, 9294, 25387, 11925, 119, 102]\n",
        "print(\"Single segment type       : {}\".format(single_seg_input['token_type_ids']))\n",
        "# Single segment type       : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "\n",
        "# Segments are concatened in the input to the model, with \n",
        "print()\n",
        "print(\"Multi segment token (str): {}\".format(tokenizer.convert_ids_to_tokens(multi_seg_input['input_ids'])))\n",
        "# Multi segment token (str): ['[CLS]', '이', '##순', '##신', '##은', '조선', '중', '##기의', '무', '##신', '##이다', '.', '[SEP]', '그는', '임', '##진', '##왜', '##란', '##을', '승', '##리로', '이', '##끌', '##었다', '.', '[SEP]']\n",
        "print(\"Multi segment token (int): {}\".format(multi_seg_input['input_ids']))\n",
        "# Multi segment token (int): [101, 9638, 119064, 25387, 10892, 59906, 9694, 46874, 9294, 25387, 11925, 119, 102, 17889, 9644, 18623, 119164, 49919, 10622, 9484, 100434, 9638, 118705, 17706, 119, 102]\n",
        "print(\"Multi segment type       : {}\".format(multi_seg_input['token_type_ids']))\n",
        "# Multi segment type       : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2hXnQzlZBC7"
      },
      "source": [
        "* list형태로 입력하면 list 형태로 출력이 됨"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-mKVFKBYOsc"
      },
      "source": [
        "# Padding highlight\n",
        "tokens = tokenizer(\n",
        "    [\"이순신은 조선 중기의 무신이다.\", \"그는 임진왜란을 승리로 이끌었다.\"], \n",
        "    padding=True  # First sentence will have some PADDED tokens to match second sequence length\n",
        ")\n",
        "\n",
        "for i in range(2):\n",
        "    print(\"Tokens (int)      : {}\".format(tokens['input_ids'][i]))\n",
        "    print(\"Tokens (str)      : {}\".format([tokenizer.convert_ids_to_tokens(s) for s in tokens['input_ids'][i]]))\n",
        "    print(\"Tokens (attn_mask): {}\".format(tokens['attention_mask'][i]))\n",
        "    print()\n",
        "\n",
        "'''\n",
        "Tokens (int)      : [101, 9638, 119064, 25387, 10892, 59906, 9694, 46874, 9294, 25387, 11925, 119, 102, 0]\n",
        "Tokens (str)      : ['[CLS]', '이', '##순', '##신', '##은', '조선', '중', '##기의', '무', '##신', '##이다', '.', '[SEP]', '[PAD]']\n",
        "Tokens (attn_mask): [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
        "\n",
        "Tokens (int)      : [101, 17889, 9644, 18623, 119164, 49919, 10622, 9484, 100434, 9638, 118705, 17706, 119, 102]\n",
        "Tokens (str)      : ['[CLS]', '그는', '임', '##진', '##왜', '##란', '##을', '승', '##리로', '이', '##끌', '##었다', '.', '[SEP]']\n",
        "Tokens (attn_mask): [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hF_Zxu3ZJ0R"
      },
      "source": [
        "### BERT 모델 테스트"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwT-AIfbZSFI"
      },
      "source": [
        "* BERT 모델을 이용하여 [MASK] token을 예측"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SV5FtSMlZWU5"
      },
      "source": [
        "text = \"이순신은 [MASK] 중기의 무신이다.\"\n",
        "tokenized_text = tokenizer.tokenize(text)\n",
        "\n",
        "print(tokenized_text)\n",
        "# ['이', '##순', '##신', '##은', '[MASK]', '중', '##기의', '무', '##신', '##이다', '.']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2w1RRFnZlTR"
      },
      "source": [
        "* `pipeline`\n",
        "  * mask를 채운 결과를 return"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2jvPbKhZdyX"
      },
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "nlp_fill = pipeline('fill-mask', model=MODEL_NAME)\n",
        "nlp_fill(\"이순신은 [MASK] 중기의 무신이다.\")\n",
        "\n",
        "'''\n",
        "/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py:337: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
        "  \"Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 \"\n",
        "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
        "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
        "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
        "[{'score': 0.8747126460075378,\n",
        "  'sequence': '이순신은 조선 중기의 무신이다.',\n",
        "  'token': 59906,\n",
        "  'token_str': '조선'},\n",
        " {'score': 0.06436426192522049,\n",
        "  'sequence': '이순신은 청 중기의 무신이다.',\n",
        "  'token': 9751,\n",
        "  'token_str': '청'},\n",
        " {'score': 0.010954886674880981,\n",
        "  'sequence': '이순신은 전 중기의 무신이다.',\n",
        "  'token': 9665,\n",
        "  'token_str': '전'},\n",
        " {'score': 0.0046471720561385155,\n",
        "  'sequence': '이순신은종 중기의 무신이다.',\n",
        "  'token': 22200,\n",
        "  'token_str': '##종'},\n",
        " {'score': 0.0036106714978814125,\n",
        "  'sequence': '이순신은기 중기의 무신이다.',\n",
        "  'token': 12310,\n",
        "  'token_str': '##기'}]\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOBDUJqXZ3H8"
      },
      "source": [
        "* 출력 결과 확인\n",
        "  * tokenizer의 결과를 model의 input으로 주어 결과를 출력함\n",
        "  * Token wise output : \n",
        "    * 입력된 문장은 13개의 token으로 이루어져 있음\n",
        "    * 13개의 token에 대한 vector값(768)\n",
        "  * Pooled output : \n",
        "    * CLS token의 vector를 얻어냄\n",
        "    * 문장 input(1)과 768차원의 문장 vector output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INi23mTuZ5HH"
      },
      "source": [
        "tokens_pt = tokenizer(\"이순신은 조선 중기의 무신이다.\", return_tensors=\"pt\")\n",
        "for key, value in tokens_pt.items():\n",
        "    print(\"{}:\\n\\t{}\".format(key, value))\n",
        "\n",
        "'''\n",
        "input_ids:\n",
        "\ttensor([[   101,   9638, 119064,  25387,  10892,  59906,   9694,  46874,   9294,\n",
        "          25387,  11925,    119,    102]])\n",
        "token_type_ids:\n",
        "\ttensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
        "attention_mask:\n",
        "\ttensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
        "'''\n",
        "\n",
        "outputs = model(**tokens_pt)\n",
        "last_hidden_state = outputs.last_hidden_state\n",
        "pooler_output = outputs.pooler_output\n",
        "\n",
        "print(\"\\nToken wise output: {}, Pooled output: {}\".format(last_hidden_state.shape, pooler_output.shape))\n",
        "# Token wise output: torch.Size([1, 13, 768]), Pooled output: torch.Size([1, 768])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmwWaUJ4aTNK"
      },
      "source": [
        "print(pooler_output) # [CLS] token to 768 dimension"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOT5GRqsbGV7"
      },
      "source": [
        "* vocab을 추가한 경우 반드시 model의 embedding layer 사이즈를 늘려야 함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "je5ssluibF-k"
      },
      "source": [
        "print(model.get_input_embeddings())\n",
        "# Embedding(119547, 768, padding_idx=0)\n",
        "model.resize_token_embeddings(tokenizer.vocab_size + added_token_num)\n",
        "print(model.get_input_embeddings())\n",
        "# Embedding(119554, 768)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6I1rrHd9bSjT"
      },
      "source": [
        "* [CLS] token을 활용하여 문장의 유사도 측정\n",
        "  * pooled output은 문장 input과 문장 vector output을 출력함\n",
        "  * 두 문장간의 유사도 계산이 가능함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leCBrwUNbV_I"
      },
      "source": [
        "sent1 = tokenizer(\"오늘 하루 어떻게 보냈나요?\", return_tensors=\"pt\")\n",
        "sent2 = tokenizer(\"오늘은 어떤 하루를 보내셨나요?\", return_tensors=\"pt\")\n",
        "sent3 = tokenizer(\"이순신은 조선 중기의 무신이다.\", return_tensors=\"pt\")\n",
        "sent4 = tokenizer(\"깟뻬뜨랑 리뿔이 뜨럽거 므리커럭이 케쇽 냐왜쇼 우뤼갸 쳥쇼섀료다혀뚜여\", return_tensors=\"pt\")\n",
        "\n",
        "\n",
        "outputs = model(**sent1)\n",
        "sent_1_pooler_output = outputs.pooler_output\n",
        "\n",
        "outputs = model(**sent2)\n",
        "sent_2_pooler_output = outputs.pooler_output\n",
        "\n",
        "outputs = model(**sent3)\n",
        "sent_3_pooler_output = outputs.pooler_output\n",
        "\n",
        "outputs = model(**sent4)\n",
        "sent_4_pooler_output = outputs.pooler_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjwoO3fyb8Ad"
      },
      "source": [
        "* `nn.CosineSimilarity()` 함수를 사용하여 유사도를 측정함\n",
        "  * 유사도가 높다고 해도 의미가 유사하지 않을 수 있음"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAmJ66ycbuir"
      },
      "source": [
        "from torch import nn\n",
        "\n",
        "cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "print(cos(sent_1_pooler_output, sent_2_pooler_output))\n",
        "# tensor([0.9757], grad_fn=<DivBackward0>) # 유사도가 높음\n",
        "print(cos(sent_2_pooler_output, sent_3_pooler_output))\n",
        "# tensor([0.6075], grad_fn=<DivBackward0>)\n",
        "print(cos(sent_3_pooler_output, sent_4_pooler_output))\n",
        "# tensor([0.6167], grad_fn=<DivBackward0>)\n",
        "print(cos(sent_1_pooler_output, sent_4_pooler_output))\n",
        "# tensor([0.9389], grad_fn=<DivBackward0>) # 유사도가 높음"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtQZ5QFQbvVj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}