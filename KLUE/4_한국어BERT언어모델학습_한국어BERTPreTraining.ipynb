{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4_한국어BERT언어모델학습_한국어BERTPreTraining.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoByec6HOGPb"
      },
      "source": [
        "# 한국어 BERT 모델 학습"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDCcmxiyfOzI"
      },
      "source": [
        "* task 해결 과정\n",
        "  * dataset 생성\n",
        "  * dataloader 생성\n",
        "  * trainer argument 채워줌\n",
        "  * train 함"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhZnNMDHOJNZ"
      },
      "source": [
        "## 학습 데이터 가져오기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVF9e4W-N_oA"
      },
      "source": [
        "!mkdir my_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wm6F7ThKQPRL"
      },
      "source": [
        "* 개인적으로 데이터 준비"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0nxeY6PQZy6"
      },
      "source": [
        "* 학습 준비"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWkVK7t9Qaig"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-cMez9rQejp"
      },
      "source": [
        "### Tokenizer 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32HRWqvgQhoY"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lbbw474Qxxd"
      },
      "source": [
        "* `BertWordPieceTokenizer` class를 만들고 train을 하게 되면 사용할 수 있는 Word Piece Toeknizer를 바로 획득 가능"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0o8FnTqQiRd"
      },
      "source": [
        "from tokenizers import BertWordPieceTokenizer\n",
        "\n",
        "# Initialize an empty tokenizer\n",
        "wp_tokenizer = BertWordPieceTokenizer(\n",
        "    clean_text=True,   # [\"이순신\", \"##은\", \" \", \"조선\"] ->  [\"이순신\", \"##은\", \"조선\"]\n",
        "    # if char == \" \" or char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
        "    handle_chinese_chars=True,  # 한자는 모두 char 단위로 쪼게버립니다.\n",
        "    strip_accents=False,    # True: [YehHamza] -> [Yep, Hamza]\n",
        "    lowercase=False,    # Hello -> hello\n",
        ")\n",
        "\n",
        "# And then train\n",
        "wp_tokenizer.train(\n",
        "    files=\"my_data/wiki_20190620_small.txt\",\n",
        "    vocab_size=20000,   # vocab size 를 지정해줄 수 있습니다.\n",
        "    min_frequency=2,\n",
        "    show_progress=True,\n",
        "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"],\n",
        "    wordpieces_prefix=\"##\"\n",
        ")\n",
        "\n",
        "# Save the files\n",
        "wp_tokenizer.save_model(\"wordPieceTokenizer\", \"my_tokenizer\") # ['wordPieceTokenizer/my_tokenizer-vocab.txt']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vaWdq4MQmaw"
      },
      "source": [
        "print(wp_tokenizer.get_vocab_size()) # 20000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPu2CuunQntE"
      },
      "source": [
        "text = \"이순신은 조선 중기의 무신이다.\"\n",
        "tokenized_text = wp_tokenizer.encode(text)\n",
        "print(tokenized_text.tokens)\n",
        "# ['이', '##순', '##신은', '조선', '중', '##기의', '무신', '##이다', '.']\n",
        "print(tokenized_text.ids)\n",
        "# [704, 1346, 7588, 2001, 753, 2603, 13158, 1896, 16]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2k2tmr-LRDan"
      },
      "source": [
        "## BERT 학습"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mH2nROkYSw8w"
      },
      "source": [
        "* BERT 껍데기을 만든 후 dataset을 dataloader를 통해서 계속 먹여줌으로써 껍데기의 weight를 조절해주며 학습함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUJN53IWStCB"
      },
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Px8UoXFBSt0n"
      },
      "source": [
        "from transformers import BertConfig, BertForPreTraining, BertTokenizerFast"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c48HZ0BHSunr"
      },
      "source": [
        "tokenizer = BertTokenizerFast(\n",
        "    vocab_file='/content/wordPieceTokenizer/my_tokenizer-vocab.txt',\n",
        "    max_len=128,\n",
        "    do_lower_case=False,\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzq7dVN1TNON"
      },
      "source": [
        "* 데이터셋을 만들 때 [MASK] token을 부착하여 넘길수도 있음\n",
        "  * special token으로 등록되지 않은 경우 쪼개질 수 있음"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMQtVvR9TN7O"
      },
      "source": [
        "print(tokenizer.tokenize(\"뷁은 [MASK] 조선 중기의 무신이다.\"))\n",
        "# ['[UNK]', '[', 'M', '##AS', '##K', ']', '조선', '중', '##기의', '무신', '##이다', '.']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBTlfNMVTUWx"
      },
      "source": [
        "* [MASK] token 을 special token에 추가"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teNmkEkaTX5V"
      },
      "source": [
        "tokenizer.add_special_tokens({'mask_token':'[MASK]'})\n",
        "print(tokenizer.tokenize(\"이순신은 [MASK] 중기의 무신이다.\"))\n",
        "# ['이', '##순', '##신은', '[MASK]', '중', '##기의', '무신', '##이다', '.']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkVEHkp8ThJC"
      },
      "source": [
        "* BERT 껍데기 생성\n",
        "  * configuration을 통해 조건을 조절함\n",
        "    * `vocab_size`\n",
        "      * default는 영어기준으로 되어있기 때문에 반드시 수정이 필요함\n",
        "    * `hidden_size` : tranformer의 hidden vector size\n",
        "      * 동작의 빠른 속도가 필요한 경우 값을 줄여서 효과를 기대해볼 수 있음\n",
        "    * `num_hidden_layers` : 쌓고자 하는 hidden layer 개수\n",
        "      * BERT 같은 pretrained model을 사용하지만, 좀 더 빠른 상태에서 동작하길 원하는 경우 layer 수를 줄여도(3, 6 정도) 성능 차이가 적음\n",
        "    * `num_attention_heads` : transformer의 multi head self-attention의 개수\n",
        "    * `intermediate_size` : transformer 내에 있는 feed-forward network의 dimension size\n",
        "    * `hidden_act` : activation function\n",
        "    * `hidden_dropout_prob` : dropout 정보\n",
        "    * `max_position_embeddings` : model이 받을 수 있는 input token의 최대 size\n",
        "      * default는 512\n",
        "      * 댓글 분석인경우 32, 64 정도에서 대부분 처리 가능함\n",
        "      * task가 장문으로 되어있는 경우 1024 로 설정할 수도 있음\n",
        "    * `type_vocab_size` : type id 범위\n",
        "      * BERT는 segmentA, segmentB로 구분되어 있기 때문에 2(종류)로 정의\n",
        "    * `pad_token_id` : tokenizer에서 pad token이 가지는 id\n",
        "    * `position_embedding_type`\n",
        "      * 'absolute' : input token의 위치에 따라 절대값으로 positional embedding을 함\n",
        "\n",
        "* `BertForPreTraining`\n",
        "  * transformers에서 제공하는 model type\n",
        "  * 정의한 configuration을 넣어주면 model 껍데기가 생성됨"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THFYU25jTaz9"
      },
      "source": [
        "from transformers import BertConfig, BertForPreTraining\n",
        "\n",
        "config = BertConfig(    # https://huggingface.co/transformers/model_doc/bert.html#bertconfig\n",
        "    vocab_size=20000,\n",
        "    # hidden_size=512,\n",
        "    # num_hidden_layers=12,    # layer num\n",
        "    # num_attention_heads=8,    # transformer attention head number\n",
        "    # intermediate_size=3072,   # transformer 내에 있는 feed-forward network의 dimension size\n",
        "    # hidden_act=\"gelu\",\n",
        "    # hidden_dropout_prob=0.1,\n",
        "    # attention_probs_dropout_prob=0.1,\n",
        "    max_position_embeddings=128,    # embedding size 최대 몇 token까지 input으로 사용할 것인지 지정\n",
        "    # type_vocab_size=2,    # token type ids의 범위 (BERT는 segmentA, segmentB로 2종류)\n",
        "    # pad_token_id=0,\n",
        "    # position_embedding_type=\"absolute\"\n",
        ")\n",
        "\n",
        "model = BertForPreTraining(config=config)\n",
        "model.num_parameters() # 101720098"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3fp3bp2TmT_"
      },
      "source": [
        "from transformers import DataCollatorForLanguageModeling"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MI9gqhsTTnuO"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from transformers.tokenization_utils import PreTrainedTokenizer\n",
        "from typing import Dict, List, Optional\n",
        "import os\n",
        "import json\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import time\n",
        "import warnings\n",
        "\n",
        "from filelock import FileLock\n",
        "\n",
        "from transformers.utils import logging\n",
        "\n",
        "logger = logging.get_logger(__name__)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_A4cYzuXbn0"
      },
      "source": [
        "* dataset 구성\n",
        "  * document 단위로 학습이 이루어짐\n",
        "    * ex. 2개의 document가 서로 다른 인물에 대한 정보일 경우 첫 번째 문서의 마지막 문장과 두 번째 문장의 첫번째 문장이 next sentence prediction으로 연결되지 않도록 방지"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2B3dqoUSXXwV"
      },
      "source": [
        "class TextDatasetForNextSentencePrediction(Dataset):\n",
        "    \"\"\"\n",
        "    This will be superseded by a framework-agnostic approach soon.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        tokenizer: PreTrainedTokenizer,\n",
        "        file_path: str,\n",
        "        block_size: int,\n",
        "        overwrite_cache=False,\n",
        "        short_seq_probability=0.1,\n",
        "        nsp_probability=0.5,\n",
        "    ):\n",
        "        # 여기 부분은 학습 데이터를 caching하는 부분입니다 :-)\n",
        "        assert os.path.isfile(file_path), f\"Input file path {file_path} not found\"\n",
        "\n",
        "        self.block_size = block_size - tokenizer.num_special_tokens_to_add(pair=True) # max embedding token size\n",
        "        self.short_seq_probability = short_seq_probability\n",
        "        self.nsp_probability = nsp_probability\n",
        "\n",
        "        directory, filename = os.path.split(file_path)\n",
        "        cached_features_file = os.path.join(\n",
        "            directory,\n",
        "            \"cached_nsp_{}_{}_{}\".format(\n",
        "                tokenizer.__class__.__name__,\n",
        "                str(block_size),\n",
        "                filename,\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        lock_path = cached_features_file + \".lock\"\n",
        "\n",
        "        # Input file format:\n",
        "        # (1) One sentence per line. These should ideally be actual sentences, not\n",
        "        # entire paragraphs or arbitrary spans of text. (Because we use the\n",
        "        # sentence boundaries for the \"next sentence prediction\" task).\n",
        "        # (2) Blank lines between documents. Document boundaries are needed so\n",
        "        # that the \"next sentence prediction\" task doesn't span between documents.\n",
        "        #\n",
        "        # Example:\n",
        "        # I am very happy.\n",
        "        # Here is the second sentence.\n",
        "        #\n",
        "        # A new document.\n",
        "\n",
        "        with FileLock(lock_path):\n",
        "            if os.path.exists(cached_features_file) and not overwrite_cache:\n",
        "                start = time.time()\n",
        "                with open(cached_features_file, \"rb\") as handle:\n",
        "                    self.examples = pickle.load(handle)\n",
        "                logger.info(\n",
        "                    f\"Loading features from cached file {cached_features_file} [took %.3f s]\", time.time() - start\n",
        "                )\n",
        "            else: # cash가 없는 경우\n",
        "                logger.info(f\"Creating features from dataset file at {directory}\")\n",
        "                # 여기서부터 본격적으로 dataset을 만듭니다.\n",
        "                self.documents = [[]] # document 단위로 학습이 이루어짐\n",
        "                with open(file_path, encoding=\"utf-8\") as f:\n",
        "                    while True: # 일단 문장을 읽고\n",
        "                        line = f.readline()\n",
        "                        if not line:\n",
        "                            break\n",
        "                        line = line.strip() # 공백 및 문장 바꾸는 기호 제거\n",
        "\n",
        "                        # 이중 띄어쓰기가 발견된다면, 나왔던 문장들을 모아 하나의 문서로 묶어버립니다.\n",
        "                        ## documents 마지막에 token을 append함\n",
        "                        # 즉, 문단 단위로 데이터를 저장합니다.\n",
        "                        if not line and len(self.documents[-1]) != 0:\n",
        "                            self.documents.append([]) \n",
        "                        tokens = tokenizer.tokenize(line)\n",
        "                        tokens = tokenizer.convert_tokens_to_ids(tokens)\n",
        "                        if tokens:\n",
        "                            self.documents[-1].append(tokens)\n",
        "                # 이제 코퍼스 전체를 읽고, 문서 데이터를 생성했습니다! :-)\n",
        "                logger.info(f\"Creating examples from {len(self.documents)} documents.\") # examples : 데이터에 사용되는 데이터 덩어리\n",
        "                self.examples = []\n",
        "                # 본격적으로 학습을 위한 데이터로 변형시켜볼까요?\n",
        "                for doc_index, document in enumerate(self.documents):\n",
        "                    self.create_examples_from_document(document, doc_index) # 함수로 가봅시다. # 최종 dataset이 만들어짐\n",
        "\n",
        "                start = time.time()\n",
        "                with open(cached_features_file, \"wb\") as handle:\n",
        "                    pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "                logger.info(\n",
        "                    \"Saving features into cached file %s [took %.3f s]\", cached_features_file, time.time() - start\n",
        "                )\n",
        "\n",
        "    def create_examples_from_document(self, document: List[List[int]], doc_index: int):\n",
        "        \"\"\"Creates examples for a single document.\"\"\"\n",
        "        # 문장의 앞, 뒤에 [CLS], [SEP] token이 부착되기 때문에, 내가 지정한 size에서 2 만큼 빼줍니다.\n",
        "        # 예를 들어 128 token 만큼만 학습 가능한 model을 선언했다면, 학습 데이터로부터는 최대 126 token만 가져오게 됩니다.\n",
        "        max_num_tokens = self.block_size - self.tokenizer.num_special_tokens_to_add(pair=True)\n",
        "\n",
        "        # We *usually* want to fill up the entire sequence since we are padding\n",
        "        # to `block_size` anyways, so short sequences are generally wasted\n",
        "        # computation. However, we *sometimes*\n",
        "        # (i.e., short_seq_prob == 0.1 == 10% of the time) want to use shorter\n",
        "        # sequences to minimize the mismatch between pretraining and fine-tuning.\n",
        "        # The `target_seq_length` is just a rough target however, whereas\n",
        "        # `block_size` is a hard limit.\n",
        "\n",
        "        # 여기가 재밌는 부분인데요!\n",
        "        # 위에서 설명했듯이, 학습 데이터는 126 token(128-2)을 채워서 만들어지는게 목적입니다.\n",
        "        # 하지만 나중에 BERT를 사용할 때, 126 token 이내의 짧은 문장을 테스트하는 경우도 분명 많을 것입니다 :-)\n",
        "        ## 512 token을 다 채워서 학습한 BERT model의 경우 짧은 문장을 처리할 능력이 없을 수 있음\n",
        "        # 그래서 short_seq_probability 만큼의 데이터에서는 2-126 사이의 random 값으로 학습 데이터를 만들게 됩니다.\n",
        "        ## 더 다양한 데이터를 반영해서 처리 가능함\n",
        "        target_seq_length = max_num_tokens # BERT가 학습할 때, segmentA와 segmentB가 합쳐져서 학습됨\n",
        "        if random.random() < self.short_seq_probability: # random.random() : 0.0 ~ 1 사이의 값으로 return\n",
        "            target_seq_length = random.randint(2, max_num_tokens)\n",
        "\n",
        "        current_chunk = []  # a buffer stored current working segments\n",
        "        current_length = 0\n",
        "        i = 0\n",
        "\n",
        "        # 데이터 구축의 단위는 document 입니다\n",
        "        # 이 때, 무조건 문장_1[SEP]문장_2 이렇게 만들어지는 것이 아니라,\n",
        "        # 126 token을 꽉 채울 수 있게 문장_1+문장_2[SEP]문장_3+문장_4 형태로 만들어질 수 있습니다.\n",
        "        while i < len(document):\n",
        "            segment = document[i]\n",
        "            current_chunk.append(segment)\n",
        "            current_length += len(segment)\n",
        "            if i == len(document) - 1 or current_length >= target_seq_length:\n",
        "                if current_chunk:\n",
        "                    # `a_end` is how many segments from `current_chunk` go into the `A`\n",
        "                    # (first) sentence.\n",
        "                    a_end = 1\n",
        "                    # 여기서 문장_1+문장_2 가 이루어졌을 때, 길이를 random하게 짤라버립니다 :-)\n",
        "                    ## 문장을 자른 후 [SEP] 부착하고 다음 문장을 나열함\n",
        "                    if len(current_chunk) >= 2:\n",
        "                        a_end = random.randint(1, len(current_chunk) - 1)\n",
        "                    tokens_a = []\n",
        "                    for j in range(a_end):\n",
        "                        tokens_a.extend(current_chunk[j])\n",
        "                    # 이제 [SEP] 뒷 부분인 segmentB를 살펴볼까요?\n",
        "                    tokens_b = []\n",
        "                    # 50%의 확률로 랜덤하게 다른 문장을 선택하거나, 다음 문장을 학습데이터로 만듭니다.\n",
        "                    if len(current_chunk) == 1 or random.random() < self.nsp_probability:\n",
        "                        is_random_next = True\n",
        "                        target_b_length = target_seq_length - len(tokens_a)\n",
        "\n",
        "                        # This should rarely go for more than one iteration for large\n",
        "                        # corpora. However, just to be careful, we try to make sure that\n",
        "                        # the random document is not the same as the document\n",
        "                        # we're processing.\n",
        "                        for _ in range(10):\n",
        "                            random_document_index = random.randint(0, len(self.documents) - 1)\n",
        "                            if random_document_index != doc_index:\n",
        "                                break\n",
        "                        # 여기서 랜덤하게 선택합니다 :-)\n",
        "                        random_document = self.documents[random_document_index]\n",
        "                        random_start = random.randint(0, len(random_document) - 1)\n",
        "                        for j in range(random_start, len(random_document)):\n",
        "                            tokens_b.extend(random_document[j])\n",
        "                            if len(tokens_b) >= target_b_length:\n",
        "                                break\n",
        "                        # We didn't actually use these segments so we \"put them back\" so\n",
        "                        # they don't go to waste.\n",
        "                        num_unused_segments = len(current_chunk) - a_end\n",
        "                        i -= num_unused_segments\n",
        "                    # Actual next\n",
        "                    else:\n",
        "                        is_random_next = False\n",
        "                        for j in range(a_end, len(current_chunk)):\n",
        "                            tokens_b.extend(current_chunk[j])\n",
        "\n",
        "                    # 이제 126 token을 넘는다면 truncation을 해야합니다.\n",
        "                    # 이 때, 126 token 이내로 들어온다면 행위를 멈추고,\n",
        "                    # 만약 126 token을 넘는다면, segmentA와 segmentB에서 랜덤하게 하나씩 제거합니다.\n",
        "                    ## truncation rule을 단순히 뒷부분 자르는 것으로 지정한다면, segmentA와 segmentB의 비율 차이가 심하거나\n",
        "                    ## [SEP]이내로 잘릴수도 있음\n",
        "                    def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens):\n",
        "                        \"\"\"Truncates a pair of sequences to a maximum sequence length.\"\"\"\n",
        "                        while True:\n",
        "                            total_length = len(tokens_a) + len(tokens_b)\n",
        "                            if total_length <= max_num_tokens:\n",
        "                                break\n",
        "                            trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n",
        "                            assert len(trunc_tokens) >= 1\n",
        "                            # We want to sometimes truncate from the front and sometimes from the\n",
        "                            # back to add more randomness and avoid biases.\n",
        "                            if random.random() < 0.5:\n",
        "                                del trunc_tokens[0]\n",
        "                            else:\n",
        "                                trunc_tokens.pop()\n",
        "\n",
        "                    truncate_seq_pair(tokens_a, tokens_b, max_num_tokens)\n",
        "\n",
        "                    assert len(tokens_a) >= 1\n",
        "                    assert len(tokens_b) >= 1\n",
        "\n",
        "                    # add special tokens\n",
        "                    input_ids = self.tokenizer.build_inputs_with_special_tokens(tokens_a, tokens_b) # vocab id\n",
        "                    # add token type ids, 0 for sentence a, 1 for sentence b\n",
        "                    token_type_ids = self.tokenizer.create_token_type_ids_from_sequences(tokens_a, tokens_b)\n",
        "                    \n",
        "                    # 드디어 아래 항목에 대한 데이터셋이 만들어졌습니다! :-)\n",
        "                    # 즉, segmentA[SEP]segmentB, [0, 0, .., 0, 1, 1, ..., 1], NSP 데이터가 만들어진 것입니다 :-)\n",
        "                    # 그럼 다음은.. 이 데이터에 [MASK] 를 씌워야겠죠?\n",
        "                    example = {\n",
        "                        \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
        "                        \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
        "                        \"next_sentence_label\": torch.tensor(1 if is_random_next else 0, dtype=torch.long),\n",
        "                    }\n",
        "\n",
        "                    self.examples.append(example)\n",
        "\n",
        "                current_chunk = []\n",
        "                current_length = 0\n",
        "\n",
        "            i += 1\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.examples[i]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TL9udtOCcaSW"
      },
      "source": [
        "* `DataCollatorForLanguageModeling` : 정의한 mask propability에 맞추어 masking함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRnF0cgdcJLa"
      },
      "source": [
        "dataset = TextDatasetForNextSentencePrediction(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path='[/content/my_data/wiki_small.txt]',\n",
        "    block_size=128,\n",
        "    overwrite_cache=False,\n",
        "    short_seq_probability=0.1,\n",
        "    nsp_probability=0.5,\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(    # [MASK] 를 씌우는 것은 저희가 구현하지 않아도 됩니다! :-)\n",
        "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D80t0NAqchsX"
      },
      "source": [
        "* 데이터 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sN035SQLcjLP"
      },
      "source": [
        "for example in dataset.examples[0:1]:\n",
        "    print(example)\n",
        "\n",
        "## 2 : [CLS], 3 : [SEP]\n",
        "'''\n",
        "{'input_ids': tensor([    2,     5,  5504,  9439,  2489,  2428,  2779,  1968,  5379,  3111,\n",
        "         1940,  2407,    16,  5497, 10310, 16250,   553,  1073,   822,  1464,\n",
        "         1217,   931, 16494, 12290,  1042,  3666,    16,  6528,  8936,  1022,\n",
        "         2677,  1906,    16,   174,   985,  4021,  1019,  8598,   728,  1271,\n",
        "           93,  7743,    93, 10414,  1063, 18368,  3503, 18888,    16,  6435,\n",
        "         1968,  4021,   277,  3361,   658,  1195,  2105,  1933, 17664,    93,\n",
        "          437,  1155,     3,   494,  2736,     5, 17664, 12976,     5,   377,\n",
        "         7719,    16,  4186,  6528,   750,   542,  1256,  2795,  4859,  5152,\n",
        "         4769,   174, 11846, 15561,   655,  2786,  9395,  1945,  2370,  2895,\n",
        "         2053,    14,  9874,  6528,   750,   762,  1061,  6459,  5152,  2823,\n",
        "         3950,  6528,   750,   762,  2092,  6204,  1899,    16,  2829,  4530,\n",
        "          728, 16250, 18217,  2665, 17627, 13573,  2492,    14,  3951,  1916,\n",
        "        16017,  6528,   762,  2873,     3]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
        "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
        "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
        "        1, 1, 1, 1, 1]), 'next_sentence_label': tensor(0)}\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEcdPYaIcmkX"
      },
      "source": [
        "[MASK]를 부착하는 data collator의 역할 확인\n",
        "  * labels : mask된 token의 원래 id를 알려주는 역할을 함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqaSuXvnclaY"
      },
      "source": [
        "print(data_collator(dataset.examples))\n",
        "'''\n",
        "{'input_ids': tensor([[    2,     4,  5504,  ...,   762,  2873,     3],\n",
        "        [    2,  6528,  7895,  ...,     0,     0,     0],\n",
        "        [    2,  6793,  1900,  ...,  1895,    16,     3],\n",
        "        ...,\n",
        "        [    2,  3953,  3262,  ...,     4,    16,     3],\n",
        "        [    2,     4,  2412,  ...,  1913,     4,     3],\n",
        "        [    2, 12577, 10891,  ...,  1047,  2833,     3]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
        "        [0, 0, 0,  ..., 0, 0, 0],\n",
        "        [0, 0, 0,  ..., 1, 1, 1],\n",
        "        ...,\n",
        "        [0, 0, 0,  ..., 1, 1, 1],\n",
        "        [0, 0, 0,  ..., 1, 1, 1],\n",
        "        [0, 0, 0,  ..., 1, 1, 1]]), 'next_sentence_label': tensor([0, 1, 1,  ..., 1, 1, 1]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
        "        [1, 1, 1,  ..., 0, 0, 0],\n",
        "        [1, 1, 1,  ..., 1, 1, 1],\n",
        "        ...,\n",
        "        [1, 1, 1,  ..., 1, 1, 1],\n",
        "        [1, 1, 1,  ..., 1, 1, 1],\n",
        "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([[-100,    5, -100,  ..., -100, -100, -100],\n",
        "        [-100, -100, -100,  ..., -100, -100, -100],\n",
        "        [-100, -100, -100,  ..., -100, -100, -100],\n",
        "        ...,\n",
        "        [-100, -100, -100,  ..., 1887, -100, -100],\n",
        "        [-100, 1988, -100,  ..., -100,   16, -100], # 특정 token이 16번째 vocab token으로 replace된 것을 알려줌\n",
        "        [-100, -100, -100,  ..., -100, -100, -100]])}\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fq9t2WjPc_e7"
      },
      "source": [
        "print(data_collator(dataset.examples)['input_ids'][0])\n",
        "'''\n",
        "tensor([    2,     5,  5504,  9439,     4,  2428,  2779,  1968,  5379,  3111,\n",
        "         1940,     4,    16,  5497, 10310, 16250,   553,  1073,   822,  1464,\n",
        "         1217,   931, 16494, 12290,  1042,  3666,    16,  6528,  8936,  1022,\n",
        "         6038,     4,    16,   174,   985,  4021,  1019,  8598,   728,  1271,\n",
        "           93,  7743,    93, 10414,     4, 18368,  3503, 18888,    16,  6435,\n",
        "         1968,  4021,   277,  3361,   658,  1195,  2105,  1933, 17664,    93,\n",
        "          437,  1155,     3,   494,  2736,     4, 17664, 12976,     4,   377,\n",
        "         7719,    16,  4186,  6528,   750,   542,  1256,  2795,  4859,  5152,\n",
        "         4769,   174, 11846,     4,   655,  2786,  9395,     4,  2370,  2895,\n",
        "            4,    14,  9874,  6528,   750,   762,  1061,  6459,  5152,  2823,\n",
        "         3950,  6528,   750,   762,  2092,     4,  1899,    16,  2829,  4530,\n",
        "          728, 16250, 18217,  2665, 17627, 13573,  2492,    14,  3951,  1916,\n",
        "        16017,  6528,   762,     4,     3])\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VWUYTIOdfI8"
      },
      "source": [
        "* decoding하여 관찰"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGDJGNEMdd1o"
      },
      "source": [
        "tokenizer.decode(data_collator(dataset.examples)['input_ids'][0].tolist())\n",
        "'''\n",
        "'[CLS] 주니어는 민주당 출신 미국 [MASK]번째 대통령 이다. 지미 카터는 [MASK] 섬 [MASK] 카운 [MASK] 플레인스 마을에서 [MASK]. 조지아 공과대학교를 졸업 [MASK]구와 그 후 해군에대백과사전 전함 · 원자력 · 잠수함의 승무원으로 일하였다. 1953년 미국 해군 대 [MASK] 예편하였고 이후 땅콩 경선과 면화 등을 [UNK] 많은 돈을 벌었다 [MASK] 그의 별명이 \" [SEP] 와 같이 쓸 수 있다. 그런데 formula _ 27은 formula _ 28차의 [MASK]므로, 다항식의 차수에 대한 귀납법을위원회 증명이 끝난다. [MASK] formula _ 19차 다항식의 경우, 위의 따름정리를 적용하면 이 역시 복소수체 위에서 중근을 [MASK] 경우 formula _ 19개의 근을 갖는다. [SEP]\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpI7zWQYd82O"
      },
      "source": [
        "* `Trainer`\n",
        "  * transformers에서 제공하는 train 기능\n",
        "    * `output_dir` : model이 output되는 위치\n",
        "    * `overwrite_output_dir` : model이 학습되면서 overwrite가 될건지에 대한 여부\n",
        "    * `num_train_epochs` : 주어진 데이터를 대상으로 주는 epoch 수\n",
        "    * `per_gpu_train_batch_size`\n",
        "    * `save_steps` : 저장 기준이 되는 step 수\n",
        "      * 1000 : 1000 step마다 데이터를 저장함\n",
        "    * `save_total_limit` : 데이터를 저장하는 최대 개수\n",
        "      * 2 : 마지막 2개의 데이터를 제외하고 이전 데이터들을 제거함\n",
        "    * `logging_steps` : log를 찍어주는 기준이 되는 step 수\n",
        "      * 100 : 100 step마다 log를 찍어줌"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQ10FvQFdpDo"
      },
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='model_output',\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=10,\n",
        "    per_gpu_train_batch_size=32,\n",
        "    save_steps=1000,\n",
        "    save_total_limit=2,\n",
        "    logging_steps=100\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=dataset\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4Lt7U-ddrnz"
      },
      "source": [
        "trainer.train() # wiki 전체 데이터로 학습 시, 1 epoch에 9시간 정도 소요됩니다!! "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VrPkYExflWE"
      },
      "source": [
        "* 특정 directory로 model을 저장함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcE0QEInd3fr"
      },
      "source": [
        "trainer.save_model('./model_output')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wC_lc8Evfqe4"
      },
      "source": [
        "* [MASK] 테스트\n",
        "  * 실습에서 사용된 model은 작고 빠르게 학습했기 때문에 성능이 좋지는 않음"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cmOVY66e-tP"
      },
      "source": [
        "from transformers import BertForMaskedLM, pipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oDYkFIAf2HR"
      },
      "source": [
        "* [MASK] task를 위해서 model을 다르게 load함\n",
        "\n",
        "* `BertForMaskedLM`을 사용하여 학습한 model 가져오기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8s2BFbDRe_k1"
      },
      "source": [
        "my_model = BertForMaskedLM.from_pretrained('model_output')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4uk005ZgIxJ"
      },
      "source": [
        "* tokenizer 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tE8_h8efAgm"
      },
      "source": [
        "tokenizer.tokenize('이순신은 [MASK] 중기의 무신이다.')\n",
        "# ['이', '##순', '##신은', '[MASK]', '중', '##기의', '무신', '##이다', '.']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ju33ZssXgLZP"
      },
      "source": [
        "* `pipeline` 생성\n",
        "  * model, tokenizer 명시\n",
        "  * GPU를 사용하는 경우 device 명시해야함\n",
        "    * 0번 GPU를 사용하는 경우 `device=0`으로 명시\n",
        "    * 명시하지 않으면 CPU로 동작함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQHy74eufBOW"
      },
      "source": [
        "nlp_fill = pipeline('fill-mask', top_k=5, model=my_model, tokenizer=tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3ovhYB-fCCv"
      },
      "source": [
        "nlp_fill('이순신은 [MASK] 중기의 무신이다.')\n",
        "'''\n",
        "[{'score': 0.030770400539040565,\n",
        "  'sequence': '[CLS] 이순신은, 중기의 무신이다. [SEP]',\n",
        "  'token': 14,\n",
        "  'token_str': ','},\n",
        " {'score': 0.03006444126367569,\n",
        "  'sequence': '[CLS] 이순신은. 중기의 무신이다. [SEP]',\n",
        "  'token': 16,\n",
        "  'token_str': '.'},\n",
        " {'score': 0.012540608644485474,\n",
        "  'sequence': '[CLS] 이순신은 _ 중기의 무신이다. [SEP]',\n",
        "  'token': 63,\n",
        "  'token_str': '_'},\n",
        " {'score': 0.008801406249403954,\n",
        "  'sequence': '[CLS] 이순신은 있다 중기의 무신이다. [SEP]',\n",
        "  'token': 1888,\n",
        "  'token_str': '있다'},\n",
        " {'score': 0.008582047186791897,\n",
        "  'sequence': '[CLS] 이순신은 formula 중기의 무신이다. [SEP]',\n",
        "  'token': 1895,\n",
        "  'token_str': 'formula'}]\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOETLyEQfC8K"
      },
      "source": [
        "nlp_fill('[MASK]는 조선 중기의 무신이다.')\n",
        "'''\n",
        "\n",
        "[{'score': 0.025915304198861122,\n",
        "  'sequence': '[CLS]. 는 조선 중기의 무신이다. [SEP]',\n",
        "  'token': 16,\n",
        "  'token_str': '.'},\n",
        " {'score': 0.024867292493581772,\n",
        "  'sequence': '[CLS], 는 조선 중기의 무신이다. [SEP]',\n",
        "  'token': 14,\n",
        "  'token_str': ','},\n",
        " {'score': 0.008652042597532272,\n",
        "  'sequence': '[CLS]에 는 조선 중기의 무신이다. [SEP]',\n",
        "  'token': 1018,\n",
        "  'token_str': '##에'},\n",
        " {'score': 0.008601967245340347,\n",
        "  'sequence': '[CLS] formula 는 조선 중기의 무신이다. [SEP]',\n",
        "  'token': 1895,\n",
        "  'token_str': 'formula'},\n",
        " {'score': 0.008583025075495243,\n",
        "  'sequence': '[CLS] 이 는 조선 중기의 무신이다. [SEP]',\n",
        "  'token': 704,\n",
        "  'token_str': '이'}]\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uuorg7ZfENO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}