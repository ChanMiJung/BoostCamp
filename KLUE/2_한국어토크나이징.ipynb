{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2_한국어토크나이징.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyB6tgrpBFcO"
      },
      "source": [
        "# 한국어 Tokenizing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4TSDJOWYgm3"
      },
      "source": [
        "* Tokenizing의 목적\n",
        "  1. 의미를 지닌 단위로 자연어를 분절\n",
        "  2. Model의 학습 시, 동일한 size로 입력\n",
        "    * model에 입력시 수학적으로 (vocab 번호)입력되어야 함\n",
        "    * model은 size(batch size)가 고정이 되어있기 때문에 input도 model size에 맞춰줘야함\n",
        "      * batch : model의 input을 행렬로 처리\n",
        "    * matrix의 끝부분을 padding 처리를 통해서 size를 맞춤\n",
        "* tokenizer는 특정 사이즈로 token의 개수를 조절하는 함수가 필수로 포함되어야 함"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PoThorBD5oM"
      },
      "source": [
        "* 한국어 tokenizing 단계\n",
        "  1. 어절 단위\n",
        "  2. 형태소 단위\n",
        "  3. 음절 단위\n",
        "  4. 자소 단위\n",
        "  5. WordPiece 단위"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4-3ze9dECjY"
      },
      "source": [
        "## 실습용 데이터 준비\n",
        "* '2_자연어전처리' 과정을 통해 전처리가 완료된 파일 준비"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1F2Q2_khDna"
      },
      "source": [
        "* 데이터 확인\n",
        "  * `open(파일주소, 형식, encoding=)`으로 파일 읽어 옴\n",
        "    * `r` : read, 파일 내용을 읽어옴\n",
        "    * `encoding=` : encoding format 지정\n",
        "      * 한글은 encoding format이 맞지 않으면 글자가 깨지는 현상이 발생함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EeBLRiMABB6X"
      },
      "source": [
        "data = open('my_data/wiki_small.txt', 'r', encoding='utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hE9MebyMhGMc"
      },
      "source": [
        "lines = data.readlines() # 전체 문장을 list에 저장하는 함수입니다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qTPLp4PhHPr"
      },
      "source": [
        "for line in lines[0:10]:\n",
        "    print(line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCKi3GrbhPgL"
      },
      "source": [
        "## 1. 어절 단위 tokenizing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1AS8WgxYWiv"
      },
      "source": [
        "* 어절 단위의 tokenizing\n",
        "  * 띄어쓰기 단위로 분리함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-RyC9FYYVwM"
      },
      "source": [
        "text = \"이순신은 조선 중기의 무신이다.\"\n",
        "tokenized_text = text.split(\" \")    # split 함수는 입력 string에 대해서 특정 string을 기반으로 분리해줍니다.\n",
        "print(tokenized_text)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyJ3Q-OfaFGl"
      },
      "source": [
        "* padding 처리\n",
        "  * vocab에 'padding'에 대한 번호가 존재한다면 tokenize_text에 자연어로 들어가도 됨\n",
        "\n",
        "* 아래 예제에서 token이 4개이고 `max_seq_length`가 10이기 때문에 'padding'이 6개가 추가됨\n",
        "  * 이 기능이 BERT에 내장되어 있음"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6sMSm_TaTDd"
      },
      "source": [
        "# 예제\n",
        "max_seq_length = 10\n",
        "# padding\n",
        "tokenized_text += [\"padding\"] * (max_seq_length - len(tokenized_text))\n",
        "print(tokenized_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4qRCDblanDu"
      },
      "source": [
        "* 아래 예제에서 token이 4개이고 `max_seq_length`가 2이기 때문에 앞에서부터 2개의 token을 제외한 나머지를 잘라냄(truncation)\n",
        "  * 이 기능이 BERT에 내장되어 있음"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odrYJ5TfbERs"
      },
      "source": [
        "# 예제\n",
        "max_seq_length = 2\n",
        "# filtering\n",
        "tokenized_text = tokenized_text[0:max_seq_length]\n",
        "print(tokenized_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7T5mEJAnbLps"
      },
      "source": [
        "* tokenizer class 구현\n",
        "  * token의 개수가 부족할 때 padding 처리하고, 개수가 많을 때는 token을 잘라서 반환하는 함수 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNaLNZHebSTF"
      },
      "source": [
        "class Tokenizer:\n",
        "    def __init__(self):\n",
        "        self.tokenizer_type_list = [\"word\"]\n",
        "        self.pad_token = \"<pad>\"\n",
        "        self.max_seq_length = 10\n",
        "        self.padding = False\n",
        "    def tokenize(self, text, tokenizer_type): \n",
        "        assert tokenizer_type in self.tokenizer_type_list, \"정의되지 않은 tokenizer_type입니다.\"\n",
        "        if tokenizer_type == \"word\":\n",
        "            tokenized_text = text.split(\" \")\n",
        "        if self.padding:\n",
        "            tokenized_text += [self.pad_token] * (self.max_seq_length - len(tokenized_text))\n",
        "            return tokenized_text[:self.max_seq_length]\n",
        "        else:\n",
        "            return tokenized_text[:self.max_seq_length]\n",
        "    def batch_tokenize(self, texts, tokenizer_type):\n",
        "        for i, text in enumerate(texts):\n",
        "            texts[i] = self.tokenize(text, tokenizer_type)\n",
        "        return texts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHGm1YvdbS9Y"
      },
      "source": [
        "my_tokenizer = Tokenizer()\n",
        "my_tokenizer.pad_token = \"[PAD]\"\n",
        "my_tokenizer.max_seq_length = 10\n",
        "my_tokenizer.padding = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sMURyd4bT3w"
      },
      "source": [
        "print(my_tokenizer.tokenize(\"이순신은 조선 중기의 무신이다.\", \"word\"))\n",
        "print(my_tokenizer.batch_tokenize([\"이순신은 조선 중기의 무신이다.\", \"그는 임진왜란을 승리로 이끌었다.\"], \"word\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqA1BaC0bcJy"
      },
      "source": [
        "## 2. 형태소 단위 tokenizing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLbpffE0bpAx"
      },
      "source": [
        "* 형태소 분석기 Mecab 사용\n",
        "  '2_자연어처리'에서 사용되었음"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qj4zjHTzdQ_u"
      },
      "source": [
        "# !pip install konlpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-jugsZadSq5"
      },
      "source": [
        "# !bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cgMEc2cdYYV"
      },
      "source": [
        "from konlpy.tag import Mecab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXClqZ7kdiRO"
      },
      "source": [
        "* 형태소 분석기 성능 테스트시 2개의 문장이 많이 사용됨\n",
        "  * '아버지가방에들어가신다.'\n",
        "  * '이순신은 조선 중기의 무신이다.'\n",
        "    * 이순신 -> PS\n",
        "    * 조선 -> DT TI\n",
        "    * 중기 -> TI\n",
        "    * 무신 -> OC\n",
        "    * 이순신 - 직업 - 무신\n",
        "    * 이순신 - 출생지 - 조선"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaLx-1wndh6m"
      },
      "source": [
        "# 예제\n",
        "print(mecab.pos(\"아버지가방에들어가신다.\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdFlqQPudZiv"
      },
      "source": [
        "mecab = Mecab()\n",
        "text = \"이순신은 조선 중기의 무신이다.\"\n",
        "\n",
        "tokenized_text = [lemma[0] for lemma in mecab.pos(text)]\n",
        "print(tokenized_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CktV5Xj4eRKg"
      },
      "source": [
        "* `Tokenizer` class에 형태소 tokenizer를 추가"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TcP-hTjeXSC"
      },
      "source": [
        "class Tokenizer:\n",
        "    def __init__(self):\n",
        "        self.tokenizer_type_list = [\"word\", \"morph\"]\n",
        "        self.pad_token = \"<pad>\"\n",
        "        self.max_seq_length = 10\n",
        "        self.padding = False\n",
        "    def tokenize(self, text, tokenizer_type): \n",
        "        assert tokenizer_type in self.tokenizer_type_list, \"정의되지 않은 tokenizer_type입니다.\"\n",
        "        if tokenizer_type == \"word\":\n",
        "            tokenized_text = text.split(\" \")\n",
        "        elif tokenizer_type == \"morph\":\n",
        "            tokenized_text = [lemma[0] for lemma in mecab.pos(text)]\n",
        "        if self.padding:\n",
        "            tokenized_text += [self.pad_token] * (self.max_seq_length - len(tokenized_text))\n",
        "            return tokenized_text[:self.max_seq_length]\n",
        "        else:\n",
        "            return tokenized_text[:self.max_seq_length]\n",
        "    def batch_tokenize(self, texts, tokenizer_type):\n",
        "        for i, text in enumerate(texts):\n",
        "            texts[i] = self.tokenize(text, tokenizer_type)\n",
        "        return texts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8OhM20teYVF"
      },
      "source": [
        "my_tokenizer = Tokenizer()\n",
        "my_tokenizer.pad_token = \"[PAD]\"\n",
        "my_tokenizer.max_seq_length = 10\n",
        "my_tokenizer.padding = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NolB4YK6el_g"
      },
      "source": [
        "print(my_tokenizer.tokenize(\"이순신은 조선 중기의 무신이다.\", \"morph\"))\n",
        "print(my_tokenizer.batch_tokenize([\"이순신은 조선 중기의 무신이다.\", \"그는 임진왜란을 승리로 이끌었다.\"], \"morph\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeRx7qDweodW"
      },
      "source": [
        "## 3. 음절 단위 tokenizing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBipzk2Yerfb"
      },
      "source": [
        "* 하나의 자연어를 한 글자씩 분리함"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCizfS5qfYgs"
      },
      "source": [
        "* `list()`로 음절단위 tokenizing함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MH0hDtE1ewBO"
      },
      "source": [
        "# 예제\n",
        "text = \"이순신은 조선 중기의 무신이다.\"\n",
        "tokenized_text = list(text)    # split 함수는 입력 string에 대해서 특정 string을 기반으로 분리해줍니다.\n",
        "print(tokenized_text)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNLBzKhoexuo"
      },
      "source": [
        "* `Tokenizer` class에 음절 tokenizer 추가"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Od17y8jSe11w"
      },
      "source": [
        "class Tokenizer:\n",
        "    def __init__(self):\n",
        "        self.tokenizer_type_list = [\"word\", \"morph\", \"syllable\"]\n",
        "        self.pad_token = \"<pad>\"\n",
        "        self.max_seq_length = 10\n",
        "        self.padding = False\n",
        "    def tokenize(self, text, tokenizer_type): \n",
        "        assert tokenizer_type in self.tokenizer_type_list, \"정의되지 않은 tokenizer_type입니다.\"\n",
        "        if tokenizer_type == \"word\":\n",
        "            tokenized_text = text.split(\" \")\n",
        "        elif tokenizer_type == \"morph\":\n",
        "            tokenized_text = [lemma[0] for lemma in mecab.pos(text)]\n",
        "        elif tokenizer_type == \"syllable\":\n",
        "            tokenized_text = list(text)\n",
        "        if self.padding:\n",
        "            tokenized_text += [self.pad_token] * (self.max_seq_length - len(tokenized_text))\n",
        "            return tokenized_text[:self.max_seq_length]\n",
        "        else:\n",
        "            return tokenized_text[:self.max_seq_length]\n",
        "    def batch_tokenize(self, texts, tokenizer_type):\n",
        "        for i, text in enumerate(texts):\n",
        "            texts[i] = self.tokenize(text, tokenizer_type)\n",
        "        return texts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "p8ky7C2Se2vO",
        "outputId": "18e9ff26-0e45-41aa-874e-5e75699dddee"
      },
      "source": [
        "my_tokenizer = Tokenizer()\n",
        "my_tokenizer.pad_token = \"[PAD]\"\n",
        "my_tokenizer.max_seq_length = 20\n",
        "my_tokenizer.padding = True"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-4c8de5bca3ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmy_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmy_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"[PAD]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmy_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_seq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmy_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Tokenizer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fy_3JDQye3Dy"
      },
      "source": [
        "print(my_tokenizer.tokenize(\"이순신은 조선 중기의 무신이다.\", \"syllable\"))\n",
        "print(my_tokenizer.batch_tokenize([\"이순신은 조선 중기의 무신이다.\", \"그는 임진왜란을 승리로 이끌었다.\"], \"syllable\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQd45LfRe-F2"
      },
      "source": [
        "## 4. 자소 단위 tokenizing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOj5GA8lfAgF"
      },
      "source": [
        "* 한글은 하나의 문자도 최대 초성, 중성, 종성 총 3개의 자소로 분리가 가능함"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zi-MhsHkfwaO"
      },
      "source": [
        "* 자소를 분리할 때 대부분 유니코드를 사용함\n",
        "  * python은 쉽게 자소를 분리할 수 있는 라이브러리가 존재함"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zS3EhamMfF6f"
      },
      "source": [
        "* hgtk\n",
        "  * 자소 분리 라이브러리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqzxmfrXfK2L"
      },
      "source": [
        "# !pip install hgtk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fl5BevL6fMrV"
      },
      "source": [
        "import hgtk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pY6LhQfigFAm"
      },
      "source": [
        "* `tokenized_text`에 음절 단위를 구분해주는 'ᴥ'기호가 포함되어 있음\n",
        "  * 음절 구분이 되어있지 않으면 혼란이 올 수 있음"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uh1LQlQIfNlb"
      },
      "source": [
        "text = \"이순신은 조선 중기의 무신이다.\"\n",
        "tokenized_text = list(hgtk.text.decompose(text))\n",
        "print(tokenized_text)\n",
        "# ㅇ ㅣ ㅅ ㅜ ㄴ ㅅ ㅣ ... # 음절 구분이 어려움 \n",
        "# ['ㅇ', 'ㅣ', 'ᴥ', 'ㅅ', 'ㅜ', 'ㄴ', 'ᴥ', 'ㅅ', 'ㅣ', 'ㄴ', 'ᴥ', 'ㅇ', 'ㅡ', 'ㄴ', 'ᴥ', ' ', 'ㅈ', 'ㅗ', 'ᴥ', 'ㅅ', 'ㅓ', 'ㄴ', 'ᴥ', ' ', 'ㅈ', 'ㅜ', 'ㅇ', 'ᴥ', 'ㄱ', 'ㅣ', 'ᴥ', 'ㅇ', 'ㅢ', 'ᴥ', ' ', 'ㅁ', 'ㅜ', 'ᴥ', 'ㅅ', 'ㅣ', 'ㄴ', 'ᴥ', 'ㅇ', 'ㅣ', 'ᴥ', 'ㄷ', 'ㅏ', 'ᴥ', '.']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5Tbh97kfdWr"
      },
      "source": [
        "* `Tokenizer` class에 자소 단위 tokenizer 추가"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2QuqARCfhND"
      },
      "source": [
        "class Tokenizer:\n",
        "    def __init__(self):\n",
        "        self.tokenizer_type_list = [\"word\", \"morph\", \"syllable\", \"jaso\"]\n",
        "        self.pad_token = \"<pad>\"\n",
        "        self.max_seq_length = 10\n",
        "        self.padding = False\n",
        "    def tokenize(self, text, tokenizer_type): \n",
        "        assert tokenizer_type in self.tokenizer_type_list, \"정의되지 않은 tokenizer_type입니다.\"\n",
        "        if tokenizer_type == \"word\":\n",
        "            tokenized_text = text.split(\" \")\n",
        "        elif tokenizer_type == \"morph\":\n",
        "            tokenized_text = [lemma[0] for lemma in mecab.pos(text)]\n",
        "        elif tokenizer_type == \"syllable\":\n",
        "            tokenized_text = list(text)\n",
        "        elif tokenizer_type == \"jaso\":\n",
        "            tokenized_text = list(hgtk.text.decompose(text))\n",
        "        if self.padding:\n",
        "            tokenized_text += [self.pad_token] * (self.max_seq_length - len(tokenized_text))\n",
        "            return tokenized_text[:self.max_seq_length]\n",
        "        else:\n",
        "            return tokenized_text[:self.max_seq_length]\n",
        "    def batch_tokenize(self, texts, tokenizer_type):\n",
        "        for i, text in enumerate(texts):\n",
        "            texts[i] = self.tokenize(text, tokenizer_type)\n",
        "        return texts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpydZAT5fiLP"
      },
      "source": [
        "my_tokenizer = Tokenizer()\n",
        "my_tokenizer.pad_token = \"[PAD]\"\n",
        "my_tokenizer.max_seq_length = 20\n",
        "my_tokenizer.padding = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBB0WlmEfi2_"
      },
      "source": [
        "print(my_tokenizer.tokenize(\"이순신은 조선 중기의 무신이다.\", \"jaso\"))\n",
        "print(my_tokenizer.batch_tokenize([\"이순신은 조선 중기의 무신이다.\", \"그는 임진왜란을 승리로 이끌었다.\"], \"jaso\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DH191klfkmh"
      },
      "source": [
        "## 5. WordPiece tokenizing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFwrlEHhfnaJ"
      },
      "source": [
        "# !pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OcIh2ZaforP"
      },
      "source": [
        "# !mkdir wordPieceTokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXUkNA3iiFsD"
      },
      "source": [
        "* `BertWordPieceTokenizer`\n",
        "  * BERT를 위한 word piece tokenizer\n",
        "  * vocabulary dictionary를 생성함\n",
        "\n",
        "  \n",
        "* tokenizer 에 대한 옵션\n",
        "  * `clean_text`: 기대하는 tokenizer의 형태\n",
        "    * `True` : input sequence의 단어 사이의 띄어쓰기 제거\n",
        "      * ex. [이순신, ##은, ' ', 조선] -> [이순신, ##은, 조선]\n",
        "      * BERT에서는 '##'으로 어절의 위치를 구분할 수 있기 때문에 띄어쓰기가 없어도 됨\n",
        "  * `handle_chinese_chars` : 중국어, 일본어 분리 방법\n",
        "    * `True` : 음절 단위로 분리\n",
        "      * vocab도 분리됨\n",
        "      * '##'이 붙지 않음\n",
        "  * `strip_accents` : camel case로 작성된 문자열을 분리여부\n",
        "    * `True` : camel case로 작성된 문자열을 분리함\n",
        "  * `lowercase` : 모든 알파벳을 소문자로 변경여부\n",
        "    * `True` : 모든 알파벳을 소문자로 변경\n",
        "    * `False`가 성능이 더 좋음"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPCvBEcfkinR"
      },
      "source": [
        "* `train()` 옵션\n",
        "  * `files` : 파일명(주소포함) 입력\n",
        "    * corpus를 input으로 넣음\n",
        "  * `vocab_size` : 만들고 싶은 vocab의 size\n",
        "    * vocab을 전부 채울때까지 wordpiece 알고리즘이 동작하기 때문에, size가 크면 음절단위로 만들어짐\n",
        "  * `min_frequency` : 단어 등장 최소 빈도 수\n",
        "    * `min_frequency=2` : 2번 이상 등장한 경우에만 vocab에 추가\n",
        "  * `special_tokens` : 내부적으로 이미 정의되어 있음\n",
        "    * \"[PAD]\" : padding token\n",
        "    * \"[UNK]\" : unknown token\n",
        "    * \"[CLS]\" : 맨 앞에 붙는 token\n",
        "    * \"[SEP]\" : 맨 뒤에 붙는 token\n",
        "    * \"[MASK]\" : masking을 위한 token\n",
        "  * `wordpieces_prefix=\"##\"` : prefix를 '##'으로 지정"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEHYBy_umHjh"
      },
      "source": [
        "* `save_model(directory, )`\n",
        "  * model을 저장함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXquQutsf-DL"
      },
      "source": [
        "from tokenizers import BertWordPieceTokenizer\n",
        "\n",
        "# Initialize an empty tokenizer\n",
        "wp_tokenizer = BertWordPieceTokenizer(\n",
        "    clean_text=True,    # [이순신, ##은, ' ', 조선]\n",
        "    handle_chinese_chars=True,\n",
        "    strip_accents=False,    # True: [YepHamza] -> [Yep, Hamza]\n",
        "    lowercase=False,\n",
        ")\n",
        "\n",
        "# And then train\n",
        "wp_tokenizer.train(\n",
        "    files=\"my_data/wiki_20190620_small.txt\",\n",
        "    vocab_size=10000,\n",
        "    min_frequency=2,\n",
        "    show_progress=True,\n",
        "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"],\n",
        "    limit_alphabet=1000,\n",
        "    wordpieces_prefix=\"##\"\n",
        ")\n",
        "\n",
        "# Save the files\n",
        "wp_tokenizer.save_model(\"wordPieceTokenizer\", \"my_tokenizer\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IX-nt7uqgfsK"
      },
      "source": [
        "print(wp_tokenizer.get_vocab_size())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXx7eX0amhZz"
      },
      "source": [
        "* tokenizer의 `encode()`함수를 통해 문장을 encoding할 수 있음\n",
        "\n",
        "* `print(tokenized_text)`\n",
        "  * transformers에서 정의한 `Encoding` class 내에 token의 개수(num_tokens), 가져올 수 있는 정보(attributes)\n",
        "    * ids : vocab id\n",
        "    * type_ids : BERT에서는 segment type"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLnNDVnWgrRi"
      },
      "source": [
        "text = \"이순신은 조선 중기의 무신이다.\"\n",
        "tokenized_text = wp_tokenizer.encode(text)\n",
        "print(tokenized_text)\n",
        "# Encoding(num_tokens=10, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
        "print(tokenized_text.tokens)\n",
        "# ['이', '##순', '##신은', '조선', '중', '##기의', '무', '##신이', '##다', '.']\n",
        "print(tokenized_text.ids)\n",
        "# [706, 1246, 7604, 2000, 754, 2602, 453, 8470, 1031, 16]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Cx2GCqzgu7f"
      },
      "source": [
        "* `Tokenizer` class에 wordpiece tokenizer 추가"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBQYlxnrgsFI"
      },
      "source": [
        "class Tokenizer:\n",
        "    def __init__(self):\n",
        "        self.tokenizer_type_list = [\"word\", \"morph\", \"syllable\", \"jaso\", \"wordPiece\"]\n",
        "        self.pad_token = \"<pad>\"\n",
        "        self.max_seq_length = 10\n",
        "        self.padding = False\n",
        "    def tokenize(self, text, tokenizer_type): \n",
        "        assert tokenizer_type in self.tokenizer_type_list, \"정의되지 않은 tokenizer_type입니다.\"\n",
        "        if tokenizer_type == \"word\":\n",
        "            tokenized_text = text.split(\" \")\n",
        "        elif tokenizer_type == \"morph\":\n",
        "            tokenized_text = [lemma[0] for lemma in mecab.pos(text)]\n",
        "        elif tokenizer_type == \"syllable\":\n",
        "            tokenized_text = list(text)\n",
        "        elif tokenizer_type == \"jaso\":\n",
        "            tokenized_text = list(hgtk.text.decompose(text))\n",
        "        elif tokenizer_type == \"wordPiece\":\n",
        "            tokenized_text = wp_tokenizer.encode(text).tokens\n",
        "        if self.padding:\n",
        "            tokenized_text += [self.pad_token] * (self.max_seq_length - len(tokenized_text))\n",
        "            return tokenized_text[:self.max_seq_length]\n",
        "        else:\n",
        "            return tokenized_text[:self.max_seq_length]\n",
        "    def batch_tokenize(self, texts, tokenizer_type):\n",
        "        for i, text in enumerate(texts):\n",
        "            texts[i] = self.tokenize(text, tokenizer_type)\n",
        "        return texts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZvdcn82g0Kk"
      },
      "source": [
        "my_tokenizer = Tokenizer()\n",
        "my_tokenizer.pad_token = \"[PAD]\"\n",
        "my_tokenizer.max_seq_length = 10\n",
        "my_tokenizer.padding = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwdrbb3Qg07N"
      },
      "source": [
        "print(my_tokenizer.tokenize(\"이순신은 조선 중기의 무신이다.\", \"wordPiece\"))\n",
        "print(my_tokenizer.batch_tokenize([\"이순신은 조선 중기의 무신이다.\", \"그는 임진왜란을 승리로 이끌었다.\"], \"wordPiece\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2bOVE3qnruH"
      },
      "source": [
        "* 구현된 tokenizing 함수 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnT-WsMPnuXP"
      },
      "source": [
        "print(my_tokenizer.tokenize(\"이순신은 조선 중기의 무신이다.\", \"word\"))\n",
        "print(my_tokenizer.tokenize(\"이순신은 조선 중기의 무신이다.\", \"morph\"))\n",
        "print(my_tokenizer.tokenize(\"이순신은 조선 중기의 무신이다.\", \"syllable\"))\n",
        "print(my_tokenizer.tokenize(\"이순신은 조선 중기의 무신이다.\", \"jaso\"))\n",
        "print(my_tokenizer.tokenize(\"이순신은 조선 중기의 무신이다.\", \"wordPiece\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCO1kwvfnwcp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}