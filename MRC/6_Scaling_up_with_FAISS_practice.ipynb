{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6_Scaling_up_with_FAISS_practice.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T72UNmyC8iMw"
      },
      "source": [
        "# FAISS 실습"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aWA3tKa8k88"
      },
      "source": [
        "## Requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eRDocMF8mbW"
      },
      "source": [
        "!pip install datasets\n",
        "!pip install transformers\n",
        "!pip install faiss-cpu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXejuldb8nMB"
      },
      "source": [
        "## passage / question encoder 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1sAxXz78rU6"
      },
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "import numpy as np\n",
        "from tqdm import tqdm, trange\n",
        "import random\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import BertModel, BertPreTrainedModel, AdamW, TrainingArguments, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import (DataLoader, RandomSampler, TensorDataset, SequentialSampler)\n",
        " \n",
        "torch.manual_seed(2021)\n",
        "torch.cuda.manual_seed(2021)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5I7H1S5F8srr"
      },
      "source": [
        "dataset = load_dataset(\"squad_kor_v1\")\n",
        "corpus = list(set([example['context'] for example in dataset['train']]))\n",
        "len(corpus)\n",
        "\n",
        "model_checkpoint = \"bert-base-multilingual-cased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGwKmwOw-gTl"
      },
      "source": [
        "* training dataset 준비"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qw2ioHDE8cMu"
      },
      "source": [
        "# Use subset (128 examples) of original training dataset \n",
        "sample_idx = np.random.choice(range(len(dataset['train'])), 128)\n",
        "training_dataset = dataset['train'][sample_idx]\n",
        "\n",
        "q_seqs = tokenizer(training_dataset['question'], padding=\"max_length\", truncation=True, return_tensors='pt')\n",
        "p_seqs = tokenizer(training_dataset['context'], padding=\"max_length\", truncation=True, return_tensors='pt')\n",
        "\n",
        "train_dataset = TensorDataset(p_seqs['input_ids'], p_seqs['attention_mask'], p_seqs['token_type_ids'], \n",
        "                        q_seqs['input_ids'], q_seqs['attention_mask'], q_seqs['token_type_ids'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6Ma_LkA-jXN"
      },
      "source": [
        "* BERT encoder 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNE6g-Mj-l5z"
      },
      "source": [
        "class BertEncoder(BertPreTrainedModel):\n",
        "  def __init__(self, config):\n",
        "    super(BertEncoder, self).__init__(config)\n",
        " \n",
        "    self.bert = BertModel(config)\n",
        "    self.init_weights()\n",
        "      \n",
        "  def forward(self, input_ids, \n",
        "              attention_mask=None, token_type_ids=None): \n",
        "  \n",
        "      outputs = self.bert(input_ids,\n",
        "                          attention_mask=attention_mask,\n",
        "                          token_type_ids=token_type_ids)\n",
        "      \n",
        "      pooled_output = outputs[1]\n",
        " \n",
        "      return pooled_output\n",
        " \n",
        " \n",
        "def train(args, dataset, p_model, q_model):\n",
        "  \n",
        "  # Dataloader\n",
        "  train_sampler = RandomSampler(dataset)\n",
        "  train_dataloader = DataLoader(dataset, sampler=train_sampler, batch_size=args.per_device_train_batch_size)\n",
        " \n",
        "  # Optimizer\n",
        "  no_decay = ['bias', 'LayerNorm.weight']\n",
        "  optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in p_model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': args.weight_decay},\n",
        "        {'params': [p for n, p in p_model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
        "        {'params': [p for n, p in q_model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': args.weight_decay},\n",
        "        {'params': [p for n, p in q_model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "        ]\n",
        "  optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
        "  t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
        "  scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total)\n",
        " \n",
        "  # Start training!\n",
        "  global_step = 0\n",
        "  \n",
        "  p_model.zero_grad()\n",
        "  q_model.zero_grad()\n",
        "  torch.cuda.empty_cache()\n",
        "  \n",
        "  train_iterator = trange(int(args.num_train_epochs), desc=\"Epoch\")\n",
        " \n",
        "  for _ in train_iterator:\n",
        "    epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\")\n",
        " \n",
        "    for step, batch in enumerate(epoch_iterator):\n",
        "      q_encoder.train()\n",
        "      p_encoder.train()\n",
        "      \n",
        "      if torch.cuda.is_available():\n",
        "        batch = tuple(t.cuda() for t in batch)\n",
        " \n",
        "      p_inputs = {'input_ids': batch[0],\n",
        "                  'attention_mask': batch[1],\n",
        "                  'token_type_ids': batch[2]\n",
        "                  }\n",
        "      \n",
        "      q_inputs = {'input_ids': batch[3],\n",
        "                  'attention_mask': batch[4],\n",
        "                  'token_type_ids': batch[5]}\n",
        "      \n",
        "      p_outputs = p_model(**p_inputs)  # (batch_size, emb_dim)\n",
        "      q_outputs = q_model(**q_inputs)  # (batch_size, emb_dim)\n",
        " \n",
        " \n",
        "      # Calculate similarity score & loss\n",
        "      sim_scores = torch.matmul(q_outputs, torch.transpose(p_outputs, 0, 1))  # (batch_size, emb_dim) x (emb_dim, batch_size) = (batch_size, batch_size)\n",
        " \n",
        "      # target: position of positive samples = diagonal element \n",
        "      targets = torch.arange(0, args.per_device_train_batch_size).long()\n",
        "      if torch.cuda.is_available():\n",
        "        targets = targets.to('cuda')\n",
        " \n",
        "      sim_scores = F.log_softmax(sim_scores, dim=1)\n",
        " \n",
        "      loss = F.nll_loss(sim_scores, targets)\n",
        "      print(loss)\n",
        " \n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      scheduler.step()\n",
        "      q_model.zero_grad()\n",
        "      p_model.zero_grad()\n",
        "      global_step += 1\n",
        "      \n",
        "      torch.cuda.empty_cache()\n",
        " \n",
        " \n",
        "    \n",
        "  return p_model, q_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zd7udmq-mb_"
      },
      "source": [
        "args = TrainingArguments(\n",
        "    output_dir=\"dense_retireval\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=2,\n",
        "    weight_decay=0.01\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdVo0ApV-qN0"
      },
      "source": [
        "# load pre-trained model on cuda (if available)\n",
        "p_encoder = BertEncoder.from_pretrained(model_checkpoint)\n",
        "q_encoder = BertEncoder.from_pretrained(model_checkpoint)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  p_encoder.cuda()\n",
        "  q_encoder.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJctVh1t-xm0"
      },
      "source": [
        "* 학습 진행하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mecZiZkx-vmw"
      },
      "source": [
        "p_encoder, q_encoder = train(args, train_dataset, p_encoder, q_encoder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiCi_K9f-z5x"
      },
      "source": [
        "## Passage Retrieval 준비하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuyMtjLE-4Lb"
      },
      "source": [
        "* search corpus 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQFr3hdm-3Pd"
      },
      "source": [
        "search_corpus = list(set([example['context'] for example in dataset['validation']]))\n",
        "len(search_corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wK8nMoq3-8T3"
      },
      "source": [
        "* embedding 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjxNV3Eb--il"
      },
      "source": [
        "eval_batch_size = 8\n",
        "\n",
        "# Construt dataloader\n",
        "valid_p_seqs = tokenizer(search_corpus, padding=\"max_length\", truncation=True, return_tensors='pt')\n",
        "valid_dataset = TensorDataset(valid_p_seqs['input_ids'], valid_p_seqs['attention_mask'], valid_p_seqs['token_type_ids'])\n",
        "valid_sampler = SequentialSampler(valid_dataset)\n",
        "valid_dataloader = DataLoader(valid_dataset, sampler=valid_sampler, batch_size=eval_batch_size)\n",
        "\n",
        "# Inference using the passage encoder to get dense embeddeings\n",
        "p_embs = []\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "  epoch_iterator = tqdm(valid_dataloader, desc=\"Iteration\", position=0, leave=True)\n",
        "  p_encoder.eval()\n",
        "\n",
        "  for _, batch in enumerate(epoch_iterator):\n",
        "    batch = tuple(t.cuda() for t in batch)\n",
        "\n",
        "    p_inputs = {'input_ids': batch[0],\n",
        "                'attention_mask': batch[1],\n",
        "                'token_type_ids': batch[2]\n",
        "                }\n",
        "        \n",
        "    outputs = p_encoder(**p_inputs).to('cpu').numpy()\n",
        "    p_embs.extend(outputs)\n",
        "\n",
        "p_embs = np.array(p_embs)\n",
        "p_embs.shape  # (num_passage, emb_dim) ## 검색할 대상 문서들의 embedding\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_jQ2AEn_Xsr"
      },
      "source": [
        "* scale이 늘어남\n",
        "  * 5개의 query"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qb2Hel0S_LuX"
      },
      "source": [
        "np.random.seed(1)\n",
        "\n",
        "sample_idx = np.random.choice(range(len(dataset['validation'])), 5)\n",
        "query = dataset['validation'][sample_idx]['question']\n",
        "ground_truth = dataset['validation'][sample_idx]['context']\n",
        "\n",
        "query"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yc4DESBT_Iqa"
      },
      "source": [
        "* 5개의 query를 matrix로 묶기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMH3kFaS_U6y"
      },
      "source": [
        "valid_q_seqs = tokenizer(query, padding=\"max_length\", truncation=True, return_tensors='pt').to('cuda')\n",
        "\n",
        "with torch.no_grad():\n",
        "  q_encoder.eval()\n",
        "  q_embs = q_encoder(**valid_q_seqs).to('cpu').numpy()\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "q_embs.shape  # (num_query, emb_dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFseNUdjQQLu"
      },
      "source": [
        "## GPU를 활용하여 passage retrieval 수행하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtKtRr0aQZ3I"
      },
      "source": [
        "* GPU에서 exhaustive search 수행"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jepswjaiQd4a"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "  p_embs_cuda = torch.Tensor(p_embs).to('cuda')\n",
        "  q_embs_cuda = torch.Tensor(q_embs).to('cuda')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08saGVDe-29O"
      },
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "dot_prod_scores = torch.matmul(q_embs_cuda, torch.transpose(p_embs_cuda, 0, 1))\n",
        "\n",
        "rank = torch.argsort(dot_prod_scores, dim=1, descending=True).squeeze()\n",
        "print(rank)\n",
        "\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time)) # 0.1 seconds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U23zGQvWQgwp"
      },
      "source": [
        "k = 5 \n",
        "\n",
        "for i, q in enumerate(query[:1]):\n",
        "  print(\"[Search query]\\n\", q, \"\\n\")\n",
        "  print(\"[Ground truth passage]\")\n",
        "  print(ground_truth[i], \"\\n\")\n",
        "\n",
        "  r = rank[i]\n",
        "  for j in range(k):\n",
        "    print(\"Top-%d passage with score %.4f\" % (j+1, dot_prod_scores[i][r[j]]))\n",
        "    print(search_corpus[r[j]])\n",
        "  print('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktNaTECnQi59"
      },
      "source": [
        "## FAISS를 활용하여 CPU에서 passage retrieval 수행하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZpOPRq-QsgB"
      },
      "source": [
        "* FAISS SQ8, IVF를 활용하여 수행"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1S3oQUlQ0gc"
      },
      "source": [
        "import faiss\n",
        "\n",
        "num_clusters = 16\n",
        "niter = 5\n",
        "k = 5\n",
        "\n",
        "# 1. Clustering\n",
        "emb_dim = p_embs.shape[-1]\n",
        "index_flat = faiss.IndexFlatL2(emb_dim)\n",
        "\n",
        "clus = faiss.Clustering(emb_dim, num_clusters)\n",
        "clus.verbose = True\n",
        "clus.niter = niter\n",
        "clus.train(p_embs, index_flat)\n",
        "centroids = faiss.vector_float_to_array(clus.centroids)\n",
        "centroids = centroids.reshape(num_clusters, emb_dim)\n",
        "\n",
        "quantizer = faiss.IndexFlatL2(emb_dim)\n",
        "quantizer.add(centroids)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFELNfy0Q17-"
      },
      "source": [
        "# 2. SQ8 + IVF indexer (IndexIVFScalarQuantizer)\n",
        "indexer = faiss.IndexIVFScalarQuantizer(quantizer, quantizer.d, quantizer.ntotal, faiss.METRIC_L2)\n",
        "indexer.train(p_embs)\n",
        "indexer.add(p_embs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxBnGp7cQ3Do"
      },
      "source": [
        "# 3. Search using indexer\n",
        "\n",
        "start_time = time.time()\n",
        "D, I = indexer.search(q_embs, k)\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time)) # 0.002seconds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXycy9DdQ42-"
      },
      "source": [
        "print('=======[Distance]=======')\n",
        "print(D)\n",
        "print('\\n')\n",
        "print('=======[Index of Top-5 Passages]=======')\n",
        "print(I)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UL_8D62uQ6Oz"
      },
      "source": [
        "for i, q in enumerate(query[:1]):\n",
        "  print(\"[Search query]\\n\", q, \"\\n\")\n",
        "  print(\"[Ground truth passage]\")\n",
        "  print(ground_truth[i], \"\\n\")\n",
        "\n",
        "  d = D[i]\n",
        "  i = I[i]\n",
        "  for j in range(k):\n",
        "    print(\"Top-%d passage with distance %.4f\" % (j+1, d[j]))\n",
        "    print(search_corpus[i[j]])\n",
        "  print('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpuvTADaQilr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}