{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3_Generation-based_MRC_practice.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuU6QxB0bJ31"
      },
      "source": [
        "# Generation-based MRC 문제 풀기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4-VYmiVbXGO"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUCLViSKbMp-"
      },
      "source": [
        "## Requirements"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSYihQB-bbSM"
      },
      "source": [
        "* package\n",
        "  * huggingface에서 제공하는 datasets과 transformers\n",
        "  * sentencepiece : 단어를 나눌 때 활용함\n",
        "  * nltk : 언어처리 관련 일반적인 tool"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSBKcoT6bAv8"
      },
      "source": [
        "!pip install datasets==1.4.1\n",
        "!pip install transformers==4.4.1\n",
        "!pip install sentencepiece==0.1.95\n",
        "!pip install nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqZFUsY8bsC-"
      },
      "source": [
        "* tokenizer 다운로드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "589eX1JVbQ7B"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHsJzACMbR9D"
      },
      "source": [
        "## 데이터 및 평가 지표 불러오기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uz5R9fykbxmk"
      },
      "source": [
        "* 데이터셋 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H61AJkVHbURs"
      },
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "datasets = load_dataset(\"squad_kor_v1\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1PVQqiSb5rJ"
      },
      "source": [
        "* EM과 F1 위주의 metric 가져오기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xNqv8IqbVIy"
      },
      "source": [
        "from datasets import load_metric\n",
        "\n",
        "metric = load_metric('squad')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2g1Ee5bbomn"
      },
      "source": [
        "## Pre-trained 모델 및 토크나이저 불러오기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8AFpd7gcqGS"
      },
      "source": [
        "* AutoModel은 사용하는 model에 따라 이름이 조금씩 다름"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmkZx_V4bwm7"
      },
      "source": [
        "from transformers import (\n",
        "    AutoConfig,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    AutoTokenizer\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPwm4CwlAztp"
      },
      "source": [
        "* model 이름 정의\n",
        "  * mt5-small\n",
        "    * mt5 : multilingual t5\n",
        "    * BART와 용도가 비슷함\n",
        "    * 성능이나 디테일한 부분은 다름\n",
        "    * seq-to-seq\n",
        "    * text를 input으로 가져와서 text를 output으로 내는 generation model\n",
        "    * small : 비교적 작은 사이즈 사용\n",
        "    * vocabulary size가 아주 크고, 각 단어의 embedding을 저장해야하기 때문에 필요한 저장공간이 큼(약 1.2GB)\n",
        "    * pre-trained language model을 활용하면 해당 구간의 언어만 활용하기 때문에 차지하는 용량이 줄어듬"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIJbnHorAvyh"
      },
      "source": [
        "model_name = \"google/mt5-small\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZSe1KW8b1k1"
      },
      "source": [
        "from transformers import (\n",
        "    AutoConfig,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    AutoTokenizer\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfBMezTtb2FG"
      },
      "source": [
        "config = AutoConfig.from_pretrained(\n",
        "    model_name,\n",
        "    cache_dir=None,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    cache_dir=None,\n",
        "    use_fast=True,\n",
        ")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "    model_name,\n",
        "    config=config,\n",
        "    cache_dir=None,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ME-3bDlxb3C_"
      },
      "source": [
        "## 설정하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-8wY9jmB81b"
      },
      "source": [
        "* max_target_length\n",
        "  * seq-to-seq이기 때문에 encoding 뿐만아니라 decoding을 해야함\n",
        "  * decoding 단계에서 target length의 max를 정해줘야함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRj8GfdCdLHD"
      },
      "source": [
        "max_source_length = 1024\n",
        "max_target_length = 128\n",
        "padding = False\n",
        "preprocessing_num_workers=12\n",
        "num_beams = 2\n",
        "max_train_samples = 16\n",
        "max_val_samples = 16\n",
        "num_train_epochs = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TM8H4cnGdLuk"
      },
      "source": [
        "## 전처리하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuYkx9_TCSby"
      },
      "source": [
        "* preprocess_function은 extraction-based에 비해 비교적 간단함\n",
        "  * input과 target을 가져옴\n",
        "  * tokenize함\n",
        "  * tokenize한 값들을 model input에 해당하는 key로 지정함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toAPhJx6dNjL"
      },
      "source": [
        "def preprocess_function(examples):\n",
        "    inputs = [f'question: {q}  context: {c} </s>' for q, c in zip(examples['question'], examples['context'])]\n",
        "    targets = [f'{a[\"text\"][0]} </s>' for a in examples['answers']]\n",
        "    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n",
        "\n",
        "    # Setup the tokenizer for targets\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(targets, max_length=max_target_length, padding=padding, truncation=True)\n",
        "\n",
        "    # tokenize한 값들을 model input에 해당하는 key로 지정함\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    model_inputs[\"example_id\"] = []\n",
        "    for i in range(len(model_inputs[\"labels\"])):\n",
        "        model_inputs[\"example_id\"].append(examples[\"id\"][i])\n",
        "    return model_inputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mY2Uc8x4dOV5"
      },
      "source": [
        "column_names = datasets['train'].column_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFYkdcfRDErV"
      },
      "source": [
        "* preprocess_function을 rambda 함수로 넣어줌으로서, 필요한 process를 효율적으로 진행함\n",
        "\n",
        "* map을 활용하는 이유\n",
        "  * num_worker를 활용하여 분산처리 하거나 thread 또는 process 별로 활용하는 등의 효율성을 높일 수 있기 때문\n",
        "  * 효율성이 중요하지 않다면 직접 preprocess_function을 apply하는 방법으로 진행해도 됨"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frJfqQtodO47"
      },
      "source": [
        "train_dataset = datasets[\"train\"]\n",
        "train_dataset = train_dataset.select(range(max_train_samples))\n",
        "train_dataset = train_dataset.map(\n",
        "            preprocess_function,\n",
        "            batched=True,\n",
        "            num_proc=preprocessing_num_workers,\n",
        "            remove_columns=column_names,\n",
        "            load_from_cache_file=False,\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRA8QjLSdP8k"
      },
      "source": [
        "eval_dataset = datasets[\"validation\"]\n",
        "eval_dataset = eval_dataset.select(range(max_val_samples))\n",
        "eval_dataset = eval_dataset.map(\n",
        "            preprocess_function,\n",
        "            batched=True,\n",
        "            num_proc=preprocessing_num_workers,\n",
        "            remove_columns=column_names,\n",
        "            load_from_cache_file=False,\n",
        "        )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGWvQZiclWtV"
      },
      "source": [
        "## Fine-tuning 하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMwe-utlD5tV"
      },
      "source": [
        "* seq2seq을 위한 class 가져오기\n",
        "  * DataCollatorForSeq2Seq : 다른 sequence length를 가지고 있는 input들을 합쳐주어 parallel computing을 하기 쉽게 만들어줌"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjNl4rqAlZUn"
      },
      "source": [
        "from transformers import (\n",
        "    DataCollatorForSeq2Seq,\n",
        "    Seq2SeqTrainer,\n",
        "    Seq2SeqTrainingArguments\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAS67vSwEiex"
      },
      "source": [
        "* label_pad_token_id\n",
        "  * pad가 가지고 있는 token id\n",
        "  * pad를 무시하는 경우 pad가 어떤 token id를 가지고 있는지 알아야함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAlBEkq_lZ-m"
      },
      "source": [
        "label_pad_token_id = tokenizer.pad_token_id\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "            tokenizer,\n",
        "            model=model,\n",
        "            label_pad_token_id=label_pad_token_id,\n",
        "            pad_to_multiple_of=None,\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyTTxQTIFOJf"
      },
      "source": [
        "* postprocess_text\n",
        "  * 예측값과 정답값을 postprocessing함\n",
        "\n",
        "* compute_metrics\n",
        "  * decode된 preds와 label을 가져와서 수치를 compute함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5a_mBPodlar9"
      },
      "source": [
        "def postprocess_text(preds, labels):\n",
        "    # 공백 제거\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [label.strip() for label in labels]\n",
        "    \n",
        "    # 문장별로 새로운 줄을 부여함\n",
        "    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
        "    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
        "\n",
        "    return preds, labels\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "    \n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "    # decoded_labels is for rouge metric, not used for f1/em metric\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Some simple post-processing\n",
        "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "\n",
        "    formatted_predictions = [{\"id\": ex['id'], \"prediction_text\": decoded_preds[i]} for i, ex in enumerate(datasets[\"validation\"].select(range(max_val_samples)))]\n",
        "    references = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in datasets[\"validation\"].select(range(max_val_samples))]\n",
        "\n",
        "    result = metric.compute(predictions=formatted_predictions, references=references)\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzUPGmJelblh"
      },
      "source": [
        "args = Seq2SeqTrainingArguments(\n",
        "    output_dir='outputs', \n",
        "    do_train=True, \n",
        "    do_eval=True, \n",
        "    predict_with_generate=True,\n",
        "    num_train_epochs=num_train_epochs\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rufB3CEtldG-"
      },
      "source": [
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-a1JSP2ld4s"
      },
      "source": [
        "train_result = trainer.train(resume_from_checkpoint=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hkSYVOqleeK"
      },
      "source": [
        "train_result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myEaWU0zlfRR"
      },
      "source": [
        "## 평가하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WZmqZ_iGhNs"
      },
      "source": [
        "* max_length\n",
        "  * 중요함\n",
        "* num_beams\n",
        "  * beam search할 때 beam의 크기 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSXIjNQjlhIu"
      },
      "source": [
        "metrics = trainer.evaluate(\n",
        "    max_length=max_target_length, num_beams=num_beams, metric_key_prefix=\"eval\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROHG56RVliIq"
      },
      "source": [
        "metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIIf-dQgGy8b"
      },
      "source": [
        "* generate()\n",
        "  * output : 답이되는 id"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NA2-YbHFli7J"
      },
      "source": [
        "document = \"세종대왕은 언제 태어났어?\"\n",
        "input_ids = tokenizer(document, return_tensors='pt').input_ids\n",
        "outputs = model.generate(input_ids)\n",
        "tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mb_V7wZIljdW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}