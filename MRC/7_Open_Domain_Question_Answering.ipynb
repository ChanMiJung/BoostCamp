{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "7_Open_Domain_Question_Answering.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JFmQbL9bFNq"
      },
      "source": [
        "# Open-Domain Question Answering (ODQA) 시스템 구축하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GA2oj2xBbJVl"
      },
      "source": [
        "## Requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQVdgxo1bL6C"
      },
      "source": [
        "!pip install datasets==1.4.1 > /dev/null 2>&1 # execute command in silence\n",
        "!pip install transformers==4.4.1 > /dev/null 2>&1\n",
        "!pip install tqdm==4.41.1 > /dev/null 2>&1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwfqdA6YbMh3"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from pprint import pprint ## print를 예쁘게 해주는 utility\n",
        "\n",
        "from datasets import load_dataset, load_metric\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKTPygLUbNBh"
      },
      "source": [
        "## 데이터 및 평가지표 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgXqIYT_bPGC"
      },
      "source": [
        "dataset = load_dataset(\"squad_kor_v1\")\n",
        "# metric = load_metric('squad')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-HgZWBPbP1K"
      },
      "source": [
        "## Sparse retriever 가져오기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pmjNJ6BbRxY"
      },
      "source": [
        "corpus = list(set([example['context'] for example in dataset['train']])) \n",
        "corpus.extend(list(set([example['context'] for example in dataset['validation']])))\n",
        "tokenizer_func = lambda x: x.split(' ')\n",
        "\n",
        "vectorizer = TfidfVectorizer(tokenizer=tokenizer_func, ngram_range=(1,2)) ## unigram, bigram 사용\n",
        "sp_matrix = vectorizer.fit_transform(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_K5EZUfbSxp"
      },
      "source": [
        "def get_relevant_doc(vectorizer, query, k=1):\n",
        "    \"\"\"\n",
        "    참고: vocab 에 없는 이상한 단어로 query 하는 경우 assertion 발생 (예) 뙣뙇?\n",
        "    \"\"\"\n",
        "    query_vec = vectorizer.transform([query])\n",
        "    ## 어떤 단어도 vocab에 존재하지 않는 경우 ## 답이 나오지 않는 경우\n",
        "    assert np.sum(query_vec) != 0, \"오류가 발생했습니다. 이 오류는 보통 query에 vectorizer의 vocab에 없는 단어만 존재하는 경우 발생합니다.\"\n",
        "    result = query_vec * sp_matrix.T\n",
        "    sorted_result = np.argsort(-result.data)\n",
        "    doc_scores = result.data[sorted_result]\n",
        "    doc_ids = result.indices[sorted_result]\n",
        "    return doc_scores[:k], doc_ids[:k]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0l8k7O0fbTsL"
      },
      "source": [
        "\"\"\" 1. 정답이 있는 데이터셋으로 검색해보기 \"\"\" \n",
        "# random.seed(1)\n",
        "# sample_idx = random.choice(range(len(dataset['train'])))\n",
        "# query = dataset['train'][sample_idx]['question']\n",
        "# ground_truth = dataset['train'][sample_idx]['context']\n",
        "# answer = dataset['train'][sample_idx]['answers']\n",
        "\n",
        "\"\"\" 2. 원하는 질문을 입력해보기 \"\"\"\n",
        "query = input(\"Enter any question: \") # \"미국의 대통령은 누구인가?\"\n",
        "# query = \"미국의 대통령은 누구인가?\"\n",
        "_, doc_id = get_relevant_doc(vectorizer, query, k=1)\n",
        "\n",
        "\"\"\" 결과 확인 \"\"\"\n",
        "print(\"{} {} {}\".format('*'*20, 'Result','*'*20))\n",
        "print(\"[Search query]\\n\", query, \"\\n\")\n",
        "print(f\"[Relevant Doc ID(Top 1 passage)]: {doc_id.item()}\")\n",
        "print(corpus[doc_id.item()])\n",
        "# print(answer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMNQUsOybUas"
      },
      "source": [
        "## 훈련된 MRC 모델 가져오기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Arp17MxbWoh"
      },
      "source": [
        "import torch\n",
        "from transformers import (\n",
        "    AutoConfig,\n",
        "    AutoModelForQuestionAnswering,\n",
        "    AutoTokenizer\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JL1H2wOEbWuK"
      },
      "source": [
        "model_name = 'sangrimlee/bert-base-multilingual-cased-korquad'\n",
        "mrc_model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    use_fast=True\n",
        ")\n",
        "mrc_model = mrc_model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_FsRQwJbY4L"
      },
      "source": [
        "def get_answer_from_context(context, question, model, tokenizer):\n",
        "    encoded_dict = tokenizer.encode_plus(  \n",
        "        question,\n",
        "        context,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=512,\n",
        "    )\n",
        "    non_padded_ids = encoded_dict[\"input_ids\"][: encoded_dict[\"input_ids\"].index(tokenizer.pad_token_id)]\n",
        "    full_text = tokenizer.decode(non_padded_ids)\n",
        "    inputs = {\n",
        "    'input_ids': torch.tensor([encoded_dict['input_ids']], dtype=torch.long),\n",
        "    'attention_mask': torch.tensor([encoded_dict['attention_mask']], dtype=torch.long),\n",
        "    'token_type_ids': torch.tensor([encoded_dict['token_type_ids']], dtype=torch.long)\n",
        "    }\n",
        "\n",
        "    outputs = model(**inputs)\n",
        "    start, end = torch.max(outputs.start_logits, axis=1).indices.item(), torch.max(outputs.end_logits, axis=1).indices.item()\n",
        "    answer = tokenizer.decode(encoded_dict['input_ids'][start:end+1])\n",
        "    return answer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvHT_zWlbZ3F"
      },
      "source": [
        "context = corpus[doc_id.item()]\n",
        "answer = get_answer_from_context(context, query, mrc_model, tokenizer)\n",
        "print(\"{} {} {}\".format('*'*20, 'Result','*'*20))\n",
        "print(\"[Search query]\\n\", query, \"\\n\")\n",
        "print(f\"[Relevant Doc ID(Top 1 passage)]: {doc_id.item()}\")\n",
        "pprint(corpus[doc_id.item()], compact=True)\n",
        "print(f\"[Answer Prediction from the model]: {answer}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLMA3v6hbamH"
      },
      "source": [
        "## 통합해서 ODQA 시스템 구축하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9J4fvRTjbdrw"
      },
      "source": [
        "def open_domain_qa(query, corpus, vectorizer, model, tokenizer, k=1):\n",
        "    # 1. Retrieve k relevant docs by usign sparse matrix\n",
        "    _, doc_id = get_relevant_doc(vectorizer, query, k=1)\n",
        "    context = corpus[doc_id.item()] # k=1이기 때문에 하나의 문서를 가져옴\n",
        "\n",
        "    # 2. Predict answer from given doc by using MRC model\n",
        "    answer = get_answer_from_context(context, query, mrc_model, tokenizer)\n",
        "    print(\"{} {} {}\".format('*'*20, 'Result','*'*20))\n",
        "    print(\"[Search query]\\n\", query, \"\\n\")\n",
        "    print(f\"[Relevant Doc ID(Top 1 passage)]: {doc_id.item()}\")\n",
        "    pprint(corpus[doc_id.item()], compact=True)\n",
        "    print(f\"[Answer Prediction from the model]: {answer}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ye_Ws12Jbe9P"
      },
      "source": [
        "query = input(\"Enter any question: \") # \"미국의 대통령은 누구인가?\"\n",
        "open_domain_qa(query, corpus, vectorizer, mrc_model, tokenizer, k=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAS2dyrUbBdF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}